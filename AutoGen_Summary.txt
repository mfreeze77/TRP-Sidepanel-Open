[
    {
        "segment_1": "class RedisCache(AbstractCache)",
        "segment_2": "Implementation of AbstractCache using the Redis database.",
        "segment_3": "This class provides a concrete implementation of the AbstractCache",
        "segment_4": "interface using the Redis database for caching data.",
        "segment_5": "Attributes:",
        "segment_7": "seed str - A seed or namespace used as a prefix for cache keys.",
        "segment_8": "cache redis.Redis - The Redis client used for caching.",
        "segment_10": "Methods:",
        "segment_11": "init(self, seed, redis_url): Initializes the RedisCache with the given seed and Redis URL.",
        "segment_12": "_prefixed_key(self, key): Internal method to get a namespaced cache key.",
        "segment_13": "get(self, key, default=None): Retrieves an item from the cache.",
        "segment_14": "set(self, key, value): Sets an item in the cache.",
        "segment_16": "close(self) - Closes the Redis client.",
        "segment_17": "__enter__(self) - Context management entry.",
        "segment_18": "exit(self, exc_type, exc_value, traceback): Context management exit.",
        "segment_20": "__init__\u200b",
        "segment_21": "def __init__(seed, redis_url)",
        "segment_22": "Initialize the RedisCache instance.",
        "segment_23": "Arguments:",
        "segment_25": "seed str - A seed or namespace for the cache. This is used as a prefix for all cache keys.",
        "segment_26": "redis_url str - The URL for the Redis server.",
        "segment_28": "get\u200b",
        "segment_29": "def get(key, default=None)",
        "segment_30": "Retrieve an item from the Redis cache.",
        "segment_31": "Arguments:",
        "segment_33": "key str - The key identifying the item in the cache.",
        "segment_34": "default optional - The default value to return if the key is not found.",
        "segment_35": "Defaults to None.",
        "segment_37": "Returns:",
        "segment_38": "The deserialized value associated with the key if found, else the default value.",
        "segment_39": "set\u200b",
        "segment_40": "def set(key, value)",
        "segment_41": "Set an item in the Redis cache.",
        "segment_42": "Arguments:",
        "segment_44": "key str - The key under which the item is to be stored.",
        "segment_45": "value - The value to be stored in the cache.",
        "segment_47": "Notes:",
        "segment_48": "The value is serialized using pickle before being stored in Redis.",
        "segment_49": "close\u200b",
        "segment_50": "def close()",
        "segment_51": "Close the Redis client.",
        "segment_52": "Perform any necessary cleanup, such as closing network connections.",
        "segment_53": "__enter__\u200b",
        "segment_54": "def __enter__()",
        "segment_55": "Enter the runtime context related to the object.",
        "segment_56": "Returns:",
        "segment_58": "self - The instance itself.",
        "segment_60": "__exit__\u200b",
        "segment_61": "def __exit__(exc_type, exc_value, traceback)",
        "segment_62": "Exit the runtime context related to the object.",
        "segment_63": "Perform cleanup actions such as closing the Redis client.",
        "segment_64": "Arguments:",
        "segment_66": "exc_type - The exception type if an exception was raised in the context.",
        "segment_67": "exc_value - The exception value if an exception was raised in the context.",
        "segment_68": "traceback - The traceback if an exception was raised in the context.",
        "segment_69": "Edit this pagePreviousdisk_cacheNextclientRedisCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen 0.2.2 introduces a description field to ConversableAgent (and all subclasses), and changes GroupChat so that it uses agent descriptions rather than system_messages when choosing which agents should speak next.",
        "segment_2": "This is expected to simplify GroupChat\u2019s job, improve orchestration, and make it easier to implement new GroupChat or GroupChat-like alternatives.",
        "segment_3": "If you are a developer, and things were already working well for you, no action is needed -- backward compatibility is ensured because the description field defaults to the system_message when no description is provided.",
        "segment_4": "However, if you were struggling with getting GroupChat to work, you can now try updating the description field.",
        "segment_5": "Introduction\u200b",
        "segment_6": "As AutoGen matures and developers build increasingly complex combinations of agents, orchestration is becoming an important capability. At present, GroupChat and the GroupChatManager are the main built-in tools for orchestrating conversations between 3 or more agents. For orchestrators like GroupChat to work well, they need to know something about each agent so that they can decide who should speak and when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's system_message and name to learn about each participating agent. This is likely fine when the system prompt is short and sweet, but can lead to problems when the instructions are very long (e.g., with the AssistantAgent), or non-existent (e.g., with the UserProxyAgent).",
        "segment_7": "AutoGen 0.2.2 introduces a description field to all agents, and replaces the use of the system_message for orchestration in GroupChat and all future orchestrators. The description field defaults to the system_message to ensure backwards compatibility, so you may not need to change anything with your code if things are working well for you. However, if you were struggling with GroupChat, give setting the description field a try.",
        "segment_8": "The remainder of this post provides an example of how using the description field simplifies GroupChat's job, provides some evidence of its effectiveness, and provides tips for writing good descriptions.",
        "segment_9": "Example\u200b",
        "segment_10": "The current GroupChat orchestration system prompt has the following template:",
        "segment_11": "You are in a role play game. The following roles are available:{self._participant_roles(agents)}.Read the following conversation.Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.",
        "segment_12": "Suppose that you wanted to include 3 agents: A UserProxyAgent, an AssistantAgent, and perhaps a GuardrailsAgent.",
        "segment_13": "Prior to 0.2.2, this template would expand to:",
        "segment_14": "You are in a role play game. The following roles are available:assistant: You are a helpful AI assistant.Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply \"TERMINATE\" in the end when everything is done.user_proxy:guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:- You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.- You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.- You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.- You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.- You MUST TERMINATE the conversation if it involves seeking or giving advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice- You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.- You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.- You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.- You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.- You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word \"TERMINATE\". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_15": "As you can see, this description is super confusing:",
        "segment_17": "It is hard to make out where each agent's role-description ends",
        "segment_18": "You appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)",
        "segment_19": "It takes a lot of tokens!",
        "segment_21": "Consequently, it's not hard to see why the GroupChat manager sometimes struggles with this orchestration task.",
        "segment_22": "With AutoGen 0.2.2 onward, GroupChat instead relies on the description field. With a description field the orchestration prompt becomes:",
        "segment_23": "You are in a role play game. The following roles are available:assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_24": "This is much easier to parse and understand, and it doesn't use nearly as many tokens. Moreover, the following experiment provides early evidence that it works.",
        "segment_25": "An Experiment with Distraction\u200b",
        "segment_26": "To illustrate the impact of the description field, we set up a three-agent experiment with a reduced 26-problem subset of the HumanEval benchmark. Here, three agents were added to a GroupChat to solve programming problems. The three agents were:",
        "segment_28": "Coder (default Assistant prompt)",
        "segment_29": "UserProxy (configured to execute code)",
        "segment_30": "ExecutiveChef (added as a distraction)",
        "segment_32": "The Coder and UserProxy used the AssistantAgent and UserProxy defaults (provided above), while the ExecutiveChef was given the system prompt:",
        "segment_33": "You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.",
        "segment_34": "The ExecutiveChef is clearly the distractor here -- given that no HumanEval problems are food-related, the GroupChat should rarely consult with the chef. However, when configured with GPT-3.5-turbo-16k, we can clearly see the GroupChat struggling with orchestration:",
        "segment_35": "With versions prior to 0.2.2, using system_message:\u200b",
        "segment_37": "The Agents solve 3 out of 26 problems on their first turn",
        "segment_38": "The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)",
        "segment_40": "With version 0.2.2, using description:\u200b",
        "segment_42": "The Agents solve 7 out of 26 problems on the first turn",
        "segment_43": "The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)",
        "segment_45": "Using the description field doubles performance on this task and halves the incidence of calling upon the distractor agent.",
        "segment_46": "Tips for Writing Good Descriptions\u200b",
        "segment_47": "Since descriptions serve a different purpose than system_messages, it is worth reviewing what makes a good agent description. While descriptions are new, the following tips appear to lead to good results:",
        "segment_49": "Avoid using the 1st or 2nd person perspective. Descriptions should not contain \"I\" or \"You\", unless perhaps \"You\" is in reference to the GroupChat / orchestrator",
        "segment_50": "Include any details that might help the orchestrator know when to call upon the agent",
        "segment_51": "Keep descriptions short (e.g., \"A helpful AI assistant with strong natural language and Python coding skills.\").",
        "segment_53": "The main thing to remember is that the description is for the benefit of the GroupChatManager, not for the Agent's own use or instruction.",
        "segment_54": "Conclusion\u200b",
        "segment_55": "AutoGen 0.2.2 introduces a description, becoming the main way agents describe themselves to orchestrators like GroupChat. Since the description defaults to the system_message, there's nothing you need to change if you were already satisfied with how your group chats were working. However, we expect this feature to generally improve orchestration, so please consider experimenting with the description field if you are struggling with GroupChat or want to boost performance.Tags:AutoGenNewer PostCode execution is now by default inside docker containerOlder PostAgentOptimizer - An Agentic Way to Train Your LLM AgentTLDRIntroductionExampleAn Experiment with DistractionTips for Writing Good DescriptionsConclusionCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_59": "TL;DR:",
        "segment_61": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_62": "allows retrieval-augmented generation, and its basic usage.",
        "segment_63": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_64": "split function and vector database.",
        "segment_65": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_66": "application with Gradio.",
        "segment_68": "Introduction\u200b",
        "segment_69": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_70": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_71": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_72": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_73": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_74": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_75": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_76": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_77": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_78": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_79": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_80": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_82": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_83": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_84": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_85": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_86": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_87": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_88": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_89": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_90": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_91": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_92": "Retrieval-Augmented Assistant.",
        "segment_93": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_94": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_95": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_96": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_97": "The conversation terminates if no more documents are available for the context.",
        "segment_99": "Basic Usage of RAG Agents\u200b",
        "segment_101": "Install dependencies",
        "segment_103": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_104": "pip install \"pyautogen[retrievechat]\"",
        "segment_105": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_106": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_107": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_108": "If you install unstructured",
        "segment_109": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_110": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_111": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_113": "Import Agents",
        "segment_115": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_117": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_119": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_121": "Initialize Chat and ask a question",
        "segment_123": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_124": "Output is like:",
        "segment_125": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_127": "Create a UserProxyAgent and ask the same question",
        "segment_129": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_130": "Output is like:",
        "segment_131": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_132": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_133": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_134": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_135": "Customizing RAG Agents\u200b",
        "segment_136": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_137": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_138": "function and vector database.",
        "segment_139": "Customizing Embedding Function\u200b",
        "segment_140": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_141": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_143": "OpenAI",
        "segment_145": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_147": "HuggingFace",
        "segment_149": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_150": "More examples can be found here.",
        "segment_151": "Customizing Text Split Function\u200b",
        "segment_152": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_153": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_154": "There are also some existing text split tools which are good to reuse.",
        "segment_155": "For example, you can use all the text splitters in langchain.",
        "segment_156": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_157": "Customizing Vector Database\u200b",
        "segment_158": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_159": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_160": "For example, you can use Qdrant as below:",
        "segment_161": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_162": "Advanced Usage of RAG Agents\u200b",
        "segment_163": "Integrate with other agents in a group chat\u200b",
        "segment_164": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_165": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_166": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_167": "you'll need to call it from a function.",
        "segment_168": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_169": "Build a Chat application with Gradio\u200b",
        "segment_170": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_172": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_173": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_174": "Read More\u200b",
        "segment_175": "You can check out more example notebooks for RAG use cases:",
        "segment_177": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_178": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_179": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_180": "Tags:LLMRAGCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen offers a unified multi-agent conversation framework as a high-level abstraction of using foundation models. It features capable, customizable and conversable agents which integrate LLMs, tools, and humans via automated agent chat.",
        "segment_2": "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.",
        "segment_3": "This framework simplifies the orchestration, automation and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcome their weaknesses. It enables building next-gen LLM applications based on multi-agent conversations with minimal effort.",
        "segment_4": "Agents\u200b",
        "segment_5": "AutoGen abstracts and implements conversable agents",
        "segment_6": "designed to solve tasks through inter-agent conversations. Specifically, the agents in AutoGen have the following notable features:",
        "segment_9": "Conversable: Agents in AutoGen are conversable, which means that any agent can send",
        "segment_10": "and receive messages from other agents to initiate or continue a conversation",
        "segment_13": "Customizable: Agents in AutoGen can be customized to integrate LLMs, humans, tools, or a combination of them.",
        "segment_16": "The figure below shows the built-in agents in AutoGen.",
        "segment_18": "We have designed a generic ConversableAgent",
        "segment_19": "class for Agents that are capable of conversing with each other through the exchange of messages to jointly finish a task. An agent can communicate with other agents and perform actions. Different agents can differ in what actions they perform after receiving messages. Two representative subclasses are AssistantAgent and UserProxyAgent",
        "segment_22": "The AssistantAgent is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM inference configuration can be configured via [llm_config].",
        "segment_25": "The UserProxyAgent is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions or tools. The UserProxyAgent triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the code_execution_config parameter to False. LLM-based response is disabled by default. It can be enabled by setting llm_config to a dict corresponding to the inference configuration. When llm_config is set as a dictionary, UserProxyAgent can generate replies using an LLM when code execution is not performed.",
        "segment_28": "The auto-reply capability of ConversableAgent allows for more autonomous multi-agent communication while retaining the possibility of human intervention.",
        "segment_29": "One can also easily extend it by registering reply functions with the register_reply() method.",
        "segment_30": "In the following code, we create an AssistantAgent named \"assistant\" to serve as the assistant and a UserProxyAgent named \"user_proxy\" to serve as a proxy for the human user. We will later employ these two agents to solve a task.",
        "segment_31": "from autogen import AssistantAgent, UserProxyAgent# create an AssistantAgent instance named \"assistant\"assistant = AssistantAgent(name=\"assistant\")# create a UserProxyAgent instance named \"user_proxy\"user_proxy = UserProxyAgent(name=\"user_proxy\")",
        "segment_32": "Tool calling\u200b",
        "segment_33": "Tool calling enables agents to interact with external tools and APIs more efficiently.",
        "segment_34": "This feature allows the AI model to intelligently choose to output a JSON object containing",
        "segment_35": "arguments to call specific tools based on the user's input. A tool to be called is",
        "segment_36": "specified with a JSON schema describing its parameters and their types. Writing such JSON schema",
        "segment_37": "is complex and error-prone and that is why AutoGen framework provides two high level function decorators for automatically generating such schema using type hints on standard Python datatypes",
        "segment_38": "or Pydantic models:",
        "segment_41": "ConversableAgent.register_for_llm is used to register the function as a Tool in the llm_config of a ConversableAgent. The ConversableAgent agent can propose execution of a registered Tool, but the actual execution will be performed by a UserProxy agent.",
        "segment_44": "ConversableAgent.register_for_execution is used to register the function in the function_map of a UserProxy agent.",
        "segment_47": "The following examples illustrates the process of registering a custom function for currency exchange calculation that uses type hints and standard Python datatypes:",
        "segment_49": "First, we import necessary libraries and configure models using autogen.config_list_from_json function:",
        "segment_51": "from typing import Literalfrom pydantic import BaseModel, Fieldfrom typing_extensions import Annotatedimport autogenconfig_list = autogen.config_list_from_json( \"OAI_CONFIG_LIST\", filter_dict={ \"model\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"], },)",
        "segment_53": "We create an assistant agent and user proxy. The assistant will be responsible for suggesting which functions to call and the user proxy for the actual execution of a proposed function:",
        "segment_55": "llm_config = { \"config_list\": config_list, \"timeout\": 120,}chatbot = autogen.AssistantAgent( name=\"chatbot\", system_message=\"For currency exchange tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\", llm_config=llm_config,)# create a UserProxyAgent instance named \"user_proxy\"user_proxy = autogen.UserProxyAgent( name=\"user_proxy\", is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), human_input_mode=\"NEVER\", max_consecutive_auto_reply=10,)",
        "segment_57": "We define the function currency_calculator below as follows and decorate it with two decorators:",
        "segment_59": "@user_proxy.register_for_execution() adding the function currency_calculator to user_proxy.function_map, and",
        "segment_60": "@chatbot.register_for_llm adding a generated JSON schema of the function to llm_config of chatbot.",
        "segment_64": "CurrencySymbol = Literal[\"USD\", \"EUR\"]def exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float: if base_currency == quote_currency: return 1.0 elif base_currency == \"USD\" and quote_currency == \"EUR\": return 1 / 1.1 elif base_currency == \"EUR\" and quote_currency == \"USD\": return 1.1 else: raise ValueError(f\"Unknown currencies {base_currency}, {quote_currency}\")# NOTE: for Azure OpenAI, please use API version 2023-12-01-preview or later as# support for earlier versions will be deprecated.# For API versions 2023-10-01-preview or earlier you may# need to set `api_style=\"function\"` in the decorator if the default value does not work:# `register_for_llm(description=..., api_style=\"function\")`.@user_proxy.register_for_execution()@chatbot.register_for_llm(description=\"Currency exchange calculator.\")def currency_calculator( base_amount: Annotated[float, \"Amount of currency in base_currency\"], base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\", quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",) -> str: quote_amount = exchange_rate(base_currency, quote_currency) * base_amount return f\"{quote_amount} {quote_currency}\"",
        "segment_65": "Notice the use of Annotated to specify the type and the description of each parameter. The return value of the function must be either string or serializable to string using the json.dumps() or Pydantic model dump to JSON (both version 1.x and 2.x are supported).",
        "segment_66": "You can check the JSON schema generated by the decorator chatbot.llm_config[\"tools\"]:",
        "segment_67": "[{'type': 'function', 'function': {'description': 'Currency exchange calculator.', 'name': 'currency_calculator', 'parameters': {'type': 'object', 'properties': {'base_amount': {'type': 'number', 'description': 'Amount of currency in base_currency'}, 'base_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'USD', 'description': 'Base currency'}, 'quote_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'EUR', 'description': 'Quote currency'}}, 'required': ['base_amount']}}}]",
        "segment_68": "Python decorators are functions themselves. If you do not want to use the",
        "segment_69": "@chatbot.register... decorator syntax,",
        "segment_70": "you can call the decorators as functions:",
        "segment_71": "# Register the function with the chatbot's llm_config.currency_calculator = chatbot.register_for_llm(description=\"Currency exchange calculator.\")(currency_calculator)# Register the function with the user_proxy's function_map.user_proxy.register_for_execution()(currency_calculator)",
        "segment_72": "Alternatevely, you can also use autogen.agentchat.register_function() instead as follows:",
        "segment_73": "def currency_calculator( base_amount: Annotated[float, \"Amount of currency in base_currency\"], base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\", quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",) -> str: quote_amount = exchange_rate(base_currency, quote_currency) * base_amount return f\"{quote_amount} {quote_currency}\"autogen.agentchat.register_function( currency_calculator, agent=chatbot, executor=user_proxy, description=\"Currency exchange calculator.\",)",
        "segment_75": "Agents can now use the function as follows:",
        "segment_77": "user_proxy.initiate_chat( chatbot, message=\"How much is 123.45 USD in EUR?\",)",
        "segment_78": "Output:",
        "segment_79": "user_proxy (to chatbot):How much is 123.45 USD in EUR?--------------------------------------------------------------------------------chatbot (to user_proxy):***** Suggested tool Call: currency_calculator *****Arguments:{\"base_amount\":123.45,\"base_currency\":\"USD\",\"quote_currency\":\"EUR\"}********************************************************-------------------------------------------------------------------------------->>>>>>>> EXECUTING FUNCTION currency_calculator...user_proxy (to chatbot):***** Response from calling function \"currency_calculator\" *****112.22727272727272 EUR****************************************************************--------------------------------------------------------------------------------chatbot (to user_proxy):123.45 USD is equivalent to approximately 112.23 EUR....TERMINATE",
        "segment_80": "Use of Pydantic models further simplifies writing of such functions. Pydantic models can be used",
        "segment_81": "for both the parameters of a function and for its return type. Parameters of such functions will",
        "segment_82": "be constructed from JSON provided by an AI model, while the output will be serialized as JSON",
        "segment_83": "encoded string automatically.",
        "segment_84": "The following example shows how we could rewrite our currency exchange calculator example:",
        "segment_85": "# defines a Pydantic modelclass Currency(BaseModel): # parameter of type CurrencySymbol currency: Annotated[CurrencySymbol, Field(..., description=\"Currency symbol\")] # parameter of type float, must be greater or equal to 0 with default value 0 amount: Annotated[float, Field(0, description=\"Amount of currency\", ge=0)]def currency_calculator( base: Annotated[Currency, \"Base currency: amount and currency symbol\"], quote_currency: Annotated[CurrencySymbol, \"Quote currency symbol\"] = \"USD\",) -> Currency: quote_amount = exchange_rate(base.currency, quote_currency) * base.amount return Currency(amount=quote_amount, currency=quote_currency)autogen.agentchat.register_function( currency_calculator, agent=chatbot, executor=user_proxy, description=\"Currency exchange calculator.\",)",
        "segment_86": "The generated JSON schema has additional properties such as minimum value encoded:",
        "segment_87": "[{'type': 'function', 'function': {'description': 'Currency exchange calculator.', 'name': 'currency_calculator', 'parameters': {'type': 'object', 'properties': {'base': {'properties': {'currency': {'description': 'Currency symbol', 'enum': ['USD', 'EUR'], 'title': 'Currency', 'type': 'string'}, 'amount': {'default': 0, 'description': 'Amount of currency', 'minimum': 0.0, 'title': 'Amount', 'type': 'number'}}, 'required': ['currency'], 'title': 'Currency', 'type': 'object', 'description': 'Base currency: amount and currency symbol'}, 'quote_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'USD', 'description': 'Quote currency symbol'}}, 'required': ['base']}}}]",
        "segment_88": "For more in-depth examples, please check the following:",
        "segment_91": "Currency calculator examples - View Notebook",
        "segment_94": "Use Provided Tools as Functions - View Notebook",
        "segment_97": "Use Tools via Sync and Async Function Calling - View Notebook",
        "segment_100": "Multi-agent Conversations\u200b",
        "segment_101": "A Basic Two-Agent Conversation Example\u200b",
        "segment_102": "Once the participating agents are constructed properly, one can start a multi-agent conversation session by an initialization step as shown in the following code:",
        "segment_103": "# the assistant receives a message from the user, which contains the task descriptionuser_proxy.initiate_chat( assistant, message=\"\"\"What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?\"\"\",)",
        "segment_104": "After the initialization step, the conversation could proceed automatically. Find a visual illustration of how the user_proxy and assistant collaboratively solve the above task autonomously below:",
        "segment_107": "The assistant receives a message from the user_proxy, which contains the task description.",
        "segment_108": "The assistant then tries to write Python code to solve the task and sends the response to the user_proxy.",
        "segment_109": "Once the user_proxy receives a response from the assistant, it tries to reply by either soliciting human input or preparing an automatically generated reply. If no human input is provided, the user_proxy executes the code and uses the result as the auto-reply.",
        "segment_110": "The assistant then generates a further response for the user_proxy. The user_proxy can then decide whether to terminate the conversation. If not, steps 3 and 4 are repeated.",
        "segment_112": "Supporting Diverse Conversation Patterns\u200b",
        "segment_113": "Conversations with different levels of autonomy, and human-involvement patterns\u200b",
        "segment_114": "On the one hand, one can achieve fully autonomous conversations after an initialization step. On the other hand, AutoGen can be used to implement human-in-the-loop problem-solving by configuring human involvement levels and patterns (e.g., setting the human_input_mode to ALWAYS), as human involvement is expected and/or desired in many applications.",
        "segment_115": "Static and dynamic conversations\u200b",
        "segment_116": "AutoGen, by integrating conversation-driven control utilizing both programming and natural language, inherently supports dynamic conversations. This dynamic nature allows the agent topology to adapt based on the actual conversation flow under varying input problem scenarios. Conversely, static conversations adhere to a predefined topology. Dynamic conversations are particularly beneficial in complex settings where interaction patterns cannot be predetermined.",
        "segment_118": "Registered auto-reply",
        "segment_119": "With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. For example:",
        "segment_122": "Hierarchical chat like in OptiGuide.",
        "segment_123": "Dynamic Group Chat which is a special form of hierarchical chat. In the system, we register a reply function in the group chat manager, which broadcasts messages and decides who the next speaker will be in a group chat setting.",
        "segment_124": "Finite state machine (FSM) based group chat which is a special form of dynamic group chat. In this approach, a directed transition matrix is fed into group chat. Users can specify legal transitions or specify disallowed transitions.",
        "segment_125": "Nested chat like in conversational chess.",
        "segment_128": "LLM-Based Function Call",
        "segment_129": "Another approach involves LLM-based function calls, where LLM decides if a specific function should be invoked based on the conversation's status during each inference. This approach enables dynamic multi-agent conversations, as seen in scenarios like multi-user math problem solving scenario, where a student assistant automatically seeks expertise via function calls.",
        "segment_131": "LLM Caching\u200b",
        "segment_132": "Since version 0.2.8, a configurable context manager allows you to easily",
        "segment_133": "configure LLM cache, using either DiskCache or Redis. All agents inside the",
        "segment_134": "context manager will use the same cache.",
        "segment_135": "from autogen import Cache# Use Redis as cachewith Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)# Use DiskCache as cachewith Cache.disk() as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_136": "You can vary the cache_seed parameter to get different LLM output while",
        "segment_137": "still using cache.",
        "segment_138": "# Setting the cache_seed to 1 will use a different cache from the default one# and you will see different output.with Cache.disk(cache_seed=1) as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_139": "By default DiskCache uses .cache for storage. To change the cache directory,",
        "segment_140": "set cache_path_root:",
        "segment_141": "with Cache.disk(cache_path_root=\"/tmp/autogen_cache\") as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_142": "For backward compatibility, DiskCache is on by default with cache_seed set to 41.",
        "segment_143": "To disable caching completely, set cache_seed to None in the llm_config of the agent.",
        "segment_144": "assistant = AssistantAgent( \"coding_agent\", llm_config={ \"cache_seed\": None, \"config_list\": OAI_CONFIG_LIST, \"max_tokens\": 1024, },)",
        "segment_145": "Diverse Applications Implemented with AutoGen\u200b",
        "segment_146": "The figure below shows six examples of applications built using AutoGen.",
        "segment_148": "Find a list of examples in this page: Automated Agent Chat Examples",
        "segment_149": "For Further Reading\u200b",
        "segment_150": "Interested in the research that leads to this package? Please check the following papers.",
        "segment_153": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.",
        "segment_156": "An Empirical Study on Challenging Math Problem Solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).",
        "segment_158": "Edit this pagePreviousLLM Endpoint ConfigurationNextEnhanced InferenceAgentsMulti-agent ConversationsA Basic Two-Agent Conversation ExampleSupporting Diverse Conversation PatternsLLM CachingDiverse Applications Implemented with AutoGenFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_62": "TL;DR:",
        "segment_64": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_65": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_66": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_68": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_69": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_70": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_71": "The MathChat Framework\u200b",
        "segment_72": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_73": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_76": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_79": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_81": "Writing a Python program to solve the problem directly.",
        "segment_82": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_83": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_87": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_90": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_91": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_92": "(The conversation is modified for readability.)",
        "segment_93": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_94": "Experiment Setup\u200b",
        "segment_95": "We evaluate the improvement brought by MathChat.",
        "segment_96": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_97": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_98": "We also evaluate the following methods for comparison:",
        "segment_101": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_104": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_107": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_110": "Experiment Results\u200b",
        "segment_111": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_113": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_114": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_115": "The code for experiments can be found at this repository.",
        "segment_116": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_117": "Future Directions\u200b",
        "segment_118": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_119": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_121": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_122": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_123": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_125": "For Further Reading\u200b",
        "segment_127": "Research paper of MathChat",
        "segment_128": "Documentation about autogen",
        "segment_130": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_131": "TL;DR:",
        "segment_133": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_135": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_136": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_137": "Observations\u200b",
        "segment_139": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_140": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_142": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_143": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_144": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_145": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_146": "What else can we do? We notice that:",
        "segment_147": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_148": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_149": "Solution\u200b",
        "segment_150": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_152": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_153": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_156": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_157": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_159": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_160": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_161": "GPT-4, n=1, temperature=0",
        "segment_162": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_163": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_165": "Experiment Results\u200b",
        "segment_166": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_167": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_168": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_170": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_172": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_174": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_175": "Solved by GPT-4, n=1, temperature=0:",
        "segment_177": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_179": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_181": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_183": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_185": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_186": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_188": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_189": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_191": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_192": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_193": "Discussion\u200b",
        "segment_194": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_195": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_197": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_198": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_200": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_201": "Different configurations have complementary effect.",
        "segment_205": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_206": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_207": "There are many directions of extensions in research and development:",
        "segment_209": "Generalize the way to provide feedback.",
        "segment_210": "Automate the process of optimizing the configurations.",
        "segment_211": "Build adaptive agents for different applications.",
        "segment_213": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_214": "For Further Reading\u200b",
        "segment_216": "Documentation about autogen and Research paper.",
        "segment_217": "Blog post about a related study for math.",
        "segment_218": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_219": "TL;DR:",
        "segment_221": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_222": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_223": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_225": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_226": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_227": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_228": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_229": "Experiment Setup\u200b",
        "segment_230": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_232": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_233": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_235": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_237": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_238": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_239": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_240": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_241": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_243": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_244": "Experiment Results\u200b",
        "segment_245": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_246": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_247": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_249": "However, the selected model changes on level 4 Algebra.",
        "segment_251": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_252": "On level 5 the result is similar.",
        "segment_254": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_255": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_256": "Analysis and Discussion\u200b",
        "segment_257": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_258": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_259": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_260": "For Further Reading\u200b",
        "segment_262": "Research paper about the tuning technique",
        "segment_263": "Documentation about inference tuning",
        "segment_265": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class SimpleTextBrowser()",
        "segment_2": "(In preview) An extremely simple text-based web browser comparable to Lynx. Suitable for Agentic use.",
        "segment_3": "address\u200b",
        "segment_4": "@propertydef address() -> str",
        "segment_5": "Return the address of the current page.",
        "segment_6": "viewport\u200b",
        "segment_7": "@propertydef viewport() -> str",
        "segment_8": "Return the content of the current viewport.",
        "segment_9": "page_content\u200b",
        "segment_10": "@propertydef page_content() -> str",
        "segment_11": "Return the full contents of the current page.",
        "segment_12": "visit_page\u200b",
        "segment_13": "def visit_page(path_or_uri)",
        "segment_14": "Update the address, visit the page, and return the content of the viewport.Edit this pagePreviousagent_utilsNextcode_utilsSimpleTextBrowser ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class SocietyOfMindAgent(ConversableAgent)",
        "segment_2": "(In preview) A single agent that runs a Group Chat as an inner monologue.",
        "segment_3": "At the end of the conversation (termination for any reason), the SocietyOfMindAgent",
        "segment_4": "applies the response_preparer method on the entire inner monologue message history to",
        "segment_5": "extract a final answer for the reply.",
        "segment_6": "Most arguments are inherited from ConversableAgent. New arguments are:",
        "segment_7": "chat_manager (GroupChatManager): the group chat manager that will be running the inner monologue",
        "segment_8": "response_preparer (Optional, Callable or String): If response_preparer is a callable function, then",
        "segment_9": "it should have the signature:",
        "segment_10": "f( self: SocietyOfMindAgent, messages: List[Dict])",
        "segment_11": "where self is this SocietyOfMindAgent, and messages is a list of inner-monologue messages.",
        "segment_12": "The function should return a string representing the final response (extracted or prepared)",
        "segment_13": "from that history.",
        "segment_14": "If response_preparer is a string, then it should be the LLM prompt used to extract the final",
        "segment_15": "message from the inner chat transcript.",
        "segment_16": "The default response_preparer depends on if an llm_config is provided. If llm_config is False,",
        "segment_17": "then the response_preparer deterministically returns the last message in the inner-monolgue. If",
        "segment_18": "llm_config is set to anything else, then a default LLM prompt is used.",
        "segment_19": "chat_manager\u200b",
        "segment_20": "@propertydef chat_manager() -> Union[GroupChatManager, None]",
        "segment_21": "Return the group chat manager.",
        "segment_22": "update_chat_manager\u200b",
        "segment_23": "def update_chat_manager(chat_manager: Union[GroupChatManager, None])",
        "segment_24": "Update the chat manager.",
        "segment_25": "Arguments:",
        "segment_27": "chat_manager GroupChatManager - the group chat manager",
        "segment_29": "generate_inner_monologue_reply\u200b",
        "segment_30": "def generate_inner_monologue_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_31": "Generate a reply by running the group"
    },
    {
        "segment_1": "class MathUserProxyAgent(UserProxyAgent)",
        "segment_2": "(Experimental) A MathChat agent that can handle math problems.",
        "segment_3": "MAX_CONSECUTIVE_AUTO_REPLY\u200b",
        "segment_4": "maximum number of consecutive auto replies (subject to future change)",
        "segment_5": "__init__\u200b",
        "segment_6": "def __init__(name: Optional[str] = \"MathChatAgent\", is_termination_msg: Optional[Callable[ [Dict], bool]] = _is_termination_msg_mathchat, human_input_mode: Optional[str] = \"NEVER\", default_auto_reply: Optional[Union[str, Dict, None]] = DEFAULT_REPLY, max_invalid_q_per_step=3, **kwargs)",
        "segment_7": "Arguments:",
        "segment_9": "name str - name of the agent",
        "segment_10": "is_termination_msg function - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.",
        "segment_11": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_12": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_13": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_14": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_15": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_16": "or when is_termination_msg is True and there is no human input.",
        "segment_17": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_18": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_19": "(3) (Default) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_20": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_21": "default_auto_reply str or dict or None - the default auto reply message when no code execution or llm based reply is generated.",
        "segment_22": "max_invalid_q_per_step int - (ADDED) the maximum number of invalid queries per step.",
        "segment_23": "**kwargs dict - other kwargs in UserProxyAgent.",
        "segment_25": "generate_init_message\u200b",
        "segment_26": "def generate_init_message(problem, prompt_type=\"default\", customized_prompt=None)",
        "segment_27": "Generate a prompt for the assistant agent with the given problem and prompt.",
        "segment_28": "Arguments:",
        "segment_30": "problem str - the problem to be solved.",
        "segment_31": "prompt_type str - the type of the prompt. Possible values are \"default\", \"python\", \"wolfram\".",
        "segment_32": "(1) \"default\": the prompt that allows the agent to choose between 3 ways to solve a problem:",
        "segment_34": "write a python program to solve it directly.",
        "segment_35": "solve it directly without python.",
        "segment_36": "solve it step by step with python.",
        "segment_37": "(2) \"python\":",
        "segment_38": "a simplified prompt from the third way of the \"default\" prompt, that asks the assistant",
        "segment_39": "to solve the problem step by step with python.",
        "segment_40": "(3) \"two_tools\":",
        "segment_41": "a simplified prompt similar to the \"python\" prompt, but allows the model to choose between",
        "segment_42": "Python and Wolfram Alpha to solve the problem.",
        "segment_45": "customized_prompt str - a customized prompt to be used. If it is not None, the prompt_type will be ignored.",
        "segment_47": "Returns:",
        "segment_49": "str - the generated prompt ready to be sent to the assistant agent.",
        "segment_51": "execute_one_python_code\u200b",
        "segment_52": "def execute_one_python_code(pycode)",
        "segment_53": "Execute python code blocks.",
        "segment_54": "Previous python code will be saved and executed together with the new code.",
        "segment_55": "the \"print\" function will also be added to the last line of the code if needed",
        "segment_56": "execute_one_wolfram_query\u200b",
        "segment_57": "def execute_one_wolfram_query(query: str)",
        "segment_58": "Run one wolfram query and return the output.",
        "segment_59": "Arguments:",
        "segment_61": "query - string of the query.",
        "segment_63": "Returns:",
        "segment_65": "output - string with the output of the query.",
        "segment_66": "is_success - boolean indicating whether the query was successful.",
        "segment_68": "get_from_dict_or_env\u200b",
        "segment_69": "def get_from_dict_or_env(data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None) -> str",
        "segment_70": "Get a value from a dictionary or an environment variable.",
        "segment_71": "WolframAlphaAPIWrapper Objects\u200b",
        "segment_72": "class WolframAlphaAPIWrapper(BaseModel)",
        "segment_73": "Wrapper for Wolfram Alpha.",
        "segment_74": "Docs for using:",
        "segment_76": "Go to wolfram alpha and sign up for a developer account",
        "segment_77": "Create an app and get your APP ID",
        "segment_78": "Save your APP ID into WOLFRAM_ALPHA_APPID env variable",
        "segment_79": "pip install wolframalpha",
        "segment_81": "wolfram_client\u200b",
        "segment_82": ":meta private:",
        "segment_83": "Config Objects\u200b",
        "segment_84": "class Config()",
        "segment_85": "Configuration for this pydantic object.",
        "segment_86": "validate_environment\u200b",
        "segment_87": "@root_validator(skip_on_failure=True)def validate_environment(cls, values: Dict) -> Dict",
        "segment_88": "Validate that api key and python package exists in environment.",
        "segment_89": "run\u200b",
        "segment_90": "def run(query: str) -> Tuple[str, bool]",
        "segment_91": "Run query through WolframAlpha and parse result.Edit this pagePreviousllava_agentNextmultimodal_conversable_agentMathUserProxyAgent ObjectsWolframAlphaAPIWrapper ObjectsConfig ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class AgentBuilder()",
        "segment_2": "AgentBuilder can help user build an automatic task solving process powered by multi-agent system.",
        "segment_3": "Specifically, our building pipeline includes initialize and build.",
        "segment_4": "In build(), we prompt a LLM to create multiple participant agents, and specify whether this task need programming to solve.",
        "segment_5": "User can save the built agents' config by calling save(), and load the saved configs by load(), which can skip the",
        "segment_6": "building process.",
        "segment_7": "__init__\u200b",
        "segment_8": "def __init__(config_file_or_env: Optional[str] = \"OAI_CONFIG_LIST\", config_file_location: Optional[str] = \"\", builder_model: Optional[str] = \"gpt-4\", agent_model: Optional[str] = \"gpt-4\", host: Optional[str] = \"localhost\", endpoint_building_timeout: Optional[int] = 600, max_tokens: Optional[int] = 945, max_agents: Optional[int] = 5)",
        "segment_9": "(These APIs are experimental and may change in the future.)",
        "segment_10": "Arguments:",
        "segment_12": "config_file_or_env - path or environment of the OpenAI api configs.",
        "segment_13": "builder_model - specify a model as the backbone of build manager.",
        "segment_14": "agent_model - specify a model as the backbone of participant agents.",
        "segment_15": "host - endpoint host.",
        "segment_16": "endpoint_building_timeout - timeout for building up an endpoint server.",
        "segment_17": "max_tokens - max tokens for each agent.",
        "segment_18": "max_agents - max agents for each task.",
        "segment_20": "clear_agent\u200b",
        "segment_21": "def clear_agent(agent_name: str, recycle_endpoint: Optional[bool] = True)",
        "segment_22": "Clear a specific agent by name.",
        "segment_23": "Arguments:",
        "segment_25": "agent_name - the name of agent.",
        "segment_26": "recycle_endpoint - trigger for recycle the endpoint server. If true, the endpoint will be recycled",
        "segment_27": "when there is no agent depending on.",
        "segment_29": "clear_all_agents\u200b",
        "segment_30": "def clear_all_agents(recycle_endpoint: Optional[bool] = True)",
        "segment_31": "Clear all cached agents.",
        "segment_32": "build\u200b",
        "segment_33": "def build(building_task: str, default_llm_config: Dict, coding: Optional[bool] = None, code_execution_config: Optional[Dict] = None, use_oai_assistant: Optional[bool] = False, **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]",
        "segment_34": "Auto build agents based on the building task.",
        "segment_35": "Arguments:",
        "segment_37": "building_task - instruction that helps build manager (gpt-4) to decide what agent should be built.",
        "segment_38": "coding - use to identify if the user proxy (a code interpreter) should be added.",
        "segment_39": "code_execution_config - specific configs for user proxy (e.g., last_n_messages, work_dir, ...).",
        "segment_40": "default_llm_config - specific configs for LLM (e.g., config_list, seed, temperature, ...).",
        "segment_41": "use_oai_assistant - use OpenAI assistant api instead of self-constructed agent.",
        "segment_43": "Returns:",
        "segment_45": "agent_list - a list of agents.",
        "segment_46": "cached_configs - cached configs.",
        "segment_48": "build_from_library\u200b",
        "segment_49": "def build_from_library( building_task: str, library_path_or_json: str, default_llm_config: Dict, coding: Optional[bool] = True, code_execution_config: Optional[Dict] = None, use_oai_assistant: Optional[bool] = False, embedding_model: Optional[str] = None, **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]",
        "segment_50": "Build agents from a library.",
        "segment_51": "The library is a list of agent configs, which contains the name and system_message for each agent.",
        "segment_52": "We use a build manager to decide what agent in that library should be involved to the task.",
        "segment_53": "Arguments:",
        "segment_55": "building_task - instruction that helps build manager (gpt-4) to decide what agent should be built.",
        "segment_56": "library_path_or_json - path or JSON string config of agent library.",
        "segment_57": "default_llm_config - specific configs for LLM (e.g., config_list, seed, temperature, ...).",
        "segment_58": "coding - use to identify if the user proxy (a code interpreter) should be added.",
        "segment_59": "code_execution_config - specific configs for user proxy (e.g., last_n_messages, work_dir, ...).",
        "segment_60": "use_oai_assistant - use OpenAI assistant api instead of self-constructed agent.",
        "segment_61": "embedding_model - a Sentence-Transformers model use for embedding similarity to select agents from library.",
        "segment_62": "if None, an openai model will be prompted to select agents. As reference, chromadb use \"all-mpnet-base-",
        "segment_63": "v2\" as default.",
        "segment_65": "Returns:",
        "segment_67": "agent_list - a list of agents.",
        "segment_68": "cached_configs - cached configs.",
        "segment_70": "save\u200b",
        "segment_71": "def save(filepath: Optional[str] = None) -> str",
        "segment_72": "Save building configs. If the filepath is not specific, this function will create a filename by encrypt the",
        "segment_73": "building_task string by md5 with \"save_config_\" prefix, and save config to the local path.",
        "segment_74": "Arguments:",
        "segment_76": "filepath - save path.",
        "segment_78": "Returns:",
        "segment_80": "filepath - path save.",
        "segment_82": "load\u200b",
        "segment_83": "def load(filepath: Optional[str] = None, config_json: Optional[str] = None, use_oai_assistant: Optional[bool] = False, **kwargs) -> Tuple[List[autogen.ConversableAgent], Dict]",
        "segment_84": "Load building configs and call the build function to complete building without calling online LLMs' api.",
        "segment_85": "Arguments:",
        "segment_87": "filepath - filepath or JSON string for the save config.",
        "segment_88": "config_json - JSON string for the save config.",
        "segment_89": "use_oai_assistant - use OpenAI assistant api instead of self-constructed agent.",
        "segment_91": "Returns:",
        "segment_93": "agent_list - a list of agents.",
        "segment_94": "cached_configs - cached configs.",
        "segment_95": "Edit this pagePreviousteachabilityNextcompressible_agentAgentBuilder ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def content_str(content: Union[str, List, None]) -> str",
        "segment_2": "Converts content into a string format.",
        "segment_3": "This function processes content that may be a string, a list of mixed text and image URLs, or None,",
        "segment_4": "and converts it into a string. Text is directly appended to the result string, while image URLs are",
        "segment_5": "represented by a placeholder image token. If the content is None, an empty string is returned.",
        "segment_6": "Arguments:",
        "segment_8": "content (Union[str, List, None]): The content to be processed. Can be a string, a list of dictionaries",
        "segment_9": "representing text and image URLs, or None.",
        "segment_11": "Returns:",
        "segment_13": "str - A string representation of the input content. Image URLs are replaced with an image token.",
        "segment_15": "Notes:",
        "segment_17": "The function expects each dictionary in the list to have a \"type\" key that is either \"text\" or \"image_url\".",
        "segment_18": "For \"text\" type, the \"text\" key's value is appended to the result. For \"image_url\", an image token is appended.",
        "segment_19": "This function is useful for handling content that may include both text and image references, especially",
        "segment_20": "in contexts where images need to be represented as placeholders.",
        "segment_22": "infer_lang\u200b",
        "segment_23": "def infer_lang(code)",
        "segment_24": "infer the language for the code.",
        "segment_25": "TODO: make it robust.",
        "segment_26": "extract_code\u200b",
        "segment_27": "def extract_code( text: Union[str, List], pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False) -> List[Tuple[str, str]]",
        "segment_28": "Extract code from a text.",
        "segment_29": "Arguments:",
        "segment_31": "text str or List - The content to extract code from. The content can be",
        "segment_32": "a string or a list, as returned by standard GPT or multimodal GPT.",
        "segment_33": "pattern str, optional - The regular expression pattern for finding the",
        "segment_34": "code block. Defaults to CODE_BLOCK_PATTERN.",
        "segment_35": "detect_single_line_code bool, optional - Enable the new feature for",
        "segment_36": "extracting single line code. Defaults to False.",
        "segment_38": "Returns:",
        "segment_40": "list - A list of tuples, each containing the language and the code.",
        "segment_41": "If there is no code block in the input text, the language would be \"unknown\".",
        "segment_42": "If there is code block but the language is not specified, the language would be \"\".",
        "segment_44": "generate_code\u200b",
        "segment_45": "def generate_code(pattern: str = CODE_BLOCK_PATTERN, **config) -> Tuple[str, float]",
        "segment_46": "(openai<1) Generate code.",
        "segment_47": "Arguments:",
        "segment_49": "pattern Optional, str - The regular expression pattern for finding the code block.",
        "segment_50": "The default pattern is for finding a code block in a markdown file.",
        "segment_51": "config Optional, dict - The configuration for the API call.",
        "segment_53": "Returns:",
        "segment_55": "str - The generated code.",
        "segment_56": "float - The cost of the generation.",
        "segment_58": "improve_function\u200b",
        "segment_59": "def improve_function(file_name, func_name, objective, **config)",
        "segment_60": "(openai<1) Improve the function to achieve the objective.",
        "segment_61": "improve_code\u200b",
        "segment_62": "def improve_code(files, objective, suggest_only=True, **config)",
        "segment_63": "(openai<1) Improve the code to achieve a given objective.",
        "segment_64": "Arguments:",
        "segment_66": "files list - A list of file names containing the source code.",
        "segment_67": "objective str - The objective to achieve.",
        "segment_68": "suggest_only bool - Whether to return only the suggestions or the improved code.",
        "segment_69": "config Optional, dict - The configuration for the API call.",
        "segment_71": "Returns:",
        "segment_73": "str - The improved code if suggest_only=False; a list of suggestions if suggest_only=True (default).",
        "segment_74": "float - The cost of the generation.",
        "segment_76": "is_docker_running\u200b",
        "segment_77": "def is_docker_running()",
        "segment_78": "Check if docker is running.",
        "segment_79": "Returns:",
        "segment_81": "bool - True if docker is running; False otherwise.",
        "segment_83": "in_docker_container\u200b",
        "segment_84": "def in_docker_container()",
        "segment_85": "Check if the code is running in a docker container.",
        "segment_86": "Returns:",
        "segment_88": "bool - True if the code is running in a docker container; False otherwise.",
        "segment_90": "execute_code\u200b",
        "segment_91": "def execute_code(code: Optional[str] = None, timeout: Optional[int] = None, filename: Optional[str] = None, work_dir: Optional[str] = None, use_docker: Union[List[str], str, bool] = SENTINEL, lang: Optional[str] = \"python\") -> Tuple[int, str, str]",
        "segment_92": "Execute code in a docker container.",
        "segment_93": "This function is not tested on MacOS.",
        "segment_94": "Arguments:",
        "segment_96": "code Optional, str - The code to execute.",
        "segment_97": "If None, the code from the file specified by filename will be executed.",
        "segment_98": "Either code or filename must be provided.",
        "segment_99": "timeout Optional, int - The maximum execution time in seconds.",
        "segment_100": "If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.",
        "segment_101": "filename Optional, str - The file name to save the code or where the code is stored when code is None.",
        "segment_102": "If None, a file with a randomly generated name will be created.",
        "segment_103": "The randomly generated file will be deleted after execution.",
        "segment_104": "The file name must be a relative path. Relative paths are relative to the working directory.",
        "segment_105": "work_dir Optional, str - The working directory for the code execution.",
        "segment_106": "If None, a default working directory will be used.",
        "segment_107": "The default working directory is the \"extensions\" directory under",
        "segment_108": "\"path_to_autogen\".",
        "segment_109": "use_docker list, str or bool - The docker image to use for code execution.",
        "segment_110": "Default is True, which means the code will be executed in a docker container. A default list of images will be used.",
        "segment_111": "If a list or a str of image name(s) is provided, the code will be executed in a docker container",
        "segment_112": "with the first image successfully pulled.",
        "segment_113": "If False, the code will be executed in the current environment.",
        "segment_114": "Expected behaviour:",
        "segment_116": "If use_docker is not set (i.e. left default to True) or is explicitly set to True and the docker package is available, the code will run in a Docker container.",
        "segment_117": "If use_docker is not set (i.e. left default to True) or is explicitly set to True but the Docker package is missing or docker isn't running, an error will be raised.",
        "segment_118": "If use_docker is explicitly set to False, the code will run natively.",
        "segment_119": "If the code is executed in the current environment,",
        "segment_120": "the code must be trusted.",
        "segment_123": "lang Optional, str - The language of the code. Default is \"python\".",
        "segment_125": "Returns:",
        "segment_127": "timeout0 - 0 if the code executes successfully.",
        "segment_128": "timeout1 - The error message if the code fails to execute; the stdout otherwise.",
        "segment_129": "timeout2 - The docker image name after container run when docker is used.",
        "segment_131": "generate_assertions\u200b",
        "segment_132": "def generate_assertions(definition: str, **config) -> Tuple[str, float]",
        "segment_133": "(openai<1) Generate assertions for a function.",
        "segment_134": "Arguments:",
        "segment_136": "definition str - The function definition, including the signature and docstr.",
        "segment_137": "config Optional, dict - The configuration for the API call.",
        "segment_139": "Returns:",
        "segment_141": "str - The generated assertions.",
        "segment_142": "float - The cost of the generation.",
        "segment_144": "eval_function_completions\u200b",
        "segment_145": "def eval_function_completions(responses: List[str], definition: str, test: Optional[str] = None, entry_point: Optional[str] = None, assertions: Optional[Union[str, Callable[ [str], Tuple[str, float]]]] = None, timeout: Optional[float] = 3, use_docker: Optional[bool] = True) -> Dict",
        "segment_146": "(openai<1) Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.",
        "segment_147": "Arguments:",
        "segment_149": "responses list - The list of responses.",
        "segment_150": "definition str - The input definition.",
        "segment_151": "test Optional, str - The test code.",
        "segment_152": "entry_point Optional, str - The name of the function.",
        "segment_153": "assertions Optional, str or Callable - The assertion code which serves as a filter of the responses, or an assertion generator.",
        "segment_154": "When provided, only the responses that pass the assertions will be considered for the actual test (if provided).",
        "segment_155": "timeout Optional, float - The timeout for executing the code.",
        "segment_157": "Returns:",
        "segment_159": "dict - The success metrics.",
        "segment_161": "PassAssertionFilter Objects\u200b",
        "segment_162": "class PassAssertionFilter()",
        "segment_163": "pass_assertions\u200b",
        "segment_164": "def pass_assertions(context, response, **_)",
        "segment_165": "(openai<1) Check if the response passes the assertions.",
        "segment_166": "implement\u200b",
        "segment_167": "def implement( definition: str, configs: Optional[List[Dict]] = None, assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = generate_assertions) -> Tuple[str, float]",
        "segment_168": "(openai<1) Implement a function from a definition.",
        "segment_169": "Arguments:",
        "segment_171": "definition str - The function definition, including the signature and docstr.",
        "segment_172": "configs list - The list of configurations for completion.",
        "segment_173": "assertions Optional, str or Callable - The assertion code which serves as a filter of the responses, or an assertion generator.",
        "segment_175": "Returns:",
        "segment_177": "str - The implementation.",
        "segment_178": "float - The cost of the implementation.",
        "segment_179": "int - The index of the configuration which generates the implementation.",
        "segment_180": "Edit this pagePreviousbrowser_utilsNextfunction_utilsPassAssertionFilter ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_2": "Introducing AutoBuild, building multi-agent system automatically, fast, and easily for complex tasks with minimal",
        "segment_3": "user prompt required, powered by a new designed class AgentBuilder. AgentBuilder also supports open-source LLMs by",
        "segment_4": "leveraging vLLM and FastChat.",
        "segment_5": "Checkout example notebooks and source code for reference:",
        "segment_7": "AutoBuild Examples",
        "segment_8": "AgentBuilder",
        "segment_10": "Introduction\u200b",
        "segment_11": "In this blog, we introduce AutoBuild, a pipeline that can automatically build multi-agent systems for complex tasks.",
        "segment_12": "Specifically, we design a new class called AgentBuilder, which will complete the generation of participant expert agents",
        "segment_13": "and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.",
        "segment_14": "AgentBuilder supports open-source models on Hugging Face powered by vLLM",
        "segment_15": "and FastChat. Once the user chooses to use open-source LLM, AgentBuilder will set",
        "segment_16": "up an endpoint server automatically without any user participation.",
        "segment_17": "Installation\u200b",
        "segment_19": "AutoGen:",
        "segment_21": "pip install pyautogen[autobuild]",
        "segment_23": "(Optional: if you want to use open-source LLMs) vLLM and FastChat",
        "segment_25": "pip install vllm fastchat",
        "segment_26": "Basic Example\u200b",
        "segment_27": "In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.",
        "segment_28": "Step 1: prepare configurations\u200b",
        "segment_29": "First, we need to prepare the Agent configurations.",
        "segment_30": "Specifically, a config path containing the model name and API key, and a default config for each agent, are required.",
        "segment_31": "config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST' # modify pathdefault_llm_config = { 'temperature': 0}",
        "segment_32": "Step 2: create an AgentBuilder instance\u200b",
        "segment_33": "Then, we create an AgentBuilder instance with the config path and default config.",
        "segment_34": "You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.",
        "segment_35": "from autogen.agentchat.contrib.agent_builder import AgentBuilderbuilder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')",
        "segment_36": "Step 3: specify the building task\u200b",
        "segment_37": "Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.",
        "segment_38": "Note that your building task should have a general description of the task. Adding some specific examples is better.",
        "segment_39": "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"",
        "segment_40": "Step 4: build group chat agents\u200b",
        "segment_41": "Use build() to let the build manager (with a builder_model as backbone) complete the group chat agents generation.",
        "segment_42": "If you think coding is necessary for your task, you can use coding=True to add a user proxy (a local code interpreter) into the agent list as:",
        "segment_43": "agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)",
        "segment_44": "If coding is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.",
        "segment_45": "The generated agent_list is a list of AssistantAgent instances.",
        "segment_46": "If coding is true, a user proxy (a UserProxyAssistant instance) will be added as the first element to the agent_list.",
        "segment_47": "agent_configs is a list of agent configurations including agent name, backbone LLM model, and system message.",
        "segment_48": "For example",
        "segment_49": "// an example of agent_configs. AgentBuilder will generate agents with the following configurations.[ { \"name\": \"ArXiv_Data_Scraper_Developer\", \"model\": \"gpt-4-1106-preview\", \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\", \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\" }, ...]",
        "segment_50": "Step 5: execute the task\u200b",
        "segment_51": "Let agents generated in build() complete the task collaboratively in a group chat.",
        "segment_52": "import autogendef start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\"]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task)start_task( execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\", agent_list=agent_list, llm_config=default_llm_config)",
        "segment_53": "Step 6 (Optional): clear all agents and prepare for the next task\u200b",
        "segment_54": "You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.",
        "segment_55": "builder.clear_all_agents(recycle_endpoint=True)",
        "segment_56": "If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.",
        "segment_57": "If necessary, you can use recycle_endpoint=False to retain the previous open-source LLM's endpoint server.",
        "segment_58": "Save and Load\u200b",
        "segment_59": "You can save all necessary information of the built group chat agents by",
        "segment_60": "saved_path = builder.save()",
        "segment_61": "Configurations will be saved in JSON format with the following content:",
        "segment_62": "// FILENAME: save_config_TASK_MD5.json{ \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"agent_configs\": [ { \"name\": \"...\", \"model\": \"...\", \"system_message\": \"...\", \"description\": \"...\" }, ... ], \"manager_system_message\": \"...\", \"code_execution_config\": {...}, \"default_llm_config\": {...}}",
        "segment_63": "You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename save_config_TASK_MD5.json.",
        "segment_64": "You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.",
        "segment_65": "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)agent_list, agent_config = new_builder.load(saved_path)start_task(...) # skip build()",
        "segment_66": "Use OpenAI Assistant\u200b",
        "segment_67": "Assistants API allows you to build AI assistants within your own applications.",
        "segment_68": "An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.",
        "segment_69": "AutoBuild also supports the assistant API by adding use_oai_assistant=True to build().",
        "segment_70": "# Transfer to the OpenAI Assistant API.agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)...",
        "segment_71": "(Experimental) Use Open-source LLM\u200b",
        "segment_72": "AutoBuild supports open-source LLM by vLLM and FastChat.",
        "segment_73": "Check the supported model list here.",
        "segment_74": "After satisfying the requirements, you can add an open-source LLM's huggingface repository to the config file,",
        "segment_75": "// Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.[ ... { \"model\": \"meta-llama/Llama-2-13b-chat-hf\", \"api_key\": \"EMPTY\" }]",
        "segment_76": "and specify it when initializing AgentBuilder.",
        "segment_77": "AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.",
        "segment_78": "Future work/Roadmap\u200b",
        "segment_80": "Let the builder select the best agents from a given library/database to solve the task.",
        "segment_82": "Summary\u200b",
        "segment_83": "We propose AutoBuild with a new class AgentBuilder.",
        "segment_84": "AutoBuild can help user solve their complex task with an automatically built multi-agent system.",
        "segment_85": "AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.",
        "segment_86": "More advanced features are coming soon.Tags:LLMresearchNewer PostAutoGen Studio: Interactively Explore Multi-Agent WorkflowsOlder PostHow to Assess Utility of LLM-powered Applications?IntroductionInstallationBasic ExampleStep 1: prepare configurationsStep 2: create an AgentBuilder instanceStep 3: specify the building taskStep 4: build group chat agentsStep 5: execute the taskStep 6 (Optional): clear all agents and prepare for the next taskSave and LoadUse OpenAI Assistant(Experimental) Use Open-source LLMFuture work/RoadmapSummaryCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "In Brief:",
        "segment_3": "Introducing the Multimodal Conversable Agent and the LLaVA Agent to enhance LMM functionalities.",
        "segment_4": "Users can input text and images simultaneously using the tag to specify image loading.",
        "segment_5": "Demonstrated through the GPT-4V notebook.",
        "segment_6": "Demonstrated through the LLaVA notebook.",
        "segment_8": "Introduction\u200b",
        "segment_9": "Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.",
        "segment_10": "This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.",
        "segment_11": "We support the gpt-4-vision-preview model from OpenAI and LLaVA model from Microsoft now.",
        "segment_12": "Here, we emphasize the Multimodal Conversable Agent and the LLaVA Agent due to their growing popularity.",
        "segment_13": "GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.",
        "segment_14": "Installation\u200b",
        "segment_15": "Incorporate the lmm feature during AutoGen installation:",
        "segment_16": "pip install \"pyautogen[lmm]\"",
        "segment_17": "Subsequently, import the Multimodal Conversable Agent or LLaVA Agent from AutoGen:",
        "segment_18": "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent # for GPT-4Vfrom autogen.agentchat.contrib.llava_agent import LLaVAAgent # for LLaVA",
        "segment_19": "Usage\u200b",
        "segment_20": "A simple syntax has been defined to incorporate both messages and images within a single string.",
        "segment_21": "Example of an in-context learning prompt:",
        "segment_22": "prompt = \"\"\"You are now an image classifier for facial expressions. Here aresome examples. depicts a happy expression. represents a sad expression. portrays a neutral expression.Now, identify the facial expression of this individual: \"\"\"agent = MultimodalConversableAgent()user = UserProxyAgent()user.initiate_chat(agent, message=prompt)",
        "segment_23": "The MultimodalConversableAgent interprets the input prompt, extracting images from local or internet sources.",
        "segment_24": "Advanced Usage\u200b",
        "segment_25": "Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.",
        "segment_26": "For example, the FigureCreator in our GPT-4V notebook and LLaVA notebook integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).",
        "segment_27": "The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.",
        "segment_28": "With human_input_mode=ALWAYS, you can also contribute suggestions for better visualizations.",
        "segment_29": "Reference\u200b",
        "segment_31": "GPT-4V System Card",
        "segment_32": "LLaVA GitHub",
        "segment_34": "Future Enhancements\u200b",
        "segment_35": "For further inquiries or suggestions, please open an issue in the AutoGen repository or contact me directly at beibin.li@microsoft.com.",
        "segment_36": "AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.Tags:LMMmultimodalNewer PostEcoAssistant - Using LLM Assistants More Accurately and AffordablyOlder PostAutoGen's Teachable AgentsIntroductionInstallationUsageAdvanced UsageReferenceFuture EnhancementsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TaskingAI is an AI-native application development platform that unifies modules like Model, Retrieval, Assistant, and Tool into one seamless ecosystem, streamlining the creation and deployment of applications for developers.",
        "segment_3": "Resources\u200b",
        "segment_5": "Explore the GitHub Repo",
        "segment_6": "Read the API Reference",
        "segment_8": "Key Concepts\u200b",
        "segment_9": "Project\u200b",
        "segment_10": "Projects in TaskingAI are organizational units designed to group related activities and resources. They offer a structured way to manage different initiatives or brands, allowing for clear segregation and management. Each project can be tailored with specific settings and resources, ensuring that the information and activities within one project remain distinct and isolated from others.",
        "segment_11": "Model\u200b",
        "segment_12": "TaskingAI incorporates a variety of chat completion models, each with distinct capabilities and attributes. These models serve as the core 'brains' of AI assistants, providing them with reasoning and logical capabilities. TaskingAI supports models from multiple providers, each offering different strengths in terms of input token limits, reasoning, and logic capabilities. Users can select and switch between models based on their specific needs and the complexity of the tasks at hand.",
        "segment_13": "Retrieval\u200b",
        "segment_14": "Retrievals in TaskingAI are mechanisms that enable AI assistants to access and utilize external knowledge bases. This feature allows the integration of additional information into the AI's responses, enhancing its ability to provide accurate and context-relevant answers. Retrievals are crucial for tasks that require specific, detailed, or up-to-date information, ensuring that the AI's responses are not limited by its pre-training data.",
        "segment_15": "Assistant\u200b",
        "segment_16": "The Assistant feature in TaskingAI refers to the AI entities capable of performing a wide range of tasks. These assistants are customizable and can be tailored to suit various applications, from customer service to internal training. They operate based on the models and tools provided, and their functionality can be extended through the use of retrievals, allowing them to access a broader range of information and capabilities.",
        "segment_17": "Tool\u200b",
        "segment_18": "Tools in TaskingAI are functionalities that enable AI assistants to interact with external resources and perform specific actions, such as fetching live information or communicating with external systems. These tools are typically defined in OpenAPI schema format and can be attached to assistants to enhance their capabilities. Tools are essential for tasks that require real-time data or interaction with external APIs and services, making the assistants more dynamic and versatile in their operation.",
        "segment_20": "TaskingAI is more than just a platform; it's a gateway to unlocking the full potential of AI in your daily tasks. Whether you're a developer, a researcher, or someone looking to streamline their workflow, TaskingAI offers the tools and resources to achieve your goals.",
        "segment_21": "infoAt TaskingAI, we prioritize user data protection and ensure that our models are developed with the utmost consideration for privacy and ethical standards. All customer data is encrypted at rest (AES-256) and in transit (TLS 1.2+).NextQuickstartResourcesKey ConceptsProjectModelRetrievalAssistantToolAll Copyright Reserved \u00a9 2024 TaskingAI"
    },
    {
        "segment_1": "Create a virtual environment (optional)\u200b",
        "segment_2": "When installing AutoGen locally, we recommend using a virtual environment for the installation. This will ensure that the dependencies for AutoGen are isolated from the rest of your system.",
        "segment_3": "venvCondaPoetryCreate and activate:python3 -m venv pyautogensource pyautogen/bin/activateTo deactivate later, run:deactivateInstall Conda if you have not already.Create and activate:conda create -n pyautogen python=3.10conda activate pyautogenTo deactivate later, run:conda deactivateInstall Poetry if you have not already.Create and activate:poetry initpoetry shellpoetry add pyautogenTo deactivate later, run:exit",
        "segment_4": "Install AutoGen\u200b",
        "segment_5": "AutoGen requires Python version >= 3.8, < 3.13. It can be installed from pip:",
        "segment_6": "pip install pyautogen",
        "segment_7": "infopyautogen=1 is required.",
        "segment_8": "Code execution with Docker (default)\u200b",
        "segment_9": "Even if you install AutoGen locally, we highly recommend using Docker for code execution.",
        "segment_10": "The default behaviour for code-execution agents is for code execution to be performed in a docker container.",
        "segment_11": "To turn this off: if you want to run the code locally (not recommended) then use_docker can be set to False in code_execution_config for each code-execution agent, or set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_12": "You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.:",
        "segment_13": "user_proxy = autogen.UserProxyAgent( name=\"agent\", human_input_mode=\"TERMINATE\", max_consecutive_auto_reply=10, code_execution_config={\"work_dir\":\"_output\", \"use_docker\":\"python:3\"}, llm_config=llm_config, system_message=\"\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")",
        "segment_14": "Turn off code execution entirely: if you want to turn off code execution entirely, set code_execution_config to False. E.g.:",
        "segment_15": "user_proxy = autogen.UserProxyAgent( name=\"agent\", llm_config=llm_config, code_execution_config=False,)Edit this pagePreviousGetting StartedNextDockerCreate a virtual environment (optional)Install AutoGenCode execution with Docker (default)CommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessNewer PostAutoGen Meets GPTsOlder PostMultimodal with GPT-4V and LLaVAEcoAssistantProblem setupLeveraging external APIsSolution DemonstrationAssistant HierarchyA Synergistic EffectExperimental ResultsFurther readingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def content_str(content: Union[str, List, None]) -> str",
        "segment_2": "Converts content into a string format.",
        "segment_3": "This function processes content that may be a string, a list of mixed text and image URLs, or None,",
        "segment_4": "and converts it into a string. Text is directly appended to the result string, while image URLs are",
        "segment_5": "represented by a placeholder image token. If the content is None, an empty string is returned.",
        "segment_6": "Arguments:",
        "segment_8": "content (Union[str, List, None]): The content to be processed. Can be a string, a list of dictionaries",
        "segment_9": "representing text and image URLs, or None.",
        "segment_11": "Returns:",
        "segment_13": "str - A string representation of the input content. Image URLs are replaced with an image token.",
        "segment_15": "Notes:",
        "segment_17": "The function expects each dictionary in the list to have a \"type\" key that is either \"text\" or \"image_url\".",
        "segment_18": "For \"text\" type, the \"text\" key's value is appended to the result. For \"image_url\", an image token is appended.",
        "segment_19": "This function is useful for handling content that may include both text and image references, especially",
        "segment_20": "in contexts where images need to be represented as placeholders.",
        "segment_22": "infer_lang\u200b",
        "segment_23": "def infer_lang(code)",
        "segment_24": "infer the language for the code.",
        "segment_25": "TODO: make it robust.",
        "segment_26": "extract_code\u200b",
        "segment_27": "def extract_code( text: Union[str, List], pattern: str = CODE_BLOCK_PATTERN, detect_single_line_code: bool = False) -> List[Tuple[str, str]]",
        "segment_28": "Extract code from a text.",
        "segment_29": "Arguments:",
        "segment_31": "text str or List - The content to extract code from. The content can be",
        "segment_32": "a string or a list, as returned by standard GPT or multimodal GPT.",
        "segment_33": "pattern str, optional - The regular expression pattern for finding the",
        "segment_34": "code block. Defaults to CODE_BLOCK_PATTERN.",
        "segment_35": "detect_single_line_code bool, optional - Enable the new feature for",
        "segment_36": "extracting single line code. Defaults to False.",
        "segment_38": "Returns:",
        "segment_40": "list - A list of tuples, each containing the language and the code.",
        "segment_41": "If there is no code block in the input text, the language would be \"unknown\".",
        "segment_42": "If there is code block but the language is not specified, the language would be \"\".",
        "segment_44": "generate_code\u200b",
        "segment_45": "def generate_code(pattern: str = CODE_BLOCK_PATTERN, **config) -> Tuple[str, float]",
        "segment_46": "(openai<1) Generate code.",
        "segment_47": "Arguments:",
        "segment_49": "pattern Optional, str - The regular expression pattern for finding the code block.",
        "segment_50": "The default pattern is for finding a code block in a markdown file.",
        "segment_51": "config Optional, dict - The configuration for the API call.",
        "segment_53": "Returns:",
        "segment_55": "str - The generated code.",
        "segment_56": "float - The cost of the generation.",
        "segment_58": "improve_function\u200b",
        "segment_59": "def improve_function(file_name, func_name, objective, **config)",
        "segment_60": "(openai<1) Improve the function to achieve the objective.",
        "segment_61": "improve_code\u200b",
        "segment_62": "def improve_code(files, objective, suggest_only=True, **config)",
        "segment_63": "(openai<1) Improve the code to achieve a given objective.",
        "segment_64": "Arguments:",
        "segment_66": "files list - A list of file names containing the source code.",
        "segment_67": "objective str - The objective to achieve.",
        "segment_68": "suggest_only bool - Whether to return only the suggestions or the improved code.",
        "segment_69": "config Optional, dict - The configuration for the API call.",
        "segment_71": "Returns:",
        "segment_73": "str - The improved code if suggest_only=False; a list of suggestions if suggest_only=True (default).",
        "segment_74": "float - The cost of the generation.",
        "segment_76": "is_docker_running\u200b",
        "segment_77": "def is_docker_running()",
        "segment_78": "Check if docker is running.",
        "segment_79": "Returns:",
        "segment_81": "bool - True if docker is running; False otherwise.",
        "segment_83": "in_docker_container\u200b",
        "segment_84": "def in_docker_container()",
        "segment_85": "Check if the code is running in a docker container.",
        "segment_86": "Returns:",
        "segment_88": "bool - True if the code is running in a docker container; False otherwise.",
        "segment_90": "execute_code\u200b",
        "segment_91": "def execute_code(code: Optional[str] = None, timeout: Optional[int] = None, filename: Optional[str] = None, work_dir: Optional[str] = None, use_docker: Union[List[str], str, bool] = SENTINEL, lang: Optional[str] = \"python\") -> Tuple[int, str, str]",
        "segment_92": "Execute code in a docker container.",
        "segment_93": "This function is not tested on MacOS.",
        "segment_94": "Arguments:",
        "segment_96": "code Optional, str - The code to execute.",
        "segment_97": "If None, the code from the file specified by filename will be executed.",
        "segment_98": "Either code or filename must be provided.",
        "segment_99": "timeout Optional, int - The maximum execution time in seconds.",
        "segment_100": "If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.",
        "segment_101": "filename Optional, str - The file name to save the code or where the code is stored when code is None.",
        "segment_102": "If None, a file with a randomly generated name will be created.",
        "segment_103": "The randomly generated file will be deleted after execution.",
        "segment_104": "The file name must be a relative path. Relative paths are relative to the working directory.",
        "segment_105": "work_dir Optional, str - The working directory for the code execution.",
        "segment_106": "If None, a default working directory will be used.",
        "segment_107": "The default working directory is the \"extensions\" directory under",
        "segment_108": "\"path_to_autogen\".",
        "segment_109": "use_docker list, str or bool - The docker image to use for code execution.",
        "segment_110": "Default is True, which means the code will be executed in a docker container. A default list of images will be used.",
        "segment_111": "If a list or a str of image name(s) is provided, the code will be executed in a docker container",
        "segment_112": "with the first image successfully pulled.",
        "segment_113": "If False, the code will be executed in the current environment.",
        "segment_114": "Expected behaviour:",
        "segment_116": "If use_docker is not set (i.e. left default to True) or is explicitly set to True and the docker package is available, the code will run in a Docker container.",
        "segment_117": "If use_docker is not set (i.e. left default to True) or is explicitly set to True but the Docker package is missing or docker isn't running, an error will be raised.",
        "segment_118": "If use_docker is explicitly set to False, the code will run natively.",
        "segment_119": "If the code is executed in the current environment,",
        "segment_120": "the code must be trusted.",
        "segment_123": "lang Optional, str - The language of the code. Default is \"python\".",
        "segment_125": "Returns:",
        "segment_127": "timeout0 - 0 if the code executes successfully.",
        "segment_128": "timeout1 - The error message if the code fails to execute; the stdout otherwise.",
        "segment_129": "timeout2 - The docker image name after container run when docker is used.",
        "segment_131": "generate_assertions\u200b",
        "segment_132": "def generate_assertions(definition: str, **config) -> Tuple[str, float]",
        "segment_133": "(openai<1) Generate assertions for a function.",
        "segment_134": "Arguments:",
        "segment_136": "definition str - The function definition, including the signature and docstr.",
        "segment_137": "config Optional, dict - The configuration for the API call.",
        "segment_139": "Returns:",
        "segment_141": "str - The generated assertions.",
        "segment_142": "float - The cost of the generation.",
        "segment_144": "eval_function_completions\u200b",
        "segment_145": "def eval_function_completions(responses: List[str], definition: str, test: Optional[str] = None, entry_point: Optional[str] = None, assertions: Optional[Union[str, Callable[ [str], Tuple[str, float]]]] = None, timeout: Optional[float] = 3, use_docker: Optional[bool] = True) -> Dict",
        "segment_146": "(openai<1) Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.",
        "segment_147": "Arguments:",
        "segment_149": "responses list - The list of responses.",
        "segment_150": "definition str - The input definition.",
        "segment_151": "test Optional, str - The test code.",
        "segment_152": "entry_point Optional, str - The name of the function.",
        "segment_153": "assertions Optional, str or Callable - The assertion code which serves as a filter of the responses, or an assertion generator.",
        "segment_154": "When provided, only the responses that pass the assertions will be considered for the actual test (if provided).",
        "segment_155": "timeout Optional, float - The timeout for executing the code.",
        "segment_157": "Returns:",
        "segment_159": "dict - The success metrics.",
        "segment_161": "PassAssertionFilter Objects\u200b",
        "segment_162": "class PassAssertionFilter()",
        "segment_163": "pass_assertions\u200b",
        "segment_164": "def pass_assertions(context, response, **_)",
        "segment_165": "(openai<1) Check if the response passes the assertions.",
        "segment_166": "implement\u200b",
        "segment_167": "def implement( definition: str, configs: Optional[List[Dict]] = None, assertions: Optional[Union[str, Callable[[str], Tuple[str, float]]]] = generate_assertions) -> Tuple[str, float]",
        "segment_168": "(openai<1) Implement a function from a definition.",
        "segment_169": "Arguments:",
        "segment_171": "definition str - The function definition, including the signature and docstr.",
        "segment_172": "configs list - The list of configurations for completion.",
        "segment_173": "assertions Optional, str or Callable - The assertion code which serves as a filter of the responses, or an assertion generator.",
        "segment_175": "Returns:",
        "segment_177": "str - The implementation.",
        "segment_178": "float - The cost of the implementation.",
        "segment_179": "int - The index of the configuration which generates the implementation.",
        "segment_180": "Edit this pagePreviousbrowser_utilsNextfunction_utilsPassAssertionFilter ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def gather_usage_summary( agents: List[Agent]) -> Tuple[Dict[str, any], Dict[str, any]]",
        "segment_2": "Gather usage summary from all agents.",
        "segment_3": "Arguments:",
        "segment_5": "agents - (list): List of agents.",
        "segment_7": "Returns:",
        "segment_10": "tuple - (total_usage_summary, actual_usage_summary)",
        "segment_11": "Example return:",
        "segment_12": "total_usage_summary = {",
        "segment_15": "'total_cost' - 0.0006090000000000001,",
        "segment_16": "'gpt-35-turbo':",
        "segment_17": "{",
        "segment_20": "'cost' - 0.0006090000000000001,",
        "segment_23": "'prompt_tokens' - 242,",
        "segment_26": "'completion_tokens' - 123,",
        "segment_29": "'total_tokens' - 365",
        "segment_30": "}",
        "segment_31": "}",
        "segment_32": "actual_usage_summary follows the same format.",
        "segment_33": "If none of the agents incurred any cost (not having a client), then the total_usage_summary and actual_usage_summary will be {'total_cost': 0}.",
        "segment_35": "Edit this pagePreviousopenai_utilsNextbrowser_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name=\"RetrieveChatAgent\", human_input_mode: Optional[str] = \"ALWAYS\", is_termination_msg: Optional[Callable[[Dict], bool]] = None, retrieve_config: Optional[Dict] = None, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - name of the agent.",
        "segment_7": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_8": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_9": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_10": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_11": "or when is_termination_msg is True and there is no human input.",
        "segment_12": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_13": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_14": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_15": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_16": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_17": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_18": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_19": "retrieve_config dict or None - config for the retrieve agent.",
        "segment_20": "To use default config, set to None. Otherwise, set to a dictionary with the following keys:",
        "segment_22": "task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System",
        "segment_23": "prompt will be different for different tasks. The default value is default, which supports both code and qa.",
        "segment_24": "client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.",
        "segment_25": "will be used. If you want to use other vector db, extend this class and override the retrieve_docs function.",
        "segment_26": "docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file,",
        "segment_27": "the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.",
        "segment_28": "extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite.,",
        "segment_29": "when set to true it enables the system to assign unique IDs starting from \"length+i\" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..",
        "segment_30": "By default, \"extra_docs\" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.",
        "segment_31": "collection_name (Optional, str): the name of the collection.",
        "segment_32": "If key not provided, a default name autogen-docs will be used.",
        "segment_33": "model (Optional, str): the model to use for the retrieve chat.",
        "segment_34": "If key not provided, a default model gpt-4 will be used.",
        "segment_35": "chunk_token_size (Optional, int): the chunk token size for the retrieve chat.",
        "segment_36": "If key not provided, a default size max_tokens * 0.4 will be used.",
        "segment_37": "context_max_tokens (Optional, int): the context max token size for the retrieve chat.",
        "segment_38": "If key not provided, a default size max_tokens * 0.8 will be used.",
        "segment_39": "chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are",
        "segment_40": "\"multi_lines\" and \"one_line\". If key not provided, a default mode human_input_mode0 will be used.",
        "segment_41": "must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.",
        "segment_42": "If chunk_mode is \"one_line\", this parameter will be ignored.",
        "segment_43": "embedding_model (Optional, str): the embedding model to use for the retrieve chat.",
        "segment_44": "If key not provided, a default model human_input_mode1 will be used. All available models",
        "segment_45": "can be found at human_input_mode2.",
        "segment_46": "customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.",
        "segment_47": "customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".",
        "segment_48": "If not \"\" and the customized_answer_prefix is not in the answer, human_input_mode3 will be triggered.",
        "segment_49": "update_context (Optional, bool): if False, will not apply human_input_mode3 for interactive retrieval. Default is True.",
        "segment_50": "custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.",
        "segment_51": "The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).",
        "segment_52": "Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.",
        "segment_53": "custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.",
        "segment_54": "Default is None, will use the default function in human_input_mode5.",
        "segment_55": "custom_text_types (Optional, List[str]): a list of file types to be processed. Default is human_input_mode6.",
        "segment_56": "This only applies to files under the directories in human_input_mode7. Explicitly included files and urls will be chunked regardless of their types.",
        "segment_57": "recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.",
        "segment_58": "parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.",
        "segment_59": "on_disk (Optional, bool): Whether to store the collection on disk. Default is False.",
        "segment_60": "quantization_config: Quantization configuration. If None, quantization will be disabled.",
        "segment_61": "hnsw_config: HNSW configuration. If None, default configuration will be used.",
        "segment_62": "You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/`human_input_mode`8-index.",
        "segment_63": "API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_64": "payload_indexing: Whether to create a payload index for the document field. Default is False.",
        "segment_65": "You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/`human_input_mode`9-index",
        "segment_66": "API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index",
        "segment_69": "is_termination_msg0 dict - other kwargs in UserProxyAgent.",
        "segment_71": "retrieve_docs\u200b",
        "segment_72": "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")",
        "segment_73": "Arguments:",
        "segment_75": "problem str - the problem to be solved.",
        "segment_76": "n_results int - the number of results to be retrieved. Default is 20.",
        "segment_77": "search_string str - only docs that contain an exact match of this string will be retrieved. Default is \"\".",
        "segment_79": "create_qdrant_from_dir\u200b",
        "segment_80": "def create_qdrant_from_dir( dir_path: str, max_tokens: int = 4000, client: QdrantClient = None, collection_name: str = \"all-my-documents\", chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, embedding_model: str = \"BAAI/bge-small-en-v1.5\", custom_text_split_function: Callable = None, custom_text_types: List[str] = TEXT_FORMATS, recursive: bool = True, extra_docs: bool = False, parallel: int = 0, on_disk: bool = False, quantization_config: Optional[models.QuantizationConfig] = None, hnsw_config: Optional[models.HnswConfigDiff] = None, payload_indexing: bool = False, qdrant_client_options: Optional[Dict] = {})",
        "segment_81": "Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a",
        "segment_82": "url to a single file.",
        "segment_83": "Arguments:",
        "segment_85": "dir_path str - the path to the directory, file or url.",
        "segment_86": "max_tokens Optional, int - the maximum number of tokens per chunk. Default is 4000.",
        "segment_87": "client Optional, QdrantClient - the QdrantClient instance. Default is None.",
        "segment_88": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_89": "chunk_mode Optional, str - the chunk mode. Default is \"multi_lines\".",
        "segment_90": "must_break_at_empty_line Optional, bool - Whether to break at empty line. Default is True.",
        "segment_91": "embedding_model Optional, str - the embedding model to use. Default is \"BAAI/bge-small-en-v1.5\".",
        "segment_92": "The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.",
        "segment_93": "custom_text_split_function Optional, Callable - a custom function to split a string into a list of strings.",
        "segment_94": "Default is None, will use the default function in autogen.retrieve_utils.split_text_to_chunks.",
        "segment_95": "custom_text_types Optional, List[str] - a list of file types to be processed. Default is TEXT_FORMATS.",
        "segment_96": "max_tokens0 Optional, bool - whether to search documents recursively in the dir_path. Default is True.",
        "segment_97": "max_tokens1 Optional, bool - whether to add more documents in the collection. Default is False",
        "segment_98": "max_tokens2 Optional, int - How many parallel workers to use for embedding. Defaults to the number of CPU cores",
        "segment_99": "max_tokens3 Optional, bool - Whether to store the collection on disk. Default is False.",
        "segment_100": "max_tokens4 - Quantization configuration. If None, quantization will be disabled.",
        "segment_101": "max_tokens5 - https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_102": "max_tokens6 - HNSW configuration. If None, default configuration will be used.",
        "segment_103": "max_tokens5 - https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_104": "max_tokens8 - Whether to create a payload index for the document field. Default is False.",
        "segment_105": "max_tokens9 - (Optional, dict): the options for instantiating the qdrant client.",
        "segment_106": "max_tokens5 - https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.",
        "segment_108": "query_qdrant\u200b",
        "segment_109": "def query_qdrant( query_texts: List[str], n_results: int = 10, client: QdrantClient = None, collection_name: str = \"all-my-documents\", search_string: str = \"\", embedding_model: str = \"BAAI/bge-small-en-v1.5\", qdrant_client_options: Optional[Dict] = {}) -> List[List[QueryResponse]]",
        "segment_110": "Perform a similarity search with filters on a Qdrant collection",
        "segment_111": "Arguments:",
        "segment_113": "query_texts List[str] - the query texts.",
        "segment_114": "n_results Optional, int - the number of results to return. Default is 10.",
        "segment_115": "client Optional, API - the QdrantClient instance. A default in-memory client will be instantiated if None.",
        "segment_116": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_117": "search_string Optional, str - the search string. Default is \"\".",
        "segment_118": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if embedding_function is not None.",
        "segment_119": "qdrant_client_options - (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.",
        "segment_121": "Returns:",
        "segment_123": "List[List[QueryResponse]] - the query result. The format is:",
        "segment_124": "class QueryResponse(BaseModel, extra=\"forbid\"): # type: ignore",
        "segment_125": "id - Union[str, int]",
        "segment_126": "embedding - Optional[List[float]]",
        "segment_127": "n_results0 - Dict[str, Any]",
        "segment_128": "n_results1 - str",
        "segment_129": "n_results2 - float"
    },
    {
        "segment_1": "class AbstractCache(ABC)",
        "segment_2": "Abstract base class for cache implementations.",
        "segment_3": "This class defines the basic interface for cache operations.",
        "segment_4": "Implementing classes should provide concrete implementations for",
        "segment_5": "these methods to handle caching mechanisms.",
        "segment_6": "get\u200b",
        "segment_7": "@abstractmethoddef get(key, default=None)",
        "segment_8": "Retrieve an item from the cache.",
        "segment_9": "Abstract method that must be implemented by subclasses to",
        "segment_10": "retrieve an item from the cache.",
        "segment_11": "Arguments:",
        "segment_13": "key str - The key identifying the item in the cache.",
        "segment_14": "default optional - The default value to return if the key is not found.",
        "segment_15": "Defaults to None.",
        "segment_17": "Returns:",
        "segment_18": "The value associated with the key if found, else the default value.",
        "segment_19": "Raises:",
        "segment_21": "NotImplementedError - If the subclass does not implement this method.",
        "segment_23": "set\u200b",
        "segment_24": "@abstractmethoddef set(key, value)",
        "segment_25": "Set an item in the cache.",
        "segment_26": "Abstract method that must be implemented by subclasses to",
        "segment_27": "store an item in the cache.",
        "segment_28": "Arguments:",
        "segment_30": "key str - The key under which the item is to be stored.",
        "segment_31": "value - The value to be stored in the cache.",
        "segment_33": "Raises:",
        "segment_35": "NotImplementedError - If the subclass does not implement this method.",
        "segment_37": "close\u200b",
        "segment_38": "@abstractmethoddef close()",
        "segment_39": "Close the cache.",
        "segment_40": "Abstract method that should be implemented by subclasses to",
        "segment_41": "perform any necessary cleanup, such as closing network connections or",
        "segment_42": "releasing resources.",
        "segment_43": "Raises:",
        "segment_45": "NotImplementedError - If the subclass does not implement this method.",
        "segment_47": "__enter__\u200b",
        "segment_48": "@abstractmethoddef __enter__()",
        "segment_49": "Enter the runtime context related to this object.",
        "segment_50": "The with statement will bind this method\u2019s return value to the target(s)",
        "segment_51": "specified in the as clause of the statement, if any.",
        "segment_52": "Raises:",
        "segment_54": "NotImplementedError - If the subclass does not implement this method.",
        "segment_56": "__exit__\u200b",
        "segment_57": "@abstractmethoddef __exit__(exc_type, exc_value, traceback)",
        "segment_58": "Exit the runtime context and close the cache.",
        "segment_59": "Abstract method that should be implemented by subclasses to handle",
        "segment_60": "the exit from a with statement. It is responsible for resource",
        "segment_61": "release and cleanup.",
        "segment_62": "Arguments:",
        "segment_64": "exc_type - The exception type if an exception was raised in the context.",
        "segment_65": "exc_value - The exception value if an exception was raised in the context.",
        "segment_66": "traceback - The traceback if an exception was raised in the context.",
        "segment_68": "Raises:",
        "segment_70": "NotImplementedError - If the subclass does not implement this method.",
        "segment_71": "Edit this pagePrevioususer_proxy_agentNextcacheAbstractCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen now supports custom models! This feature empowers users to define and load their own models, allowing for a more flexible and personalized inference mechanism. By adhering to a specific protocol, you can integrate your custom model for use with AutoGen and respond to prompts any way needed by using any model/API call/hardcoded response you want.",
        "segment_2": "NOTE: Depending on what model you use, you may need to play with the default prompts of the Agent's",
        "segment_3": "Quickstart\u200b",
        "segment_4": "An interactive and easy way to get started is by following the notebook here which loads a local model from HuggingFace into AutoGen and uses it for inference, and making changes to the class provided.",
        "segment_5": "Step 1: Create the custom model client class\u200b",
        "segment_6": "To get started with using custom models in AutoGen, you need to create a model client class that adheres to the ModelClient protocol defined in client.py. The new model client class should implement these methods:",
        "segment_8": "create(): Returns a response object that implements the ModelClientResponseProtocol (more details in the Protocol section).",
        "segment_9": "message_retrieval(): Processes the response object and returns a list of strings or a list of message objects (more details in the Protocol section).",
        "segment_10": "cost(): Returns the cost of the response.",
        "segment_11": "get_usage(): Returns a dictionary with keys from RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"].",
        "segment_13": "E.g. of a bare bones dummy custom class:",
        "segment_14": "class CustomModelClient: def __init__(self, config, **kwargs): print(f\"CustomModelClient config: {config}\") def create(self, params): num_of_responses = params.get(\"n\", 1) # can create my own data response class # here using SimpleNamespace for simplicity # as long as it adheres to the ModelClientResponseProtocol response = SimpleNamespace() response.choices = [] response.model = \"model_name\" # should match the OAI_CONFIG_LIST registration for _ in range(num_of_responses): text = \"this is a dummy text response\" choice = SimpleNamespace() choice.message = SimpleNamespace() choice.message.content = text choice.message.function_call = None response.choices.append(choice) return response def message_retrieval(self, response): choices = response.choices return [choice.message.content for choice in choices] def cost(self, response) -> float: response.cost = 0 return 0 @staticmethod def get_usage(response): return {}",
        "segment_15": "Step 2: Add the configuration to the OAI_CONFIG_LIST\u200b",
        "segment_16": "The field that is necessary is setting model_client_cls to the name of the new class (as a string) \"model_client_cls\":\"CustomModelClient\". Any other fields will be forwarded to the class constructor, so you have full control over what parameters to specify and how to use them. E.g.:",
        "segment_17": "{ \"model\": \"Open-Orca/Mistral-7B-OpenOrca\", \"model_client_cls\": \"CustomModelClient\", \"device\": \"cuda\", \"n\": 1, \"params\": { \"max_length\": 1000, }}",
        "segment_18": "Step 3: Register the new custom model to the agent that will use it\u200b",
        "segment_19": "If a configuration with the field \"model_client_cls\":\"\" has been added to an Agent's config list, then the corresponding model with the desired class must be registered after the agent is created and before the conversation is initialized:",
        "segment_20": "my_agent.register_model_client(model_client_cls=CustomModelClient, [other args that will be forwarded to CustomModelClient constructor])",
        "segment_21": "model_client_cls=CustomModelClient arg matches the one specified in the OAI_CONFIG_LIST and CustomModelClient is the class that adheres to the ModelClient protocol (more details on the protocol below).",
        "segment_22": "If the new model client is in the config list but not registered by the time the chat is initialized, then an error will be raised.",
        "segment_23": "Protocol details\u200b",
        "segment_24": "A custom model class can be created in many ways, but needs to adhere to the ModelClient protocol and response structure which is defined in client.py and shown below.",
        "segment_25": "The response protocol is currently using the minimum required fields from the autogen codebase that match the OpenAI response structure. Any response protocol that matches the OpenAI response structure will probably be more resilient to future changes, but we are starting off with minimum requirements to make adpotion of this feature easier.",
        "segment_26": "class ModelClient(Protocol): \"\"\" A client class must implement the following methods: - create must return a response object that implements the ModelClientResponseProtocol - cost must return the cost of the response - get_usage must return a dict with the following keys: - prompt_tokens - completion_tokens - total_tokens - cost - model This class is used to create a client that can be used by OpenAIWrapper. The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed. The message_retrieval method must be implemented to return a list of str or a list of messages from the response. \"\"\" RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"] class ModelClientResponseProtocol(Protocol): class Choice(Protocol): class Message(Protocol): content: Optional[str] message: Message choices: List[Choice] model: str def create(self, params) -> ModelClientResponseProtocol: ... def message_retrieval( self, response: ModelClientResponseProtocol ) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]: \"\"\" Retrieve and return a list of strings or a list of Choice.Message from the response. NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used. \"\"\" ... def cost(self, response: ModelClientResponseProtocol) -> float: ... @staticmethod def get_usage(response: ModelClientResponseProtocol) -> Dict: \"\"\"Return usage summary of the response using RESPONSE_USAGE_KEYS.\"\"\" ...",
        "segment_27": "Troubleshooting steps\u200b",
        "segment_28": "If something doesn't work then run through the checklist:",
        "segment_30": "Make sure you have followed the client protocol and client response protocol when creating the custom model class",
        "segment_32": "create() method: ModelClientResponseProtocol must be followed when returning an inference response during create call.",
        "segment_33": "message_retrieval() method: returns a list of strings or a list of message objects. If a list of message objects is returned, they currently must contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.",
        "segment_34": "cost()method: returns an integer, and if you don't care about cost tracking you can just return 0.",
        "segment_35": "get_usage(): returns a dictionary, and if you don't care about usage tracking you can just return an empty dictionary {}.",
        "segment_38": "Make sure you have a corresponding entry in the OAI_CONFIG_LIST and that that entry has the \"model_client_cls\":\"\" field.",
        "segment_39": "Make sure you have registered the client using the corresponding config entry and your new class agent.register_model_client(model_client_cls=, [other optional args])",
        "segment_40": "Make sure that all of the custom models defined in the OAI_CONFIG_LIST have been registered.",
        "segment_41": "Any other troubleshooting might need to be done in the custom code itself.",
        "segment_43": "Conclusion\u200b",
        "segment_44": "With the ability to use custom models, AutoGen now offers even more flexibility and power for your AI applications. Whether you've trained your own model or want to use a specific pre-trained model, AutoGen can accommodate your needs. Happy coding!Tags:AutoGenAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsJanuary 25, 2024 \u00b7 7 min readAdam FourneyPrincipal Researcher Microsoft ResearchQingyun WuAssistant Professor at the Pennsylvania State University",
        "segment_45": "AutoGenBench is a standalone tool for evaluating AutoGen agents and workflows on common benchmarks.",
        "segment_46": "TLDR\u200b",
        "segment_47": "Today we are releasing AutoGenBench \u2013 a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.",
        "segment_48": "AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.",
        "segment_50": "See the AutoGenBench README for information on installation and running benchmarks.",
        "segment_51": "See the AutoGenBench CONTRIBUTING guide for information on developing or contributing benchmark datasets.",
        "segment_53": "Quick Start\u200b",
        "segment_54": "Get started quickly by running the following commands in a bash terminal.",
        "segment_55": "Note: You may need to adjust the path to the OAI_CONFIG_LIST, as appropriate.",
        "segment_56": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)pip install autogenbenchautogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate Results/human_eval_two_agents",
        "segment_57": "Introduction\u200b",
        "segment_58": "Measurement and evaluation are core components of every major AI or ML research project. The same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to AgentEval. In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluation; and conclude with an open call for contributions.",
        "segment_59": "Design Principles\u200b",
        "segment_60": "AutoGenBench is designed around three core design principles. Knowing these principles will help you understand the tool, its operation and its output. These three principles are:",
        "segment_63": "Repetition: LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.",
        "segment_66": "Isolation: Agents interact with their worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to ordering effects that can impact future measurements. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in its own Docker container. This ensures that all runs start with the same initial conditions. (Docker is also a much safer way to run agent-produced code, in general.)",
        "segment_69": "Instrumentation: While top-line metrics are great for comparing agents or models, we often want much more information about how the agents are performing, where they are getting stuck, and how they can be improved. We may also later think of new research questions that require computing a different set of metrics. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like AgentEval.",
        "segment_72": "Installing and Running AutoGenBench\u200b",
        "segment_73": "As noted above, isolation is a key design principle, and so AutoGenBench must be run in an environment where Docker is available (desktop or Engine). It will not run in GitHub codespaces, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see https://www.docker.com/products/docker-desktop/.",
        "segment_74": "Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With pip, installation can be achieved as follows:",
        "segment_75": "pip install autogenbench",
        "segment_76": "After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter.",
        "segment_77": "If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:",
        "segment_78": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)",
        "segment_79": "A Typical Session\u200b",
        "segment_80": "Once AutoGenBench and necessary keys are installed, a typical session will look as follows:",
        "segment_81": "autogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate results/human_eval_two_agents",
        "segment_82": "Where:",
        "segment_84": "autogenbench clone HumanEval downloads and expands the HumanEval benchmark scenario.",
        "segment_85": "cd HumanEval; cat README.md navigates to the benchmark directory, and prints the README (which you should always read!)",
        "segment_86": "autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl",
        "segment_87": "runs a 10% subsample of the tasks defined in Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.",
        "segment_88": "autogenbench tabulate results/human_eval_two_agents tabulates the results of the run.",
        "segment_90": "After running the above tabulate command, you should see output similar to the following:",
        "segment_91": "Trial 0 Trial 1 Trial 2Task Id Success Success Success------------- --------- --------- ---------HumanEval_107 False True TrueHumanEval_22 True True TrueHumanEval_43 True True TrueHumanEval_88 True True TrueHumanEval_14 True True TrueHumanEval_157 True True TrueHumanEval_141 True True TrueHumanEval_57 True True TrueHumanEval_154 True True TrueHumanEval_153 True True TrueHumanEval_93 False True FalseHumanEval_137 True True TrueHumanEval_143 True True TrueHumanEval_13 True True TrueHumanEval_49 True True TrueHumanEval_95 True True True------------- --------- --------- ---------Successes 14 16 15Failures 2 0 1Missing 0 0 0Total 16 16 16CAUTION: 'autogenbench tabulate' is in early preview.Please do not cite these values in academic work without first inspecting and verifying the results in the logs yourself.",
        "segment_92": "From this output we can see the results of the three separate repetitions of each task, and final summary statistics of each run. In this case, the results were generated via GPT-4 (as defined in the OAI_CONFIG_LIST that was provided), and used the TwoAgents template. It is important to remember that AutoGenBench evaluates specific end-to-end configurations of agents (as opposed to evaluating a model or cognitive framework more generally).",
        "segment_93": "Finally, complete execution traces and logs can be found in the Results folder. See the AutoGenBench README for more details about command-line options and output formats. Each of these commands also offers extensive in-line help via:",
        "segment_95": "autogenbench --help",
        "segment_96": "autogenbench clone --help",
        "segment_97": "autogenbench run --help",
        "segment_98": "autogenbench tabulate --help",
        "segment_100": "Roadmap\u200b",
        "segment_101": "While we are announcing AutoGenBench, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:",
        "segment_103": "Onboard many additional benchmarks beyond those shipping today",
        "segment_104": "Greatly improve logging and telemetry",
        "segment_105": "Introduce new core metrics including total costs, task completion time, conversation turns, etc.",
        "segment_106": "Provide tighter integration with AgentEval and AutoGen Studio",
        "segment_108": "For an up to date tracking of our work items on this project, please see AutoGenBench Work Items",
        "segment_109": "Call for Participation\u200b",
        "segment_110": "Finally, we want to end this blog post with an open call for contributions. AutoGenBench is still nascent, and has much opportunity for improvement. New benchmarks are constantly being published, and will need to be added. Everyone may have their own distinct set of metrics that they care most about optimizing, and these metrics should be onboarded. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the contributor\u2019s guide and join our Discord discussion in the #autogenbench channel!Tags:AutoGenCode execution is now by default inside docker containerJanuary 23, 2024 \u00b7 3 min readOlga VrousgouSenior Software Engineer at Microsoft ResearchTLDR\u200b",
        "segment_111": "AutoGen 0.2.8 enhances operational safety by making 'code execution inside a Docker container' the default setting, focusing on informing users about its operations and empowering them to make informed decisions regarding code execution.",
        "segment_112": "The new release introduces a breaking change where the use_docker argument is set to True by default in code executing agents. This change underscores our commitment to prioritizing security and safety in AutoGen.",
        "segment_113": "Introduction\u200b",
        "segment_114": "AutoGen has code-executing agents, usually defined as a UserProxyAgent, where code execution is by default ON. Until now, unless explicitly specified by the user, any code generated by other agents would be executed by code-execution agents locally, i.e. wherever AutoGen was being executed. If AutoGen happened to be run in a docker container then the risks of running code were minimized. However, if AutoGen runs outside of Docker, it's easy particularly for new users to overlook code-execution risks.",
        "segment_115": "AutoGen has now changed to by default execute any code inside a docker container (unless execution is already happening inside a docker container). It will launch a Docker image (either user-provided or default), execute the new code, and then terminate the image, preparing for the next code execution cycle.",
        "segment_116": "We understand that not everyone is concerned about this especially when playing around with AutoGen for the first time. We have provided easy ways to turn this requirement off. But we believe that making sure that the user is aware of the fact that code will be executed locally, and prompting them to think about the security implications of running code locally is the right step for AutoGen.",
        "segment_117": "Example\u200b",
        "segment_118": "The example shows the default behaviour which is that any code generated by assistant agent and executed by user_proxy agent, will attempt to use a docker container to execute the code. If docker is not running, it will throw an error. User can decide to activate docker or opt in for local code execution.",
        "segment_119": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_jsonassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")",
        "segment_120": "To opt out of from this default behaviour there are some options.",
        "segment_121": "Diasable code execution entirely\u200b",
        "segment_123": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_125": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config=False)",
        "segment_126": "Run code execution locally\u200b",
        "segment_128": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_129": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_131": "E.g.:",
        "segment_132": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})",
        "segment_133": "Related documentation\u200b",
        "segment_135": "Code execution with docker",
        "segment_136": "How to disable code execution in docker",
        "segment_138": "Conclusion\u200b",
        "segment_139": "AutoGen 0.2.8 now improves the code execution safety and is ensuring that the user is properly informed of what autogen is doing and can make decisions around code-execution.Tags:AutoGenAll About Agent DescriptionsDecember 29, 2023 \u00b7 9 min readAdam FourneyPrincipal Researcher Microsoft ResearchTLDR\u200b",
        "segment_140": "AutoGen 0.2.2 introduces a description field to ConversableAgent (and all subclasses), and changes GroupChat so that it uses agent descriptions rather than system_messages when choosing which agents should speak next.",
        "segment_141": "This is expected to simplify GroupChat\u2019s job, improve orchestration, and make it easier to implement new GroupChat or GroupChat-like alternatives.",
        "segment_142": "If you are a developer, and things were already working well for you, no action is needed -- backward compatibility is ensured because the description field defaults to the system_message when no description is provided.",
        "segment_143": "However, if you were struggling with getting GroupChat to work, you can now try updating the description field.",
        "segment_144": "Introduction\u200b",
        "segment_145": "As AutoGen matures and developers build increasingly complex combinations of agents, orchestration is becoming an important capability. At present, GroupChat and the GroupChatManager are the main built-in tools for orchestrating conversations between 3 or more agents. For orchestrators like GroupChat to work well, they need to know something about each agent so that they can decide who should speak and when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's system_message and name to learn about each participating agent. This is likely fine when the system prompt is short and sweet, but can lead to problems when the instructions are very long (e.g., with the AssistantAgent), or non-existent (e.g., with the UserProxyAgent).",
        "segment_146": "AutoGen 0.2.2 introduces a description field to all agents, and replaces the use of the system_message for orchestration in GroupChat and all future orchestrators. The description field defaults to the system_message to ensure backwards compatibility, so you may not need to change anything with your code if things are working well for you. However, if you were struggling with GroupChat, give setting the description field a try.",
        "segment_147": "The remainder of this post provides an example of how using the description field simplifies GroupChat's job, provides some evidence of its effectiveness, and provides tips for writing good descriptions.",
        "segment_148": "Example\u200b",
        "segment_149": "The current GroupChat orchestration system prompt has the following template:",
        "segment_150": "You are in a role play game. The following roles are available:{self._participant_roles(agents)}.Read the following conversation.Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.",
        "segment_151": "Suppose that you wanted to include 3 agents: A UserProxyAgent, an AssistantAgent, and perhaps a GuardrailsAgent.",
        "segment_152": "Prior to 0.2.2, this template would expand to:",
        "segment_153": "You are in a role play game. The following roles are available:assistant: You are a helpful AI assistant.Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply \"TERMINATE\" in the end when everything is done.user_proxy:guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:- You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.- You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.- You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.- You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.- You MUST TERMINATE the conversation if it involves seeking or giving advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice- You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.- You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.- You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.- You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.- You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word \"TERMINATE\". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_154": "As you can see, this description is super confusing:",
        "segment_156": "It is hard to make out where each agent's role-description ends",
        "segment_157": "You appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)",
        "segment_158": "It takes a lot of tokens!",
        "segment_160": "Consequently, it's not hard to see why the GroupChat manager sometimes struggles with this orchestration task.",
        "segment_161": "With AutoGen 0.2.2 onward, GroupChat instead relies on the description field. With a description field the orchestration prompt becomes:",
        "segment_162": "You are in a role play game. The following roles are available:assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_163": "This is much easier to parse and understand, and it doesn't use nearly as many tokens. Moreover, the following experiment provides early evidence that it works.",
        "segment_164": "An Experiment with Distraction\u200b",
        "segment_165": "To illustrate the impact of the description field, we set up a three-agent experiment with a reduced 26-problem subset of the HumanEval benchmark. Here, three agents were added to a GroupChat to solve programming problems. The three agents were:",
        "segment_167": "Coder (default Assistant prompt)",
        "segment_168": "UserProxy (configured to execute code)",
        "segment_169": "ExecutiveChef (added as a distraction)",
        "segment_171": "The Coder and UserProxy used the AssistantAgent and UserProxy defaults (provided above), while the ExecutiveChef was given the system prompt:",
        "segment_172": "You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.",
        "segment_173": "The ExecutiveChef is clearly the distractor here -- given that no HumanEval problems are food-related, the GroupChat should rarely consult with the chef. However, when configured with GPT-3.5-turbo-16k, we can clearly see the GroupChat struggling with orchestration:",
        "segment_174": "With versions prior to 0.2.2, using system_message:\u200b",
        "segment_176": "The Agents solve 3 out of 26 problems on their first turn",
        "segment_177": "The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)",
        "segment_179": "With version 0.2.2, using description:\u200b",
        "segment_181": "The Agents solve 7 out of 26 problems on the first turn",
        "segment_182": "The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)",
        "segment_184": "Using the description field doubles performance on this task and halves the incidence of calling upon the distractor agent.",
        "segment_185": "Tips for Writing Good Descriptions\u200b",
        "segment_186": "Since descriptions serve a different purpose than system_messages, it is worth reviewing what makes a good agent description. While descriptions are new, the following tips appear to lead to good results:",
        "segment_188": "Avoid using the 1st or 2nd person perspective. Descriptions should not contain \"I\" or \"You\", unless perhaps \"You\" is in reference to the GroupChat / orchestrator",
        "segment_189": "Include any details that might help the orchestrator know when to call upon the agent",
        "segment_190": "Keep descriptions short (e.g., \"A helpful AI assistant with strong natural language and Python coding skills.\").",
        "segment_192": "The main thing to remember is that the description is for the benefit of the GroupChatManager, not for the Agent's own use or instruction.",
        "segment_193": "Conclusion\u200b",
        "segment_194": "AutoGen 0.2.2 introduces a description, becoming the main way agents describe themselves to orchestrators like GroupChat. Since the description defaults to the system_message, there's nothing you need to change if you were already satisfied with how your group chats were working. However, we expect this feature to generally improve orchestration, so please consider experimenting with the description field if you are struggling with GroupChat or want to boost performance.Tags:AutoGenAutoGen Studio: Interactively Explore Multi-Agent WorkflowsDecember 1, 2023 \u00b7 10 min readVictor DibiaPrincipal RSDE at Microsoft ResearchGagan BansalSenior Researcher at Microsoft ResearchSaleema AmershiSenior Principal Research Manager at Microsoft Research",
        "segment_195": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_196": "TLDR\u200b",
        "segment_197": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_199": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_200": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_201": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_202": "Publish your sessions to a local gallery.",
        "segment_204": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_205": "pip install autogenstudio",
        "segment_206": "Introduction\u200b",
        "segment_207": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_208": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_210": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_212": "Getting Started with AutoGen Studio\u200b",
        "segment_213": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_214": "Configuring an LLM Provider\u200b",
        "segment_215": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_216": "For example, in your terminal, you would set the API key like this:",
        "segment_217": "export OPENAI_API_KEY=",
        "segment_218": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_219": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_220": "Installation\u200b",
        "segment_221": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_224": "Install from PyPi",
        "segment_225": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_226": "pip install autogenstudio",
        "segment_229": "Install from Source",
        "segment_231": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_233": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_236": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_237": "pip install -e .",
        "segment_240": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_241": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_244": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_247": "Running the Application\u200b",
        "segment_248": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_249": "autogenstudio ui --port 8081",
        "segment_250": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_251": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_252": "What Can You Do with AutoGen Studio?\u200b",
        "segment_253": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_254": "Build\u200b",
        "segment_256": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_257": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_259": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_260": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_261": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_262": "Playground\u200b",
        "segment_264": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_265": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_266": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_267": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_268": "Gallery\u200b",
        "segment_269": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_270": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_271": "The AutoGen Studio API\u200b",
        "segment_272": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_273": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_274": "Road Map and Next Steps\u200b",
        "segment_275": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_277": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_278": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_279": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_280": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_282": "Contribution Guide\u200b",
        "segment_283": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_285": "Review the overall AutoGen project contribution guide.",
        "segment_286": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_287": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_288": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_289": "Submit a pull request with your contribution!",
        "segment_290": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_291": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_293": "FAQ\u200b",
        "segment_294": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_295": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_296": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_297": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_298": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_299": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_300": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_301": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_302": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_303": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_304": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_305": "Q: The Server Starts But I Can't Access the UI",
        "segment_306": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_307": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_308": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_5": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_6": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_7": "Observations\u200b",
        "segment_9": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_10": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_12": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_13": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_14": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_15": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_16": "What else can we do? We notice that:",
        "segment_17": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_18": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_19": "Solution\u200b",
        "segment_20": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_22": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_23": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_26": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_27": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_29": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_30": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_31": "GPT-4, n=1, temperature=0",
        "segment_32": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_33": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_35": "Experiment Results\u200b",
        "segment_36": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_37": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_38": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_40": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_42": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_44": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_45": "Solved by GPT-4, n=1, temperature=0:",
        "segment_47": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_49": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_51": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_53": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_55": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_56": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_58": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_59": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_61": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_62": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_63": "Discussion\u200b",
        "segment_64": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_65": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_67": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_68": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_70": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_71": "Different configurations have complementary effect.",
        "segment_75": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_76": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_77": "There are many directions of extensions in research and development:",
        "segment_79": "Generalize the way to provide feedback.",
        "segment_80": "Automate the process of optimizing the configurations.",
        "segment_81": "Build adaptive agents for different applications.",
        "segment_83": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_84": "For Further Reading\u200b",
        "segment_86": "Documentation about autogen and Research paper.",
        "segment_87": "Blog post about a related study for math.",
        "segment_88": "Tags:LLMGPTresearchNewer PostMathChat - An Conversational Framework to Solve Math ProblemsOlder PostDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHObservationsSolutionExperiment ResultsDiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_4": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_5": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_7": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_8": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_9": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_10": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_11": "Experiment Setup\u200b",
        "segment_12": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_14": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_15": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_17": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_19": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_20": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_21": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_22": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_23": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_25": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_26": "Experiment Results\u200b",
        "segment_27": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_28": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_29": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_31": "However, the selected model changes on level 4 Algebra.",
        "segment_33": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_34": "On level 5 the result is similar.",
        "segment_36": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_37": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_38": "Analysis and Discussion\u200b",
        "segment_39": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_40": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_41": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_42": "For Further Reading\u200b",
        "segment_44": "Research paper about the tuning technique",
        "segment_45": "Documentation about inference tuning",
        "segment_47": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostAchieve More, Pay Less - Use GPT-4 SmartlyExperiment SetupExperiment ResultsAnalysis and DiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGenBench is a standalone tool for evaluating AutoGen agents and workflows on common benchmarks.",
        "segment_2": "TLDR\u200b",
        "segment_3": "Today we are releasing AutoGenBench \u2013 a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.",
        "segment_4": "AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.",
        "segment_6": "See the AutoGenBench README for information on installation and running benchmarks.",
        "segment_7": "See the AutoGenBench CONTRIBUTING guide for information on developing or contributing benchmark datasets.",
        "segment_9": "Quick Start\u200b",
        "segment_10": "Get started quickly by running the following commands in a bash terminal.",
        "segment_11": "Note: You may need to adjust the path to the OAI_CONFIG_LIST, as appropriate.",
        "segment_12": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)pip install autogenbenchautogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate Results/human_eval_two_agents",
        "segment_13": "Introduction\u200b",
        "segment_14": "Measurement and evaluation are core components of every major AI or ML research project. The same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to AgentEval. In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluation; and conclude with an open call for contributions.",
        "segment_15": "Design Principles\u200b",
        "segment_16": "AutoGenBench is designed around three core design principles. Knowing these principles will help you understand the tool, its operation and its output. These three principles are:",
        "segment_19": "Repetition: LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.",
        "segment_22": "Isolation: Agents interact with their worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to ordering effects that can impact future measurements. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in its own Docker container. This ensures that all runs start with the same initial conditions. (Docker is also a much safer way to run agent-produced code, in general.)",
        "segment_25": "Instrumentation: While top-line metrics are great for comparing agents or models, we often want much more information about how the agents are performing, where they are getting stuck, and how they can be improved. We may also later think of new research questions that require computing a different set of metrics. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like AgentEval.",
        "segment_28": "Installing and Running AutoGenBench\u200b",
        "segment_29": "As noted above, isolation is a key design principle, and so AutoGenBench must be run in an environment where Docker is available (desktop or Engine). It will not run in GitHub codespaces, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see https://www.docker.com/products/docker-desktop/.",
        "segment_30": "Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With pip, installation can be achieved as follows:",
        "segment_31": "pip install autogenbench",
        "segment_32": "After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter.",
        "segment_33": "If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:",
        "segment_34": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)",
        "segment_35": "A Typical Session\u200b",
        "segment_36": "Once AutoGenBench and necessary keys are installed, a typical session will look as follows:",
        "segment_37": "autogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate results/human_eval_two_agents",
        "segment_38": "Where:",
        "segment_40": "autogenbench clone HumanEval downloads and expands the HumanEval benchmark scenario.",
        "segment_41": "cd HumanEval; cat README.md navigates to the benchmark directory, and prints the README (which you should always read!)",
        "segment_42": "autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl",
        "segment_43": "runs a 10% subsample of the tasks defined in Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.",
        "segment_44": "autogenbench tabulate results/human_eval_two_agents tabulates the results of the run.",
        "segment_46": "After running the above tabulate command, you should see output similar to the following:",
        "segment_47": "Trial 0 Trial 1 Trial 2Task Id Success Success Success------------- --------- --------- ---------HumanEval_107 False True TrueHumanEval_22 True True TrueHumanEval_43 True True TrueHumanEval_88 True True TrueHumanEval_14 True True TrueHumanEval_157 True True TrueHumanEval_141 True True TrueHumanEval_57 True True TrueHumanEval_154 True True TrueHumanEval_153 True True TrueHumanEval_93 False True FalseHumanEval_137 True True TrueHumanEval_143 True True TrueHumanEval_13 True True TrueHumanEval_49 True True TrueHumanEval_95 True True True------------- --------- --------- ---------Successes 14 16 15Failures 2 0 1Missing 0 0 0Total 16 16 16CAUTION: 'autogenbench tabulate' is in early preview.Please do not cite these values in academic work without first inspecting and verifying the results in the logs yourself.",
        "segment_48": "From this output we can see the results of the three separate repetitions of each task, and final summary statistics of each run. In this case, the results were generated via GPT-4 (as defined in the OAI_CONFIG_LIST that was provided), and used the TwoAgents template. It is important to remember that AutoGenBench evaluates specific end-to-end configurations of agents (as opposed to evaluating a model or cognitive framework more generally).",
        "segment_49": "Finally, complete execution traces and logs can be found in the Results folder. See the AutoGenBench README for more details about command-line options and output formats. Each of these commands also offers extensive in-line help via:",
        "segment_51": "autogenbench --help",
        "segment_52": "autogenbench clone --help",
        "segment_53": "autogenbench run --help",
        "segment_54": "autogenbench tabulate --help",
        "segment_56": "Roadmap\u200b",
        "segment_57": "While we are announcing AutoGenBench, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:",
        "segment_59": "Onboard many additional benchmarks beyond those shipping today",
        "segment_60": "Greatly improve logging and telemetry",
        "segment_61": "Introduce new core metrics including total costs, task completion time, conversation turns, etc.",
        "segment_62": "Provide tighter integration with AgentEval and AutoGen Studio",
        "segment_64": "For an up to date tracking of our work items on this project, please see AutoGenBench Work Items",
        "segment_65": "Call for Participation\u200b",
        "segment_66": "Finally, we want to end this blog post with an open call for contributions. AutoGenBench is still nascent, and has much opportunity for improvement. New benchmarks are constantly being published, and will need to be added. Everyone may have their own distinct set of metrics that they care most about optimizing, and these metrics should be onboarded. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the contributor\u2019s guide and join our Discord discussion in the #autogenbench channel!Tags:AutoGenNewer PostAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismOlder PostCode execution is now by default inside docker containerTLDRQuick StartIntroductionDesign PrinciplesInstalling and Running AutoGenBenchA Typical SessionRoadmapCall for ParticipationCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def get_key(config: Dict[str, Any]) -> str",
        "segment_2": "Get a unique identifier of a configuration.",
        "segment_3": "Arguments:",
        "segment_5": "config dict or list - A configuration.",
        "segment_7": "Returns:",
        "segment_9": "tuple - A unique identifier which can be used as a key for a dict.",
        "segment_11": "get_config_list\u200b",
        "segment_12": "def get_config_list(api_keys: List, base_urls: Optional[List] = None, api_type: Optional[str] = None, api_version: Optional[str] = None) -> List[Dict]",
        "segment_13": "Get a list of configs for OpenAI API client.",
        "segment_14": "Arguments:",
        "segment_16": "api_keys list - The api keys for openai api calls.",
        "segment_17": "base_urls list, optional - The api bases for openai api calls. If provided, should match the length of api_keys.",
        "segment_18": "api_type str, optional - The api type for openai api calls.",
        "segment_19": "api_version str, optional - The api version for openai api calls.",
        "segment_21": "Returns:",
        "segment_23": "list - A list of configs for OepnAI API calls.",
        "segment_25": "Example:",
        "segment_26": "# Define a list of API keysapi_keys = ['key1', 'key2', 'key3']# Optionally, define a list of base URLs corresponding to each API keybase_urls = ['https://api.service1.com', 'https://api.service2.com', 'https://api.service3.com']# Optionally, define the API type and version if they are common for all keysapi_type = 'azure'api_version = '2023-12-01-preview'# Call the get_config_list function to get a list of configuration dictionariesconfig_list = get_config_list(api_keys, base_urls, api_type, api_version)",
        "segment_27": "config_list_openai_aoai\u200b",
        "segment_28": "def config_list_openai_aoai( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", openai_api_base_file: Optional[str] = \"base_openai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None) -> List[Dict]",
        "segment_29": "Get a list of configs for OpenAI API client (including Azure or local model deployments that support OpenAI's chat completion API).",
        "segment_30": "This function constructs configurations by reading API keys and base URLs from environment variables or text files.",
        "segment_31": "It supports configurations for both OpenAI and Azure OpenAI services, allowing for the exclusion of one or the other.",
        "segment_32": "When text files are used, the environment variables will be overwritten.",
        "segment_33": "To prevent text files from being used, set the corresponding file name to None.",
        "segment_34": "Or set key_file_path to None to disallow reading from text files.",
        "segment_35": "Arguments:",
        "segment_37": "key_file_path str, optional - The directory path where the API key files are located. Defaults to the current directory.",
        "segment_38": "openai_api_key_file str, optional - The filename containing the OpenAI API key. Defaults to 'key_openai.txt'.",
        "segment_39": "aoai_api_key_file str, optional - The filename containing the Azure OpenAI API key. Defaults to 'key_aoai.txt'.",
        "segment_40": "openai_api_base_file str, optional - The filename containing the OpenAI API base URL. Defaults to 'base_openai.txt'.",
        "segment_41": "aoai_api_base_file str, optional - The filename containing the Azure OpenAI API base URL. Defaults to 'base_aoai.txt'.",
        "segment_42": "exclude str, optional - The API type to exclude from the configuration list. Can be 'openai' or 'aoai'. Defaults to None.",
        "segment_44": "Returns:",
        "segment_46": "List[Dict] - A list of configuration dictionaries. Each dictionary contains keys for 'api_key',",
        "segment_47": "and optionally 'base_url', 'api_type', and 'api_version'.",
        "segment_49": "Raises:",
        "segment_51": "FileNotFoundError - If the specified key files are not found and the corresponding API key is not set in the environment variables.",
        "segment_53": "Example:",
        "segment_54": "To generate configurations excluding Azure OpenAI:",
        "segment_55": "configs = config_list_openai_aoai(exclude='aoai')",
        "segment_56": "File samples:",
        "segment_59": "key_aoai.txt",
        "segment_60": "aoai-12345abcdef67890ghijklmnopqraoai-09876zyxwvuts54321fedcba",
        "segment_63": "base_aoai.txt",
        "segment_64": "https://api.azure.com/v1https://api.azure2.com/v1",
        "segment_67": "Notes:",
        "segment_69": "The function checks for API keys and base URLs in the following environment variables: 'OPENAI_API_KEY', 'AZURE_OPENAI_API_KEY',",
        "segment_70": "'OPENAI_API_BASE' and 'AZURE_OPENAI_API_BASE'. If these are not found, it attempts to read from the specified files in the",
        "segment_71": "'key_file_path' directory.",
        "segment_72": "The API version for Azure configurations is set to DEFAULT_AZURE_API_VERSION by default.",
        "segment_73": "If 'exclude' is set to 'openai', only Azure OpenAI configurations are returned, and vice versa.",
        "segment_74": "The function assumes that the API keys and base URLs in the environment variables are separated by new lines if there are",
        "segment_75": "multiple entries.",
        "segment_77": "config_list_from_models\u200b",
        "segment_78": "def config_list_from_models( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None, model_list: Optional[list] = None) -> List[Dict]",
        "segment_79": "Get a list of configs for API calls with models specified in the model list.",
        "segment_80": "This function extends config_list_openai_aoai by allowing to clone its' out for each of the models provided.",
        "segment_81": "Each configuration will have a 'model' key with the model name as its value. This is particularly useful when",
        "segment_82": "all endpoints have same set of models.",
        "segment_83": "Arguments:",
        "segment_85": "key_file_path str, optional - The path to the key files.",
        "segment_86": "openai_api_key_file str, optional - The file name of the OpenAI API key.",
        "segment_87": "aoai_api_key_file str, optional - The file name of the Azure OpenAI API key.",
        "segment_88": "aoai_api_base_file str, optional - The file name of the Azure OpenAI API base.",
        "segment_89": "exclude str, optional - The API type to exclude, \"openai\" or \"aoai\".",
        "segment_90": "model_list list, optional - The list of model names to include in the configs.",
        "segment_92": "Returns:",
        "segment_94": "list - A list of configs for OpenAI API calls, each including model information.",
        "segment_96": "Example:",
        "segment_97": "# Define the path where the API key files are located key_file_path = '/path/to/key/files' # Define the file names for the OpenAI and Azure OpenAI API keys and bases openai_api_key_file = 'key_openai.txt' aoai_api_key_file = 'key_aoai.txt' aoai_api_base_file = 'base_aoai.txt' # Define the list of models for which to create configurations model_list = ['gpt-4', 'gpt-3.5-turbo'] # Call the function to get a list of configuration dictionaries config_list = config_list_from_models( key_file_path=key_file_path, openai_api_key_file=openai_api_key_file, aoai_api_key_file=aoai_api_key_file, aoai_api_base_file=aoai_api_base_file, model_list=model_list ) # The `config_list` will contain configurations for the specified models, for example: # [ # {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-4'}, # {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-3.5-turbo'} # ]",
        "segment_98": "config_list_gpt4_gpt35\u200b",
        "segment_99": "def config_list_gpt4_gpt35( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None) -> List[Dict]",
        "segment_100": "Get a list of configs for 'gpt-4' followed by 'gpt-3.5-turbo' API calls.",
        "segment_101": "Arguments:",
        "segment_103": "key_file_path str, optional - The path to the key files.",
        "segment_104": "openai_api_key_file str, optional - The file name of the openai api key.",
        "segment_105": "aoai_api_key_file str, optional - The file name of the azure openai api key.",
        "segment_106": "aoai_api_base_file str, optional - The file name of the azure openai api base.",
        "segment_107": "exclude str, optional - The api type to exclude, \"openai\" or \"aoai\".",
        "segment_109": "Returns:",
        "segment_111": "list - A list of configs for openai api calls.",
        "segment_113": "filter_config\u200b",
        "segment_114": "def filter_config(config_list, filter_dict)",
        "segment_115": "This function filters config_list by checking each configuration dictionary against the",
        "segment_116": "criteria specified in filter_dict. A configuration dictionary is retained if for every",
        "segment_117": "key in filter_dict, see example below.",
        "segment_118": "Arguments:",
        "segment_121": "config_list list of dict - A list of configuration dictionaries to be filtered.",
        "segment_124": "filter_dict dict - A dictionary representing the filter criteria, where each key is a",
        "segment_125": "field name to check within the configuration dictionaries, and the",
        "segment_126": "corresponding value is a list of acceptable values for that field.",
        "segment_127": "If the configuration's field's value is not a list, then a match occurs",
        "segment_128": "when it is found in the list of acceptable values. If the configuration's",
        "segment_129": "field's value is a list, then a match occurs if there is a non-empty",
        "segment_130": "intersection with the acceptable values.",
        "segment_133": "Returns:",
        "segment_134": "list of dict: A list of configuration dictionaries that meet all the criteria specified",
        "segment_135": "in filter_dict.",
        "segment_136": "Example:",
        "segment_137": "# Example configuration list with various models and API types configs = [ {'model': 'gpt-3.5-turbo'}, {'model': 'gpt-4'}, {'model': 'gpt-3.5-turbo', 'api_type': 'azure'}, {'model': 'gpt-3.5-turbo', 'tags': ['gpt35_turbo', 'gpt-35-turbo']}, ] # Define filter criteria to select configurations for the 'gpt-3.5-turbo' model # that are also using the 'azure' API type filter_criteria = { 'model': ['gpt-3.5-turbo'], # Only accept configurations for 'gpt-3.5-turbo' 'api_type': ['azure'] # Only accept configurations for 'azure' API type } # Apply the filter to the configuration list filtered_configs = filter_config(configs, filter_criteria) # The resulting `filtered_configs` will be: # [{'model': 'gpt-3.5-turbo', 'api_type': 'azure', ...}] # Define a filter to select a given tag filter_criteria = { 'tags': ['gpt35_turbo'], } # Apply the filter to the configuration list filtered_configs = filter_config(configs, filter_criteria) # The resulting `filtered_configs` will be: # [{'model': 'gpt-3.5-turbo', 'tags': ['gpt35_turbo', 'gpt-35-turbo']}]",
        "segment_138": "Notes:",
        "segment_140": "If filter_dict is empty or None, no filtering is applied and config_list is returned as is.",
        "segment_141": "If a configuration dictionary in config_list does not contain a key specified in filter_dict,",
        "segment_142": "it is considered a non-match and is excluded from the result.",
        "segment_143": "If the list of acceptable values for a key in filter_dict includes None, then configuration",
        "segment_144": "dictionaries that do not have that key will also be considered a match.",
        "segment_146": "config_list_from_json\u200b",
        "segment_147": "def config_list_from_json( env_or_file: str, file_location: Optional[str] = \"\", filter_dict: Optional[Dict[str, Union[List[Union[str, None]], Set[Union[str, None]]]]] = None) -> List[Dict]",
        "segment_148": "Retrieves a list of API configurations from a JSON stored in an environment variable or a file.",
        "segment_149": "This function attempts to parse JSON data from the given env_or_file parameter. If env_or_file is an",
        "segment_150": "environment variable containing JSON data, it will be used directly. Otherwise, it is assumed to be a filename,",
        "segment_151": "and the function will attempt to read the file from the specified file_location.",
        "segment_152": "The filter_dict parameter allows for filtering the configurations based on specified criteria. Each key in the",
        "segment_153": "filter_dict corresponds to a field in the configuration dictionaries, and the associated value is a list or set",
        "segment_154": "of acceptable values for that field. If a field is missing in a configuration and None is included in the list",
        "segment_155": "of acceptable values for that field, the configuration will still be considered a match.",
        "segment_156": "Arguments:",
        "segment_158": "env_or_file str - The name of the environment variable, the filename, or the environment variable of the filename",
        "segment_159": "that containing the JSON data.",
        "segment_160": "file_location str, optional - The directory path where the file is located, if env_or_file is a filename.",
        "segment_161": "filter_dict dict, optional - A dictionary specifying the filtering criteria for the configurations, with",
        "segment_162": "keys representing field names and values being lists or sets of acceptable values for those fields.",
        "segment_164": "Example:",
        "segment_165": "env_or_file0",
        "segment_166": "Returns:",
        "segment_168": "env_or_file1 - A list of configuration dictionaries that match the filtering criteria specified in filter_dict.",
        "segment_170": "Raises:",
        "segment_172": "env_or_file3 - if env_or_file is neither found as an environment variable nor a file",
        "segment_174": "get_config\u200b",
        "segment_175": "def get_config(api_key: str, base_url: Optional[str] = None, api_type: Optional[str] = None, api_version: Optional[str] = None) -> Dict",
        "segment_176": "Constructs a configuration dictionary for a single model with the provided API configurations.",
        "segment_177": "Example:",
        "segment_178": "config = get_config( api_key=\"sk-abcdef1234567890\", base_url=\"https://api.openai.com\", api_version=\"v1\")# The 'config' variable will now contain:# {# \"api_key\": \"sk-abcdef1234567890\",# \"base_url\": \"https://api.openai.com\",# \"api_version\": \"v1\"# }",
        "segment_179": "Arguments:",
        "segment_181": "api_key str - The API key for authenticating API requests.",
        "segment_182": "base_url Optional[str] - The base URL of the API. If not provided, defaults to None.",
        "segment_183": "api_type Optional[str] - The type of API. If not provided, defaults to None.",
        "segment_184": "api_version Optional[str] - The version of the API. If not provided, defaults to None.",
        "segment_186": "Returns:",
        "segment_188": "Dict - A dictionary containing the provided API configurations.",
        "segment_190": "config_list_from_dotenv\u200b",
        "segment_191": "def config_list_from_dotenv( dotenv_file_path: Optional[str] = None, model_api_key_map: Optional[dict] = None, filter_dict: Optional[dict] = None) -> List[Dict[str, Union[str, Set[str]]]]",
        "segment_192": "Load API configurations from a specified .env file or environment variables and construct a list of configurations.",
        "segment_193": "This function will:",
        "segment_195": "Load API keys from a provided .env file or from existing environment variables.",
        "segment_196": "Create a configuration dictionary for each model using the API keys and additional configurations.",
        "segment_197": "Filter and return the configurations based on provided filters.",
        "segment_199": "model_api_key_map will default to {\"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": \"OPENAI_API_KEY\"} if none",
        "segment_200": "Arguments:",
        "segment_202": "dotenv_file_path str, optional - The path to the .env file. Defaults to None.",
        "segment_203": "model_api_key_map str/dict, optional - A dictionary mapping models to their API key configurations.",
        "segment_204": "If a string is provided as configuration, it is considered as an environment",
        "segment_205": "variable name storing the API key.",
        "segment_206": "If a dict is provided, it should contain at least 'api_key_env_var' key,",
        "segment_207": "and optionally other API configurations like 'base_url', 'api_type', and 'api_version'.",
        "segment_208": "Defaults to a basic map with 'gpt-4' and 'gpt-3.5-turbo' mapped to 'OPENAI_API_KEY'.",
        "segment_209": "filter_dict dict, optional - A dictionary containing the models to be loaded.",
        "segment_210": "Containing a 'model' key mapped to a set of model names to be loaded.",
        "segment_211": "Defaults to None, which loads all found configurations.",
        "segment_213": "Returns:",
        "segment_214": "List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model.",
        "segment_215": "Raises:",
        "segment_217": "FileNotFoundError - If the specified .env file does not exist.",
        "segment_218": "TypeError - If an unsupported type of configuration is provided in model_api_key_map.",
        "segment_220": "retrieve_assistants_by_name\u200b",
        "segment_221": "def retrieve_assistants_by_name(client: OpenAI, name: str) -> List[Assistant]",
        "segment_222": "Return the assistants with the given name from OAI assistant APIEdit this pagePreviouscompletionNextagent_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKReferenceautogenautomldefaultonlinemltunetune.schedulertune.searchertune.sparkanalysissamplespacetrialtrial_runnertuneutilsOn this pagetune.analysisExperimentAnalysis Objects\u200bclass ExperimentAnalysis()CopyAnalyze results from a Tune experiment.best_trial\u200b@propertydef best_trial() -> TrialCopyGet the best trial of the experiment",
        "segment_2": "The best trial is determined by comparing the last trial results",
        "segment_3": "using the metric and mode parameters passed to tune.run().",
        "segment_4": "If you didn't pass these parameters, use",
        "segment_5": "get_best_trial(metric, mode, scope) instead.best_config\u200b@propertydef best_config() -> DictCopyGet the config of the best trial of the experiment",
        "segment_6": "The best trial is determined by comparing the last trial results",
        "segment_7": "using the metric and mode parameters passed to tune.run().",
        "segment_8": "If you didn't pass these parameters, use",
        "segment_9": "get_best_config(metric, mode, scope) instead.results\u200b@propertydef results() -> Dict[str, Dict]CopyGet the last result of all the trials of the experimentget_best_trial\u200bdef get_best_trial(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = \"last\", filter_nan_and_inf: bool = True) -> Optional[Trial]CopyRetrieve the best trial object.",
        "segment_10": "Compares all trials' scores on metric.",
        "segment_11": "If metric is not specified, self.default_metric will be used.",
        "segment_12": "If mode is not specified, self.default_mode will be used.",
        "segment_13": "These values are usually initialized by passing the metric and",
        "segment_14": "mode parameters to tune.run().Arguments:metric str - Key for trial info to order on. Defaults to",
        "segment_15": "self.default_metric.mode str - One of [min, max]. Defaults to self.default_mode.scope str - One of [all, last, avg, last-5-avg, last-10-avg].",
        "segment_16": "If scope=last, only look at each trial's final step for",
        "segment_17": "metric, and compare across trials based on mode=[min,max].",
        "segment_18": "If scope=avg, consider the simple average over all steps",
        "segment_19": "for metric and compare across trials based on",
        "segment_20": "mode=[min,max]. If scope=last-5-avg or scope=last-10-avg,",
        "segment_21": "consider the simple average over the last 5 or 10 steps for",
        "segment_22": "metric and compare across trials based on mode=[min,max].",
        "segment_23": "If scope=all, find each trial's min/max score for metric",
        "segment_24": "based on mode, and compare trials based on mode=[min,max].filter_nan_and_inf bool - If True (default), NaN or infinite",
        "segment_25": "values are disregarded and these trials are never selected as",
        "segment_26": "the best trial.get_best_config\u200bdef get_best_config(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = \"last\") -> Optional[Dict]CopyRetrieve the best config corresponding to the trial.",
        "segment_27": "Compares all trials' scores on metric.",
        "segment_28": "If metric is not specified, self.default_metric will be used.",
        "segment_29": "If mode is not specified, self.default_mode will be used.",
        "segment_30": "These values are usually initialized by passing the metric and",
        "segment_31": "mode parameters to tune.run().Arguments:metric str - Key for trial info to order on. Defaults to",
        "segment_32": "self.default_metric.mode str - One of [min, max]. Defaults to self.default_mode.scope str - One of [all, last, avg, last-5-avg, last-10-avg].",
        "segment_33": "If scope=last, only look at each trial's final step for",
        "segment_34": "metric, and compare across trials based on mode=[min,max].",
        "segment_35": "If scope=avg, consider the simple average over all steps",
        "segment_36": "for metric and compare across trials based on",
        "segment_37": "mode=[min,max]. If scope=last-5-avg or scope=last-10-avg,",
        "segment_38": "consider the simple average over the last 5 or 10 steps for",
        "segment_39": "metric and compare across trials based on mode=[min,max].",
        "segment_40": "If scope=all, find each trial's min/max score for metric",
        "segment_41": "based on mode, and compare trials based on mode=[min,max].best_result\u200b@propertydef best_result() -> DictCopyGet the last result of the best trial of the experiment",
        "segment_42": "The best trial is determined by comparing the last trial results",
        "segment_43": "using the metric and mode parameters passed to tune.run().",
        "segment_44": "If you didn't pass these parameters, use",
        "segment_45": "get_best_trial(metric, mode, scope).last_result instead.Edit this pagePrevious\u00ab utilsNextsample \u00bbExperimentAnalysis ObjectsCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageContributingThis project welcomes and encourages all forms of contributions, including but not limited to:Pushing patches.Code review of pull requests.Documentation, examples and test cases.Readability improvement, e.g., improvement on docstr and comments.Community participation in issues, discussions, and discord.Tutorials, blog posts, talks that promote the project.Sharing application scenarios and/or related research.You can take a look at the Roadmap for Upcoming Features to identify potential things to work on.Most contributions require you to agree to a",
        "segment_2": "Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us",
        "segment_3": "the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.If you are new to GitHub here is a detailed help source on getting involved with development on GitHub.When you submit a pull request, a CLA bot will automatically determine whether you need to provide",
        "segment_4": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions",
        "segment_5": "provided by the bot. You will only need to do this once across all repos using our CLA.This project has adopted the Microsoft Open Source Code of Conduct.",
        "segment_6": "For more information see the Code of Conduct FAQ or",
        "segment_7": "contact opencode@microsoft.com with any additional questions or comments.How to make a good bug report\u200bWhen you submit an issue to GitHub, please do your best to",
        "segment_8": "follow these guidelines! This will make it a lot easier to provide you with good",
        "segment_9": "feedback:The ideal bug report contains a short reproducible code snippet. This way",
        "segment_10": "anyone can try to reproduce the bug easily (see this for more details). If your snippet is",
        "segment_11": "longer than around 50 lines, please link to a gist or a GitHub repo.If an exception is raised, please provide the full traceback.Please include your operating system type and version number, as well as",
        "segment_12": "your Python, flaml, scikit-learn versions. The version of flaml",
        "segment_13": "can be found by running the following code snippet:import flamlprint(flaml.__version__)CopyPlease ensure all code snippets and error messages are formatted in",
        "segment_14": "appropriate code blocks. See Creating and highlighting code blocks",
        "segment_15": "for more details.Becoming a Reviewer\u200bThere is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord.Developing\u200bSetup\u200bgit clone https://github.com/microsoft/FLAML.gitpip install -e FLAML[notebook,autogen]CopyIn case the pip install command fails, try escaping the brackets such as pip install -e FLAML\\[notebook,autogen\\].Docker\u200bWe provide a simple Dockerfile.docker build https://github.com/microsoft/FLAML.git#main -t flaml-devdocker run -it flaml-devCopyDevelop in Remote Container\u200bIf you use vscode, you can open the FLAML folder in a Container.",
        "segment_16": "We have provided the configuration in devcontainer.Pre-commit\u200bRun pre-commit install to install pre-commit into your git hooks. Before you commit, run",
        "segment_17": "pre-commit run to check if you meet the pre-commit requirements. If you use Windows (without WSL) and can't commit after installing pre-commit, you can run pre-commit uninstall to uninstall the hook. In WSL or Linux this is supposed to work.Coverage\u200bAny code you commit should not decrease coverage. To run all unit tests, install the [test] option under FLAML/:pip install -e.\"[test]\"coverage run -m pytest testCopyThen you can see the coverage report by",
        "segment_18": "coverage report -m or coverage html.Documentation\u200bTo build and test documentation locally, install Node.js. For example,nvm install --ltsCopyThen:npm install --global yarn # skip if you use the dev container we providedpip install pydoc-markdown==4.5.0 # skip if you use the dev container we providedcd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownyarn startCopyThe last command starts a local development server and opens up a browser window.",
        "segment_19": "Most changes are reflected live without having to restart the server.Note:",
        "segment_20": "some tips in this guide are based off the contributor guide from ray, scikit-learn, or hummingbird.Edit this pagePrevious\u00ab Tune - PyTorchNextResearch \u00bbHow to make a good bug reportBecoming a ReviewerDevelopingSetupDockerDevelop in Remote ContainerPre-commitCoverageDocumentationCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "def get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any",
        "segment_2": "Get the type annotation of a parameter.",
        "segment_3": "Arguments:",
        "segment_5": "annotation - The annotation of the parameter",
        "segment_6": "globalns - The global namespace of the function",
        "segment_8": "Returns:",
        "segment_9": "The type annotation of the parameter",
        "segment_10": "get_typed_signature\u200b",
        "segment_11": "def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature",
        "segment_12": "Get the signature of a function with type annotations.",
        "segment_13": "Arguments:",
        "segment_15": "call - The function to get the signature for",
        "segment_17": "Returns:",
        "segment_18": "The signature of the function with type annotations",
        "segment_19": "get_typed_return_annotation\u200b",
        "segment_20": "def get_typed_return_annotation(call: Callable[..., Any]) -> Any",
        "segment_21": "Get the return annotation of a function.",
        "segment_22": "Arguments:",
        "segment_24": "call - The function to get the return annotation for",
        "segment_26": "Returns:",
        "segment_27": "The return annotation of the function",
        "segment_28": "get_param_annotations\u200b",
        "segment_29": "def get_param_annotations( typed_signature: inspect.Signature) -> Dict[int, Union[Annotated[Type[Any], str], Type[Any]]]",
        "segment_30": "Get the type annotations of the parameters of a function",
        "segment_31": "Arguments:",
        "segment_33": "typed_signature - The signature of the function with type annotations",
        "segment_35": "Returns:",
        "segment_36": "A dictionary of the type annotations of the parameters of the function",
        "segment_37": "Parameters Objects\u200b",
        "segment_38": "class Parameters(BaseModel)",
        "segment_39": "Parameters of a function as defined by the OpenAI API",
        "segment_40": "Function Objects\u200b",
        "segment_41": "class Function(BaseModel)",
        "segment_42": "A function as defined by the OpenAI API",
        "segment_43": "ToolFunction Objects\u200b",
        "segment_44": "class ToolFunction(BaseModel)",
        "segment_45": "A function under tool as defined by the OpenAI API.",
        "segment_46": "get_parameter_json_schema\u200b",
        "segment_47": "def get_parameter_json_schema( k: str, v: Union[Annotated[Type[Any], str], Type[Any]], default_values: Dict[str, Any]) -> JsonSchemaValue",
        "segment_48": "Get a JSON schema for a parameter as defined by the OpenAI API",
        "segment_49": "Arguments:",
        "segment_51": "k - The name of the parameter",
        "segment_52": "v - The type of the parameter",
        "segment_53": "default_values - The default values of the parameters of the function",
        "segment_55": "Returns:",
        "segment_56": "A Pydanitc model for the parameter",
        "segment_57": "get_required_params\u200b",
        "segment_58": "def get_required_params(typed_signature: inspect.Signature) -> List[str]",
        "segment_59": "Get the required parameters of a function",
        "segment_60": "Arguments:",
        "segment_62": "signature - The signature of the function as returned by inspect.signature",
        "segment_64": "Returns:",
        "segment_65": "A list of the required parameters of the function",
        "segment_66": "get_default_values\u200b",
        "segment_67": "def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]",
        "segment_68": "Get default values of parameters of a function",
        "segment_69": "Arguments:",
        "segment_71": "signature - The signature of the function as returned by inspect.signature",
        "segment_73": "Returns:",
        "segment_74": "A dictionary of the default values of the parameters of the function",
        "segment_75": "get_parameters\u200b",
        "segment_76": "def get_parameters(required: List[str], param_annotations: Dict[str, Union[Annotated[Type[Any], str], Type[Any]]], default_values: Dict[str, Any]) -> Parameters",
        "segment_77": "Get the parameters of a function as defined by the OpenAI API",
        "segment_78": "Arguments:",
        "segment_80": "required - The required parameters of the function",
        "segment_81": "hints - The type hints of the function as returned by typing.get_type_hints",
        "segment_83": "Returns:",
        "segment_84": "A Pydantic model for the parameters of the function",
        "segment_85": "get_missing_annotations\u200b",
        "segment_86": "def get_missing_annotations(typed_signature: inspect.Signature, required: List[str]) -> Tuple[Set[str], Set[str]]",
        "segment_87": "Get the missing annotations of a function",
        "segment_88": "Ignores the parameters with default values as they are not required to be annotated, but logs a warning.",
        "segment_89": "Arguments:",
        "segment_91": "typed_signature - The signature of the function with type annotations",
        "segment_92": "required - The required parameters of the function",
        "segment_94": "Returns:",
        "segment_95": "A set of the missing annotations of the function",
        "segment_96": "get_function_schema\u200b",
        "segment_97": "def get_function_schema(f: Callable[..., Any], *, name: Optional[str] = None, description: str) -> Dict[str, Any]",
        "segment_98": "Get a JSON schema for a function as defined by the OpenAI API",
        "segment_99": "Arguments:",
        "segment_101": "f - The function to get the JSON schema for",
        "segment_102": "name - The name of the function",
        "segment_103": "description - The description of the function",
        "segment_105": "Returns:",
        "segment_106": "A JSON schema for the function",
        "segment_107": "Raises:",
        "segment_109": "TypeError - If the function is not annotated",
        "segment_111": "Examples:",
        "segment_112": "```def f(a: Annotated[str, \"Parameter a\"], b: int = 2, c: Annotated[float, \"Parameter c\"] = 0.1) -> None: passget_function_schema(f, description=\"function f\")# {'type': 'function',# 'function': {'description': 'function f',# 'name': 'f',# 'parameters': {'type': 'object',# 'properties': {'a': {'type': 'str', 'description': 'Parameter a'},# 'b': {'type': 'int', 'description': 'b'},# 'c': {'type': 'float', 'description': 'Parameter c'}},# 'required': ['a']}}} ```",
        "segment_113": "get_load_param_if_needed_function\u200b",
        "segment_114": "def get_load_param_if_needed_function( t: Any) -> Optional[Callable[[T, Type[Any]], BaseModel]]",
        "segment_115": "Get a function to load a parameter if it is a Pydantic model",
        "segment_116": "Arguments:",
        "segment_118": "t - The type annotation of the parameter",
        "segment_120": "Returns:",
        "segment_121": "A function to load the parameter if it is a Pydantic model, otherwise None",
        "segment_122": "load_basemodels_if_needed\u200b",
        "segment_123": "def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any]",
        "segment_124": "A decorator to load the parameters of a function if they are Pydantic models",
        "segment_125": "Arguments:",
        "segment_127": "func - The function with annotated parameters",
        "segment_129": "Returns:",
        "segment_130": "A function that loads the parameters before calling the original functionEdit this pagePreviouscode_utilsNextgraph_utilsParameters ObjectsFunction ObjectsToolFunction ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesAutoGen for Large Language ModelsTask Oriented AutoMLTune User Defined FunctionZero Shot AutoMLExamplesContributingResearchOn this pageTune User Defined Functionflaml.tune is a module for economical hyperparameter tuning. It is used internally by flaml.AutoML. It can also be used to directly tune a user-defined function (UDF), which is not limited to machine learning model training. You can use flaml.tune instead of flaml.AutoML if one of the following is true:Your machine learning task is not one of the built-in tasks from flaml.AutoML.Your input cannot be represented as X_train + y_train or dataframe + label.The optimization metric is not measurable via validation data only. For example, when you want to directly optimize a downstream application instead of a model accuracy metric.You need to tune a function that may not even be a machine learning procedure.Basic Tuning Procedure\u200bThere are three essential steps (assuming the knowledge of the set of hyperparameters to tune) to use flaml.tune to finish a basic tuning task:Specify the tuning objective with respect to the hyperparameters.Specify a search space of the hyperparameters.Specify tuning constraints, including constraints on the resource budget to do the tuning, constraints on the configurations, or/and constraints on a (or multiple) particular metric(s).With these steps, you can perform a basic tuning task accordingly.Tuning objective\u200bRelated arguments:evaluation_function: A user-defined evaluation function.metric: A string of the metric name to optimize for.mode: A string in ['min', 'max'] to specify the objective as minimization or maximization.The first step is to specify your tuning objective.",
        "segment_2": "To do it, you should first specify your evaluation procedure (e.g., perform a machine learning model training and validation) with respect to the hyperparameters in a user-defined function evaluation_function.",
        "segment_3": "The function requires a hyperparameter configuration as input, and can simply return a metric value in a scalar or return a dictionary of metric name and metric value pairs.In the following code, we define an evaluation function with respect to two hyperparameters named x and y according to obj:=(x\u221285000)2\u2212x/yobj := (x-85000)^2 - x/yobj:=(x\u221285000)2\u2212x/y. Note that we use this toy example here for more accessible demonstration purposes. In real use cases, the evaluation function usually cannot be written in this closed form, but instead involves a black-box and expensive evaluation procedure. Please check out Tune HuggingFace, Tune PyTorch and Tune LightGBM for real examples of tuning tasks.import timedef evaluate_config(config: dict): \"\"\"evaluate a hyperparameter configuration\"\"\" score = (config[\"x\"] - 85000) ** 2 - config[\"x\"] / config[\"y\"] # usually the evaluation takes an non-neglible cost # and the cost could be related to certain hyperparameters # here we simulate this cost by calling the time.sleep() function # here we assume the cost is proportional to x faked_evaluation_cost = config[\"x\"] / 100000 time.sleep(faked_evaluation_cost) # we can return a single float as a score on the input config: # return score # or, we can return a dictionary that maps metric name to metric value: return {\"score\": score, \"evaluation_cost\": faked_evaluation_cost, \"constraint_metric\": config[\"x\"] * config[\"y\"]}CopyWhen the evaluation function returns a dictionary of metrics, you need to specify the name of the metric to optimize via the argument metric (this can be skipped when the function is just returning a scalar). In addition, you need to specify a mode of your optimization/tuning task (maximization or minimization) via the argument mode by choosing from \"min\" or \"max\".For example,flaml.tune.run(evaluation_function=evaluate_config, metric=\"score\", mode=\"min\", ...)CopySearch space\u200bRelated arguments:config: A dictionary to specify the search space.low_cost_partial_config (optional): A dictionary from a subset of controlled dimensions to the initial low-cost values.cat_hp_cost (optional): A dictionary from a subset of categorical dimensions to the relative cost of each choice.The second step is to specify a search space of the hyperparameters through the argument config. In the search space, you need to specify valid values for your hyperparameters and can specify how these values are sampled (e.g., from a uniform distribution or a log-uniform distribution).In the following code example, we include a search space for the two hyperparameters x and y as introduced above. The valid values for both are integers in the range of [1, 100000]. The values for x are sampled uniformly in the specified range (using tune.randint(lower=1, upper=100000)), and the values for y are sampled uniformly in logarithmic space of the specified range (using tune.lograndit(lower=1, upper=100000)).from flaml import tune# construct a search space for the hyperparameters x and y.config_search_space = { \"x\": tune.lograndint(lower=1, upper=100000), \"y\": tune.randint(lower=1, upper=100000)}# provide the search space to tune.runtune.run(..., config=config_search_space, ...)CopyDetails and guidelines on hyperparameter search space\u200bThe corresponding value of a particular hyperparameter in the search space dictionary is called a domain, for example, tune.randint(lower=1, upper=100000) is the domain for the hyperparameter y.",
        "segment_4": "The domain specifies a type and valid range to sample parameters from. Supported types include float, integer, and categorical.Categorical hyperparameterIf it is a categorical hyperparameter, then you should use tune.choice(possible_choices) in which possible_choices is the list of possible categorical values of the hyperparameter. For example, if you are tuning the optimizer used in model training, and the candidate optimizers are \"sgd\" and \"adam\", you should specify the search space in the following way:{ \"optimizer\": tune.choice([\"sgd\", \"adam\"]),}CopyNumerical hyperparameterIf it is a numerical hyperparameter, you need to know whether it takes integer values or float values. In addition, you need to know:The range of valid values, i.e., what are the lower limit and upper limit of the hyperparameter value?Do you want to sample in linear scale or log scale? It is a common practice to sample in the log scale if the valid value range is large and the evaluation function changes more regularly with respect to the log domain, as shown in the following example for learning rate tuning. In this code example, we set the lower limit and the upper limit of the learning rate to be 1/1024 and 1.0, respectively. We sample in the log space because model performance changes more regularly in the log scale with respect to the learning rate within such a large search range.{ \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),}CopyWhen the search range of learning rate is small, it is more common to sample in the linear scale as shown in the following example,{ \"learning_rate\": tune.uniform(lower=0.1, upper=0.2),}CopyDo you have quantization granularity requirements?When you have a desired quantization granularity for the hyperparameter change, you can use tune.qlograndint or tune.qloguniform to realize the quantization requirement. The following code example helps you realize the need for sampling uniformly in the range of 0.1 and 0.2 with increments of 0.02, i.e., the sampled learning rate can only take values in {0.1, 0.12, 0.14, 0.16, ..., 0.2},{ \"learning_rate\": tune.quniform(lower=0.1, upper=0.2, q=0.02),}CopyYou can find the corresponding search space choice in the table below once you have answers to the aforementioned three questions.IntegerFloatlinear scaletune.randint(lower: int, upper: int)tune.uniform(lower: float, upper: float)log scaletune.lograndint(lower: int, upper: int, base: float = 10tune.loguniform(lower: float, upper: float, base: float = 10)linear scale with quantizationtune.qrandint(lower: int, upper: int, q: int = 1)tune.quniform(lower: float, upper: float, q: float = 1)log scale with quantizationtune.qlograndint(lower: int, upper, q: int = 1, base: float = 10)tune.qloguniform(lower: float, upper, q: float = 1, base: float = 10)See the example below for the commonly used types of domains.config = { # Sample a float uniformly between -5.0 and -1.0 \"uniform\": tune.uniform(-5, -1), # Sample a float uniformly between 3.2 and 5.4, # rounding to increments of 0.2 \"quniform\": tune.quniform(3.2, 5.4, 0.2), # Sample a float uniformly between 0.0001 and 0.01, while # sampling in log space \"loguniform\": tune.loguniform(1e-4, 1e-2), # Sample a float uniformly between 0.0001 and 0.1, while # sampling in log space and rounding to increments of 0.00005 \"qloguniform\": tune.qloguniform(1e-4, 1e-1, 5e-5), # Sample a random float from a normal distribution with # mean=10 and sd=2 \"randn\": tune.randn(10, 2), # Sample a random float from a normal distribution with # mean=10 and sd=2, rounding to increments of 0.2 \"qrandn\": tune.qrandn(10, 2, 0.2), # Sample a integer uniformly between -9 (inclusive) and 15 (exclusive) \"randint\": tune.randint(-9, 15), # Sample a random uniformly between -21 (inclusive) and 12 (inclusive (!)) # rounding to increments of 3 (includes 12) \"qrandint\": tune.qrandint(-21, 12, 3), # Sample a integer uniformly between 1 (inclusive) and 10 (exclusive), # while sampling in log space \"lograndint\": tune.lograndint(1, 10), # Sample a integer uniformly between 2 (inclusive) and 10 (inclusive (!)), # while sampling in log space and rounding to increments of 2 \"qlograndint\": tune.qlograndint(2, 10, 2), # Sample an option uniformly from the specified choices \"choice\": tune.choice([\"a\", \"b\", \"c\"]),}CopyCost-related hyperparameters\u200bCost-related hyperparameters are a subset of the hyperparameters which directly affect the computation cost incurred in the evaluation of any hyperparameter configuration. For example, the number of estimators (n_estimators) and the maximum number of leaves (max_leaves) are known to affect the training cost of tree-based learners. So they are cost-related hyperparameters for tree-based learners.When cost-related hyperparameters exist, the evaluation cost in the search space is heterogeneous.",
        "segment_5": "In this case, designing a search space with proper ranges of the hyperparameter values is highly non-trivial. Classical tuning algorithms such as Bayesian optimization and random search are typically sensitive to such ranges. It may take them a very high cost to find a good choice if the ranges are too large. And if the ranges are too small, the optimal choice(s) may not be included and thus not possible to be found. With our method, you can use a search space with larger ranges in the case of heterogeneous cost.Our search algorithms are designed to finish the tuning process at a low total cost when the evaluation cost in the search space is heterogeneous.",
        "segment_6": "So in such scenarios, if you are aware of low-cost configurations for the cost-related hyperparameters, you are encouraged to set them as the low_cost_partial_config, which is a dictionary of a subset of the hyperparameter coordinates whose value corresponds to a configuration with known low cost. Using the example of the tree-based methods again, since we know that small n_estimators and max_leaves generally correspond to simpler models and thus lower cost, we set {'n_estimators': 4, 'max_leaves': 4} as the low_cost_partial_config by default (note that 4 is the lower bound of search space for these two hyperparameters), e.g., in LGBM. Please find more details on how the algorithm works here.In addition, if you are aware of the cost relationship between different categorical hyperparameter choices, you are encouraged to provide this information through cat_hp_cost. It also helps the search algorithm to reduce the total cost.Tuning constraints\u200bRelated arguments:time_budget_s: The time budget in seconds.num_samples: An integer of the number of configs to try.config_constraints (optional): A list of config constraints to be satisfied.metric_constraints (optional): A list of metric constraints to be satisfied. e.g., ['precision', '>=', 0.9].The third step is to specify constraints of the tuning task. One notable property of flaml.tune is that it is able to finish the tuning process (obtaining good results) within a required resource constraint. A user can either provide the resource constraint in terms of wall-clock time (in seconds) through the argument time_budget_s, or in terms of the number of trials through the argument num_samples. The following example shows three use cases:# Set a resource constraint of 60 seconds wall-clock time for the tuning.flaml.tune.run(..., time_budget_s=60, ...)# Set a resource constraint of 100 trials for the tuning.flaml.tune.run(..., num_samples=100, ...)# Use at most 60 seconds and at most 100 trials for the tuning.flaml.tune.run(..., time_budget_s=60, num_samples=100, ...)CopyOptionally, you can provide a list of config constraints to be satisfied through the argument config_constraints and provide a list of metric constraints to be satisfied through the argument metric_constraints. We provide more details about related use cases in the Advanced Tuning Options section.Put together\u200bAfter the aforementioned key steps, one is ready to perform a tuning task by calling flaml.tune.run(). Below is a quick sequential tuning example using the pre-defined search space config_search_space and a minimization (mode='min') objective for the score metric evaluated in evaluate_config, using the default serach algorithm in flaml. The time budget is 10 seconds (time_budget_s=10).# require: pip install flaml[blendsearch]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds)CopyResult analysis\u200bOnce the tuning process finishes, it returns an ExperimentAnalysis object, which provides methods to analyze the tuning.In the following code example, we retrieve the best configuration found during the tuning, and retrieve the best trial's result from the returned analysis.analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds)print(analysis.best_config) # the best configprint(analysis.best_trial.last_result) # the best trial's resultCopyAdvanced Tuning Options\u200bThere are several advanced tuning options worth mentioning.More constraints on the tuning\u200bA user can specify constraints on the configurations to be satisfied via the argument config_constraints. The config_constraints receives a list of such constraints to be satisfied. Specifically, each constraint is a tuple that consists of (1) a function that takes a configuration as input and returns a numerical value; (2) an operation chosen from \"=\", \"\"; (3) a numerical threshold.In the following code example, we constrain the output of area, which takes a configuration as input and outputs a numerical value, to be no larger than 1000.def my_model_size(config): return config[\"n_estimators\"] * config[\"max_leaves\"]analysis = tune.run(..., config_constraints = [(my_model_size, \"=\"; (3) a numerical threshold. In the following code example, we constrain the metric training_cost to be no larger than 1 second.analysis = tune.run(..., metric_constraints = [(\"training_cost\", \"<=\", 1)]),Copyconfig_constraints vs metric_constraints:\u200bThe key difference between these two types of constraints is that the calculation of constraints in config_constraints does not rely on the computation procedure in the evaluation function, i.e., in evaluation_function. For example, when a constraint only depends on the config itself, as shown in the code example. Due to this independency, constraints in config_constraints will be checked before evaluation. So configurations that do not satisfy config_constraints will not be evaluated.Parallel tuning\u200bRelated arguments:use_ray: A boolean of whether to use ray as the backend.use_spark: A boolean of whether to use spark as the backend.resources_per_trial: A dictionary of the hardware resources to allocate per trial, e.g., {'cpu': 1}. Only valid when using ray backend.Details about parallel tuning with Spark could be found here.You can perform parallel tuning by specifying use_ray=True (requiring flaml[ray] option installed) or use_spark=True",
        "segment_7": "(requiring flaml[spark] option installed). You can also limit the amount of resources allocated per trial by specifying resources_per_trial,",
        "segment_8": "e.g., resources_per_trial={'cpu': 2} when use_ray=True.# require: pip install flaml[ray]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds use_ray=True, resources_per_trial={\"cpu\": 2} # limit resources allocated per trial)print(analysis.best_trial.last_result) # the best trial's resultprint(analysis.best_config) # the best configCopy# require: pip install flaml[spark]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds use_spark=True,)print(analysis.best_trial.last_result) # the best trial's resultprint(analysis.best_config) # the best configCopyA headsup about computation overhead. When parallel tuning is used, there will be a certain amount of computation overhead in each trial. In case each trial's original cost is much smaller than the overhead, parallel tuning can underperform sequential tuning. Sequential tuning is recommended when compute resource is limited, and each trial can consume all the resources.Trial scheduling\u200bRelated arguments:scheduler: A scheduler for executing the trials.resource_attr: A string to specify the resource dimension used by the scheduler.min_resource: A float of the minimal resource to use for the resource_attr.max_resource: A float of the maximal resource to use for the resource_attr.reduction_factor: A float of the reduction factor used for incremental pruning.A scheduler can help manage the trials' execution. It can be used to perform multi-fiedlity evalution, or/and early stopping. You can use two different types of schedulers in flaml.tune via scheduler.1. An authentic scheduler implemented in FLAML (scheduler='flaml').\u200bThis scheduler is authentic to the new search algorithms provided by FLAML. In a nutshell, it starts the search with the minimum resource. It switches between HPO with the current resource and increasing the resource for evaluation depending on which leads to faster improvement.If this scheduler is used, you need toSpecify a resource dimension. Conceptually a 'resource dimension' is a factor that affects the cost of the evaluation (e.g., sample size, the number of epochs). You need to specify the name of the resource dimension via resource_attr. For example, if resource_attr=\"sample_size\", then the config dict passed to the evaluation_function would contain a key \"sample_size\" and its value suggested by the search algorithm. That value should be used in the evaluation function to control the compute cost. The larger is the value, the more expensive the evaluation is.Provide the lower and upper limit of the resource dimension via min_resource and max_resource, and optionally provide reduction_factor, which determines the magnitude of resource (multiplicative) increase when we decide to increase the resource.In the following code example, we consider the sample size as the resource dimension. It determines how much data is used to perform training as reflected in the evaluation_function. We set the min_resource and max_resource to 1000 and the size of the full training dataset, respectively.from flaml import tunefrom functools import partialfrom flaml.automl.data import load_openml_taskdef obj_from_resource_attr(resource_attr, X_train, X_test, y_train, y_test, config): from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score # in this example sample size is our resource dimension resource = int(config[resource_attr]) sampled_X_train = X_train.iloc[:resource] sampled_y_train = y_train[:resource] # construct a LGBM model from the config # note that you need to first remove the resource_attr field # from the config as it is not part of the original search space model_config = config.copy() del model_config[resource_attr] model = LGBMClassifier(**model_config) model.fit(sampled_X_train, sampled_y_train) y_test_predict = model.predict(X_test) test_loss = 1.0 - accuracy_score(y_test, y_test_predict) return {resource_attr: resource, \"loss\": test_loss}X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"test/\")max_resource = len(y_train)resource_attr = \"sample_size\"min_resource = 1000analysis = tune.run( partial(obj_from_resource_attr, resource_attr, X_train, X_test, y_train, y_test), config={ \"n_estimators\": tune.lograndint(lower=4, upper=32768), \"max_leaves\": tune.lograndint(lower=4, upper=32768), \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0), }, metric=\"loss\", mode=\"min\", resource_attr=resource_attr, scheduler=\"flaml\", max_resource=max_resource, min_resource=min_resource, reduction_factor=2, time_budget_s=10, num_samples=-1,)CopyYou can find more details about this scheduler in this paper.2. A scheduler of the TrialScheduler class from ray.tune.\u200bThere is a handful of schedulers of this type implemented in ray.tune, for example, ASHA, HyperBand, BOHB, etc.To use this type of scheduler you can either (1) set scheduler='asha', which will automatically create an ASHAScheduler instance using the provided inputs (resource_attr, min_resource, max_resource, and reduction_factor); or (2) create an instance by yourself and provided it via scheduler, as shown in the following code example,# require: pip install flaml[ray]from ray.tune.schedulers import HyperBandSchedulermy_scheduler = HyperBandScheduler(time_attr=\"sample_size\", max_t=max_resource, reduction_factor=2)tune.run(.., scheduler=my_scheduler, ...)CopySimilar to the case where the flaml scheduler is used, you need to specify the resource dimension, use the resource dimension accordingly in your evaluation_function, and provide the necessary information needed for scheduling, such as min_resource, max_resource and reduction_factor (depending on the requirements of the specific scheduler).Different from the case when the flaml scheduler is used, the amount of resources to use at each iteration is not suggested by the search algorithm through the resource_attr in a configuration. You need to specify the evaluation schedule explicitly by yourself in the evaluation_function and report intermediate results (using tune.report()) accordingly. In the following code example, we use the ASHA scheduler by setting scheduler=\"asha\". We specify resource_attr, min_resource, min_resource and reduction_factor the same way as in the previous example (when \"flaml\" is used as the scheduler). We perform the evaluation in a customized schedule.Use ray backend or not? You can choose to use ray backend or not by specifying use_ray=True or use_ray=False. When ray backend is not used, i.e., use_ray=False, you also need to stop the evaluation function by explicitly catching the StopIteration exception, as shown in the end of the evaluation function obj_w_intermediate_report() in the following code example.def obj_w_intermediate_report(resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource, config): from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score # a customized schedule to perform the evaluation eval_schedule = [res for res in range(min_resource, max_resource, 5000)] + [max_resource] for resource in eval_schedule: sampled_X_train = X_train.iloc[:resource] sampled_y_train = y_train[:resource] # construct a LGBM model from the config model = LGBMClassifier(**config) model.fit(sampled_X_train, sampled_y_train) y_test_predict = model.predict(X_test) test_loss = 1.0 - accuracy_score(y_test, y_test_predict) # need to report the resource attribute used and the corresponding intermediate results try: tune.report(sample_size=resource, loss=test_loss) except (StopIteration, SystemExit): # do cleanup operation here returnresource_attr = \"sample_size\"min_resource = 1000max_resource = len(y_train)analysis = tune.run( partial(obj_w_intermediate_report, resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource), config={ \"n_estimators\": tune.lograndint(lower=4, upper=32768), \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0), }, metric=\"loss\", mode=\"min\", resource_attr=resource_attr, scheduler=\"asha\", max_resource=max_resource, min_resource=min_resource, reduction_factor=2, time_budget_s=10, num_samples = -1,)CopyIf you would like to do some cleanup opearation when the trial is stopped",
        "segment_9": "by the scheduler, you can do it when you catch the StopIteration (when not using ray) or SystemExit (when using ray) exception explicitly.Warm start\u200bRelated arguments:points_to_evaluate: A list of initial hyperparameter configurations to run first.evaluated_rewards: If you have previously evaluated the parameters passed in as points_to_evaluate , you can avoid re-running those trials by passing in the reward attributes as a list so the optimizer can be told the results without needing to re-compute the trial. Must be the same length or shorter length than points_to_evaluate.If you are aware of some good hyperparameter configurations, you are encouraged to provide them via points_to_evaluate. The search algorithm will try them first and use them to bootstrap the search.You can use previously evaluated configurations to warm-start your tuning.",
        "segment_10": "For example, the following code means that you know the reward for the two configs in",
        "segment_11": "points_to_evaluate are 3.99 and 1.99, respectively, and want to",
        "segment_12": "inform tune.run().def simple_obj(config): return config[\"a\"] + config[\"b\"]from flaml import tuneconfig_search_space = { \"a\": tune.uniform(lower=0, upper=0.99), \"b\": tune.uniform(lower=0, upper=3)}points_to_evaluate = [ {\"b\": .99, \"a\": 3}, {\"b\": .99, \"a\": 2}, {\"b\": .80, \"a\": 3}, {\"b\": .80, \"a\": 2},]evaluated_rewards = [3.99, 2.99]analysis = tune.run( simple_obj, config=config_search_space, mode=\"max\", points_to_evaluate=points_to_evaluate, evaluated_rewards=evaluated_rewards, time_budget_s=10, num_samples=-1,)CopyReproducibility\u200bBy default, there is randomness in our tuning process (for versions <= 0.9.1). If reproducibility is desired, you could manually set a random seed before calling tune.run(). For example, in the following code, we call np.random.seed(100) to set the random seed.",
        "segment_13": "With this random seed, running the following code multiple times will generate exactly the same search trajectory. The reproducibility can only be guaranteed in sequential tuning.import numpy as npnp.random.seed(100) # This line is not needed starting from version v0.9.2.analysis = tune.run( simple_obj, config=config_search_space, mode=\"max\", num_samples=10,)CopyLexicographic Objectives\u200bWe support tuning multiple objectives with lexicographic preference by providing argument lexico_objectives for tune.run().",
        "segment_14": "lexico_objectives is a dictionary that contains the following fields of key-value pairs:metrics: a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.modes: (optional) a list of optimization modes (each mode either \"min\" or \"max\") corresponding to the objectives in the metric list. If not provided, we use \"min\" as the default mode for all the objectives.tolerances: (optional) a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in \"metrics\"), and the values are the absolute/percentage tolerance in the form of numeric/string.targets: (optional) a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in \"metric\"), and the values are the numerical target values.In the following example, we want to minimize val_loss and pred_time of the model where val_loss has high priority. The tolerances for val_loss and pre_time are 0.02 and 0 respectively. We do not set targets for these two objectives and we set them to -inf for both objectives.lexico_objectives = {}lexico_objectives[\"metrics\"] = [\"val_loss\", \"pred_time\"]lexico_objectives[\"modes\"] = [\"min\", \"min\"]lexico_objectives[\"tolerances\"] = {\"val_loss\": 0.02, \"pred_time\": 0.0}lexico_objectives[\"targets\"] = {\"val_loss\": -float('inf'), \"pred_time\": -float('inf')}# provide the lexico_objectives to tune.runtune.run(..., search_alg=None, lexico_objectives=lexico_objectives)CopyWe also supports providing percentage tolerance as shown below.lexico_objectives[\"tolerances\"] = {\"val_loss\": \"10%\", \"pred_time\": \"0%\"}CopyNOTE:When lexico_objectives is not None, the arguments metric, mode, will be invalid, and flaml's tune uses CFO as the search_alg, which makes the input (if provided) search_alg invalid.This is a new feature that will be released in version 1.1.0 and is subject to change in the future version.Hyperparameter Optimization Algorithm\u200bTo tune the hyperparameters toward your objective, you will want to use a hyperparameter optimization algorithm which can help suggest hyperparameters with better performance (regarding your objective). flaml offers two HPO methods: CFO and BlendSearch. flaml.tune uses BlendSearch by default when the option [blendsearch] is installed.CFO: Frugal Optimization for Cost-related Hyperparameters\u200bCFO uses the randomized direct search method FLOW2 with adaptive stepsize and random restart.",
        "segment_15": "It requires a low-cost initial point as input if such point exists.",
        "segment_16": "The search begins with the low-cost initial point and gradually move to",
        "segment_17": "high cost region if needed. The local search method has a provable convergence",
        "segment_18": "rate and bounded cost.About FLOW2: FLOW2 is a simple yet effective randomized direct search method.",
        "segment_19": "It is an iterative optimization method that can optimize for black-box functions.",
        "segment_20": "FLOW2 only requires pairwise comparisons between function values to perform iterative update. Comparing to existing HPO methods, FLOW2 has the following appealing properties:It is applicable to general black-box functions with a good convergence rate in terms of loss.It provides theoretical guarantees on the total evaluation cost incurred.The GIFs attached below demonstrate an example search trajectory of FLOW2 shown in the loss and evaluation cost (i.e., the training time ) space respectively. FLOW2 is used in tuning the # of leaves and the # of trees for XGBoost. The two background heatmaps show the loss and cost distribution of all configurations. The black dots are the points evaluated in FLOW2. Black dots connected by lines are points that yield better loss performance when evaluated.From the demonstration, we can see that (1) FLOW2 can quickly move toward the low-loss region, showing good convergence property and (2) FLOW2 tends to avoid exploring the high-cost region until necessary.Example:from flaml import CFOtune.run(... search_alg=CFO(low_cost_partial_config=low_cost_partial_config),)CopyRecommended scenario: There exist cost-related hyperparameters and a low-cost",
        "segment_21": "initial point is known before optimization.",
        "segment_22": "If the search space is complex and CFO gets trapped into local optima, consider",
        "segment_23": "using BlendSearch.BlendSearch: Economical Hyperparameter Optimization With Blended Search Strategy\u200bBlendSearch combines local search with global search. It leverages the frugality",
        "segment_24": "of CFO and the space exploration ability of global search methods such as",
        "segment_25": "Bayesian optimization. Like CFO, BlendSearch requires a low-cost initial point",
        "segment_26": "as input if such point exists, and starts the search from there. Different from",
        "segment_27": "CFO, BlendSearch will not wait for the local search to fully converge before",
        "segment_28": "trying new start points. The new start points are suggested by the global search",
        "segment_29": "method and filtered based on their distance to the existing points in the",
        "segment_30": "cost-related dimensions. BlendSearch still gradually increases the trial cost.",
        "segment_31": "It prioritizes among the global search thread and multiple local search threads",
        "segment_32": "based on optimism in face of uncertainty.Example:# require: pip install flaml[blendsearch]from flaml import BlendSearchtune.run(... search_alg=BlendSearch(low_cost_partial_config=low_cost_partial_config),)CopyRecommended scenario: Cost-related hyperparameters exist, a low-cost",
        "segment_33": "initial point is known, and the search space is complex such that local search",
        "segment_34": "is prone to be stuck at local optima.Suggestion about using larger search space in BlendSearch.",
        "segment_35": "In hyperparameter optimization, a larger search space is desirable because it is more likely to include the optimal configuration (or one of the optimal configurations) in hindsight. However the performance (especially anytime performance) of most existing HPO methods is undesirable if the cost of the configurations in the search space has a large variation. Thus hand-crafted small search spaces (with relatively homogeneous cost) are often used in practice for these methods, which is subject to idiosyncrasy. BlendSearch combines the benefits of local search and global search, which enables a smart (economical) way of deciding where to explore in the search space even though it is larger than necessary. This allows users to specify a larger search space in BlendSearch, which is often easier and a better practice than narrowing down the search space by hand.For more technical details, please check our papers.Frugal Optimization for Cost-related Hyperparameters. Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.@inproceedings{wu2021cfo, title={Frugal Optimization for Cost-related Hyperparameters}, author={Qingyun Wu and Chi Wang and Silu Huang}, year={2021}, booktitle={AAAI'21},}CopyEconomical Hyperparameter Optimization With Blended Search Strategy. Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.@inproceedings{wang2021blendsearch, title={Economical Hyperparameter Optimization With Blended Search Strategy}, author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied}, year={2021}, booktitle={ICLR'21},}CopyTargeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives. Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).@inproceedings{zhang2023targeted, title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives}, author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu}, booktitle={International Conference on Learning Representations}, year={2023}, url={https://openreview.net/forum?id=0Ij9_q567Ma}}CopyEdit this pagePrevious\u00ab Task Oriented AutoMLNextZero Shot AutoML \u00bbBasic Tuning ProcedureTuning objectiveSearch spaceTuning constraintsPut togetherResult analysisAdvanced Tuning OptionsMore constraints on the tuningParallel tuningTrial schedulingWarm startReproducibilityLexicographic ObjectivesHyperparameter Optimization AlgorithmCFO: Frugal Optimization for Cost-related HyperparametersBlendSearch: Economical Hyperparameter Optimization With Blended Search StrategyCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "class AgentCapability()",
        "segment_2": "Base class for composable capabilities that can be added to an agent.",
        "segment_3": "add_to_agent\u200b",
        "segment_4": "def add_to_agent(agent: ConversableAgent)",
        "segment_5": "Adds a particular capability to the given agent. Must be implemented by the capability subclass.",
        "segment_6": "An implementation will typically call agent.register_hook() one or more times. See teachability.py as an example.Edit this pageNextteachabilityAgentCapability ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen enables collaboration among multiple ChatGPTs for complex tasks.",
        "segment_2": "TLDR\u200b",
        "segment_3": "OpenAI assistants are now integrated into AutoGen via GPTAssistantAgent.",
        "segment_4": "This enables multiple OpenAI assistants, which form the backend of the now popular GPTs, to collaborate and tackle complex tasks.",
        "segment_5": "Checkout example notebooks for reference:",
        "segment_7": "Basic example",
        "segment_8": "Code interpreter",
        "segment_9": "Function calls",
        "segment_11": "Introduction\u200b",
        "segment_12": "Earlier last week, OpenAI introduced GPTs, giving users ability to create custom ChatGPTs tailored for them.",
        "segment_13": "But what if these individual GPTs could collaborate to do even more?",
        "segment_14": "Fortunately, because of AutoGen, this is now a reality!",
        "segment_15": "AutoGen has been pioneering agents and supporting multi-agent workflows since earlier this year, and now (starting with version 0.2.0b5) we are introducing compatibility with the Assistant API, which is currently in beta preview.",
        "segment_16": "To accomplish this, we've added a new (experimental) agent called the GPTAssistantAgent that",
        "segment_17": "lets you seamlessly add these new OpenAI assistants into AutoGen-based multi-agent workflows.",
        "segment_18": "This integration shows great potential and synergy, and we plan to continue enhancing it.",
        "segment_19": "Installation\u200b",
        "segment_20": "pip install pyautogen==0.2.0b5",
        "segment_21": "Basic Example\u200b",
        "segment_22": "Here's a basic example that uses a UserProxyAgent to allow an interface",
        "segment_23": "with the GPTAssistantAgent.",
        "segment_24": "First, import the new agent and setup config_list:",
        "segment_25": "from autogen import config_list_from_jsonfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgentfrom autogen import UserProxyAgentconfig_list = config_list_from_json(\"OAI_CONFIG_LIST\")",
        "segment_26": "Then simply define the OpenAI assistant agent and give it the task!",
        "segment_27": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_28": "GPTAssistantAgent supports both creating new OpenAI assistants or reusing existing assistants",
        "segment_29": "(e.g, by providing an assistant_id).",
        "segment_30": "Code Interpreter Example\u200b",
        "segment_31": "GPTAssistantAgent allows you to specify an OpenAI tools",
        "segment_32": "(e.g., function calls, code interpreter, etc). The example below enables an assistant",
        "segment_33": "that can use OpenAI code interpreter to solve tasks.",
        "segment_34": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None, \"tools\": [ { \"type\": \"code_interpreter\" } ], })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_35": "Checkout more examples here.",
        "segment_36": "Limitations and Future Work\u200b",
        "segment_38": "Group chat managers using GPT assistant are pending.",
        "segment_39": "GPT assistants with multimodal capabilities haven't been released yet but we are committed to support them.",
        "segment_41": "Acknowledgements\u200b",
        "segment_42": "GPTAssistantAgent was made possible through collaboration with",
        "segment_43": "@IANTHEREAL,",
        "segment_44": "Jiale Liu,",
        "segment_45": "Yiran Wu,",
        "segment_46": "Qingyun Wu,",
        "segment_47": "Chi Wang, and many other AutoGen maintainers.Tags:openai-assistantCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesAutoGen for Large Language ModelsTask Oriented AutoMLTune User Defined FunctionZero Shot AutoMLExamplesContributingResearchOn this pageTune User Defined Functionflaml.tune is a module for economical hyperparameter tuning. It is used internally by flaml.AutoML. It can also be used to directly tune a user-defined function (UDF), which is not limited to machine learning model training. You can use flaml.tune instead of flaml.AutoML if one of the following is true:Your machine learning task is not one of the built-in tasks from flaml.AutoML.Your input cannot be represented as X_train + y_train or dataframe + label.The optimization metric is not measurable via validation data only. For example, when you want to directly optimize a downstream application instead of a model accuracy metric.You need to tune a function that may not even be a machine learning procedure.Basic Tuning Procedure\u200bThere are three essential steps (assuming the knowledge of the set of hyperparameters to tune) to use flaml.tune to finish a basic tuning task:Specify the tuning objective with respect to the hyperparameters.Specify a search space of the hyperparameters.Specify tuning constraints, including constraints on the resource budget to do the tuning, constraints on the configurations, or/and constraints on a (or multiple) particular metric(s).With these steps, you can perform a basic tuning task accordingly.Tuning objective\u200bRelated arguments:evaluation_function: A user-defined evaluation function.metric: A string of the metric name to optimize for.mode: A string in ['min', 'max'] to specify the objective as minimization or maximization.The first step is to specify your tuning objective.",
        "segment_2": "To do it, you should first specify your evaluation procedure (e.g., perform a machine learning model training and validation) with respect to the hyperparameters in a user-defined function evaluation_function.",
        "segment_3": "The function requires a hyperparameter configuration as input, and can simply return a metric value in a scalar or return a dictionary of metric name and metric value pairs.In the following code, we define an evaluation function with respect to two hyperparameters named x and y according to obj:=(x\u221285000)2\u2212x/yobj := (x-85000)^2 - x/yobj:=(x\u221285000)2\u2212x/y. Note that we use this toy example here for more accessible demonstration purposes. In real use cases, the evaluation function usually cannot be written in this closed form, but instead involves a black-box and expensive evaluation procedure. Please check out Tune HuggingFace, Tune PyTorch and Tune LightGBM for real examples of tuning tasks.import timedef evaluate_config(config: dict): \"\"\"evaluate a hyperparameter configuration\"\"\" score = (config[\"x\"] - 85000) ** 2 - config[\"x\"] / config[\"y\"] # usually the evaluation takes an non-neglible cost # and the cost could be related to certain hyperparameters # here we simulate this cost by calling the time.sleep() function # here we assume the cost is proportional to x faked_evaluation_cost = config[\"x\"] / 100000 time.sleep(faked_evaluation_cost) # we can return a single float as a score on the input config: # return score # or, we can return a dictionary that maps metric name to metric value: return {\"score\": score, \"evaluation_cost\": faked_evaluation_cost, \"constraint_metric\": config[\"x\"] * config[\"y\"]}CopyWhen the evaluation function returns a dictionary of metrics, you need to specify the name of the metric to optimize via the argument metric (this can be skipped when the function is just returning a scalar). In addition, you need to specify a mode of your optimization/tuning task (maximization or minimization) via the argument mode by choosing from \"min\" or \"max\".For example,flaml.tune.run(evaluation_function=evaluate_config, metric=\"score\", mode=\"min\", ...)CopySearch space\u200bRelated arguments:config: A dictionary to specify the search space.low_cost_partial_config (optional): A dictionary from a subset of controlled dimensions to the initial low-cost values.cat_hp_cost (optional): A dictionary from a subset of categorical dimensions to the relative cost of each choice.The second step is to specify a search space of the hyperparameters through the argument config. In the search space, you need to specify valid values for your hyperparameters and can specify how these values are sampled (e.g., from a uniform distribution or a log-uniform distribution).In the following code example, we include a search space for the two hyperparameters x and y as introduced above. The valid values for both are integers in the range of [1, 100000]. The values for x are sampled uniformly in the specified range (using tune.randint(lower=1, upper=100000)), and the values for y are sampled uniformly in logarithmic space of the specified range (using tune.lograndit(lower=1, upper=100000)).from flaml import tune# construct a search space for the hyperparameters x and y.config_search_space = { \"x\": tune.lograndint(lower=1, upper=100000), \"y\": tune.randint(lower=1, upper=100000)}# provide the search space to tune.runtune.run(..., config=config_search_space, ...)CopyDetails and guidelines on hyperparameter search space\u200bThe corresponding value of a particular hyperparameter in the search space dictionary is called a domain, for example, tune.randint(lower=1, upper=100000) is the domain for the hyperparameter y.",
        "segment_4": "The domain specifies a type and valid range to sample parameters from. Supported types include float, integer, and categorical.Categorical hyperparameterIf it is a categorical hyperparameter, then you should use tune.choice(possible_choices) in which possible_choices is the list of possible categorical values of the hyperparameter. For example, if you are tuning the optimizer used in model training, and the candidate optimizers are \"sgd\" and \"adam\", you should specify the search space in the following way:{ \"optimizer\": tune.choice([\"sgd\", \"adam\"]),}CopyNumerical hyperparameterIf it is a numerical hyperparameter, you need to know whether it takes integer values or float values. In addition, you need to know:The range of valid values, i.e., what are the lower limit and upper limit of the hyperparameter value?Do you want to sample in linear scale or log scale? It is a common practice to sample in the log scale if the valid value range is large and the evaluation function changes more regularly with respect to the log domain, as shown in the following example for learning rate tuning. In this code example, we set the lower limit and the upper limit of the learning rate to be 1/1024 and 1.0, respectively. We sample in the log space because model performance changes more regularly in the log scale with respect to the learning rate within such a large search range.{ \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0),}CopyWhen the search range of learning rate is small, it is more common to sample in the linear scale as shown in the following example,{ \"learning_rate\": tune.uniform(lower=0.1, upper=0.2),}CopyDo you have quantization granularity requirements?When you have a desired quantization granularity for the hyperparameter change, you can use tune.qlograndint or tune.qloguniform to realize the quantization requirement. The following code example helps you realize the need for sampling uniformly in the range of 0.1 and 0.2 with increments of 0.02, i.e., the sampled learning rate can only take values in {0.1, 0.12, 0.14, 0.16, ..., 0.2},{ \"learning_rate\": tune.quniform(lower=0.1, upper=0.2, q=0.02),}CopyYou can find the corresponding search space choice in the table below once you have answers to the aforementioned three questions.IntegerFloatlinear scaletune.randint(lower: int, upper: int)tune.uniform(lower: float, upper: float)log scaletune.lograndint(lower: int, upper: int, base: float = 10tune.loguniform(lower: float, upper: float, base: float = 10)linear scale with quantizationtune.qrandint(lower: int, upper: int, q: int = 1)tune.quniform(lower: float, upper: float, q: float = 1)log scale with quantizationtune.qlograndint(lower: int, upper, q: int = 1, base: float = 10)tune.qloguniform(lower: float, upper, q: float = 1, base: float = 10)See the example below for the commonly used types of domains.config = { # Sample a float uniformly between -5.0 and -1.0 \"uniform\": tune.uniform(-5, -1), # Sample a float uniformly between 3.2 and 5.4, # rounding to increments of 0.2 \"quniform\": tune.quniform(3.2, 5.4, 0.2), # Sample a float uniformly between 0.0001 and 0.01, while # sampling in log space \"loguniform\": tune.loguniform(1e-4, 1e-2), # Sample a float uniformly between 0.0001 and 0.1, while # sampling in log space and rounding to increments of 0.00005 \"qloguniform\": tune.qloguniform(1e-4, 1e-1, 5e-5), # Sample a random float from a normal distribution with # mean=10 and sd=2 \"randn\": tune.randn(10, 2), # Sample a random float from a normal distribution with # mean=10 and sd=2, rounding to increments of 0.2 \"qrandn\": tune.qrandn(10, 2, 0.2), # Sample a integer uniformly between -9 (inclusive) and 15 (exclusive) \"randint\": tune.randint(-9, 15), # Sample a random uniformly between -21 (inclusive) and 12 (inclusive (!)) # rounding to increments of 3 (includes 12) \"qrandint\": tune.qrandint(-21, 12, 3), # Sample a integer uniformly between 1 (inclusive) and 10 (exclusive), # while sampling in log space \"lograndint\": tune.lograndint(1, 10), # Sample a integer uniformly between 2 (inclusive) and 10 (inclusive (!)), # while sampling in log space and rounding to increments of 2 \"qlograndint\": tune.qlograndint(2, 10, 2), # Sample an option uniformly from the specified choices \"choice\": tune.choice([\"a\", \"b\", \"c\"]),}CopyCost-related hyperparameters\u200bCost-related hyperparameters are a subset of the hyperparameters which directly affect the computation cost incurred in the evaluation of any hyperparameter configuration. For example, the number of estimators (n_estimators) and the maximum number of leaves (max_leaves) are known to affect the training cost of tree-based learners. So they are cost-related hyperparameters for tree-based learners.When cost-related hyperparameters exist, the evaluation cost in the search space is heterogeneous.",
        "segment_5": "In this case, designing a search space with proper ranges of the hyperparameter values is highly non-trivial. Classical tuning algorithms such as Bayesian optimization and random search are typically sensitive to such ranges. It may take them a very high cost to find a good choice if the ranges are too large. And if the ranges are too small, the optimal choice(s) may not be included and thus not possible to be found. With our method, you can use a search space with larger ranges in the case of heterogeneous cost.Our search algorithms are designed to finish the tuning process at a low total cost when the evaluation cost in the search space is heterogeneous.",
        "segment_6": "So in such scenarios, if you are aware of low-cost configurations for the cost-related hyperparameters, you are encouraged to set them as the low_cost_partial_config, which is a dictionary of a subset of the hyperparameter coordinates whose value corresponds to a configuration with known low cost. Using the example of the tree-based methods again, since we know that small n_estimators and max_leaves generally correspond to simpler models and thus lower cost, we set {'n_estimators': 4, 'max_leaves': 4} as the low_cost_partial_config by default (note that 4 is the lower bound of search space for these two hyperparameters), e.g., in LGBM. Please find more details on how the algorithm works here.In addition, if you are aware of the cost relationship between different categorical hyperparameter choices, you are encouraged to provide this information through cat_hp_cost. It also helps the search algorithm to reduce the total cost.Tuning constraints\u200bRelated arguments:time_budget_s: The time budget in seconds.num_samples: An integer of the number of configs to try.config_constraints (optional): A list of config constraints to be satisfied.metric_constraints (optional): A list of metric constraints to be satisfied. e.g., ['precision', '>=', 0.9].The third step is to specify constraints of the tuning task. One notable property of flaml.tune is that it is able to finish the tuning process (obtaining good results) within a required resource constraint. A user can either provide the resource constraint in terms of wall-clock time (in seconds) through the argument time_budget_s, or in terms of the number of trials through the argument num_samples. The following example shows three use cases:# Set a resource constraint of 60 seconds wall-clock time for the tuning.flaml.tune.run(..., time_budget_s=60, ...)# Set a resource constraint of 100 trials for the tuning.flaml.tune.run(..., num_samples=100, ...)# Use at most 60 seconds and at most 100 trials for the tuning.flaml.tune.run(..., time_budget_s=60, num_samples=100, ...)CopyOptionally, you can provide a list of config constraints to be satisfied through the argument config_constraints and provide a list of metric constraints to be satisfied through the argument metric_constraints. We provide more details about related use cases in the Advanced Tuning Options section.Put together\u200bAfter the aforementioned key steps, one is ready to perform a tuning task by calling flaml.tune.run(). Below is a quick sequential tuning example using the pre-defined search space config_search_space and a minimization (mode='min') objective for the score metric evaluated in evaluate_config, using the default serach algorithm in flaml. The time budget is 10 seconds (time_budget_s=10).# require: pip install flaml[blendsearch]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds)CopyResult analysis\u200bOnce the tuning process finishes, it returns an ExperimentAnalysis object, which provides methods to analyze the tuning.In the following code example, we retrieve the best configuration found during the tuning, and retrieve the best trial's result from the returned analysis.analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds)print(analysis.best_config) # the best configprint(analysis.best_trial.last_result) # the best trial's resultCopyAdvanced Tuning Options\u200bThere are several advanced tuning options worth mentioning.More constraints on the tuning\u200bA user can specify constraints on the configurations to be satisfied via the argument config_constraints. The config_constraints receives a list of such constraints to be satisfied. Specifically, each constraint is a tuple that consists of (1) a function that takes a configuration as input and returns a numerical value; (2) an operation chosen from \"=\", \"\"; (3) a numerical threshold.In the following code example, we constrain the output of area, which takes a configuration as input and outputs a numerical value, to be no larger than 1000.def my_model_size(config): return config[\"n_estimators\"] * config[\"max_leaves\"]analysis = tune.run(..., config_constraints = [(my_model_size, \"=\"; (3) a numerical threshold. In the following code example, we constrain the metric training_cost to be no larger than 1 second.analysis = tune.run(..., metric_constraints = [(\"training_cost\", \"<=\", 1)]),Copyconfig_constraints vs metric_constraints:\u200bThe key difference between these two types of constraints is that the calculation of constraints in config_constraints does not rely on the computation procedure in the evaluation function, i.e., in evaluation_function. For example, when a constraint only depends on the config itself, as shown in the code example. Due to this independency, constraints in config_constraints will be checked before evaluation. So configurations that do not satisfy config_constraints will not be evaluated.Parallel tuning\u200bRelated arguments:use_ray: A boolean of whether to use ray as the backend.use_spark: A boolean of whether to use spark as the backend.resources_per_trial: A dictionary of the hardware resources to allocate per trial, e.g., {'cpu': 1}. Only valid when using ray backend.Details about parallel tuning with Spark could be found here.You can perform parallel tuning by specifying use_ray=True (requiring flaml[ray] option installed) or use_spark=True",
        "segment_7": "(requiring flaml[spark] option installed). You can also limit the amount of resources allocated per trial by specifying resources_per_trial,",
        "segment_8": "e.g., resources_per_trial={'cpu': 2} when use_ray=True.# require: pip install flaml[ray]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds use_ray=True, resources_per_trial={\"cpu\": 2} # limit resources allocated per trial)print(analysis.best_trial.last_result) # the best trial's resultprint(analysis.best_config) # the best configCopy# require: pip install flaml[spark]analysis = tune.run( evaluate_config, # the function to evaluate a config config=config_search_space, # the search space defined metric=\"score\", mode=\"min\", # the optimization mode, \"min\" or \"max\" num_samples=-1, # the maximal number of configs to try, -1 means infinite time_budget_s=10, # the time budget in seconds use_spark=True,)print(analysis.best_trial.last_result) # the best trial's resultprint(analysis.best_config) # the best configCopyA headsup about computation overhead. When parallel tuning is used, there will be a certain amount of computation overhead in each trial. In case each trial's original cost is much smaller than the overhead, parallel tuning can underperform sequential tuning. Sequential tuning is recommended when compute resource is limited, and each trial can consume all the resources.Trial scheduling\u200bRelated arguments:scheduler: A scheduler for executing the trials.resource_attr: A string to specify the resource dimension used by the scheduler.min_resource: A float of the minimal resource to use for the resource_attr.max_resource: A float of the maximal resource to use for the resource_attr.reduction_factor: A float of the reduction factor used for incremental pruning.A scheduler can help manage the trials' execution. It can be used to perform multi-fiedlity evalution, or/and early stopping. You can use two different types of schedulers in flaml.tune via scheduler.1. An authentic scheduler implemented in FLAML (scheduler='flaml').\u200bThis scheduler is authentic to the new search algorithms provided by FLAML. In a nutshell, it starts the search with the minimum resource. It switches between HPO with the current resource and increasing the resource for evaluation depending on which leads to faster improvement.If this scheduler is used, you need toSpecify a resource dimension. Conceptually a 'resource dimension' is a factor that affects the cost of the evaluation (e.g., sample size, the number of epochs). You need to specify the name of the resource dimension via resource_attr. For example, if resource_attr=\"sample_size\", then the config dict passed to the evaluation_function would contain a key \"sample_size\" and its value suggested by the search algorithm. That value should be used in the evaluation function to control the compute cost. The larger is the value, the more expensive the evaluation is.Provide the lower and upper limit of the resource dimension via min_resource and max_resource, and optionally provide reduction_factor, which determines the magnitude of resource (multiplicative) increase when we decide to increase the resource.In the following code example, we consider the sample size as the resource dimension. It determines how much data is used to perform training as reflected in the evaluation_function. We set the min_resource and max_resource to 1000 and the size of the full training dataset, respectively.from flaml import tunefrom functools import partialfrom flaml.automl.data import load_openml_taskdef obj_from_resource_attr(resource_attr, X_train, X_test, y_train, y_test, config): from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score # in this example sample size is our resource dimension resource = int(config[resource_attr]) sampled_X_train = X_train.iloc[:resource] sampled_y_train = y_train[:resource] # construct a LGBM model from the config # note that you need to first remove the resource_attr field # from the config as it is not part of the original search space model_config = config.copy() del model_config[resource_attr] model = LGBMClassifier(**model_config) model.fit(sampled_X_train, sampled_y_train) y_test_predict = model.predict(X_test) test_loss = 1.0 - accuracy_score(y_test, y_test_predict) return {resource_attr: resource, \"loss\": test_loss}X_train, X_test, y_train, y_test = load_openml_task(task_id=7592, data_dir=\"test/\")max_resource = len(y_train)resource_attr = \"sample_size\"min_resource = 1000analysis = tune.run( partial(obj_from_resource_attr, resource_attr, X_train, X_test, y_train, y_test), config={ \"n_estimators\": tune.lograndint(lower=4, upper=32768), \"max_leaves\": tune.lograndint(lower=4, upper=32768), \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0), }, metric=\"loss\", mode=\"min\", resource_attr=resource_attr, scheduler=\"flaml\", max_resource=max_resource, min_resource=min_resource, reduction_factor=2, time_budget_s=10, num_samples=-1,)CopyYou can find more details about this scheduler in this paper.2. A scheduler of the TrialScheduler class from ray.tune.\u200bThere is a handful of schedulers of this type implemented in ray.tune, for example, ASHA, HyperBand, BOHB, etc.To use this type of scheduler you can either (1) set scheduler='asha', which will automatically create an ASHAScheduler instance using the provided inputs (resource_attr, min_resource, max_resource, and reduction_factor); or (2) create an instance by yourself and provided it via scheduler, as shown in the following code example,# require: pip install flaml[ray]from ray.tune.schedulers import HyperBandSchedulermy_scheduler = HyperBandScheduler(time_attr=\"sample_size\", max_t=max_resource, reduction_factor=2)tune.run(.., scheduler=my_scheduler, ...)CopySimilar to the case where the flaml scheduler is used, you need to specify the resource dimension, use the resource dimension accordingly in your evaluation_function, and provide the necessary information needed for scheduling, such as min_resource, max_resource and reduction_factor (depending on the requirements of the specific scheduler).Different from the case when the flaml scheduler is used, the amount of resources to use at each iteration is not suggested by the search algorithm through the resource_attr in a configuration. You need to specify the evaluation schedule explicitly by yourself in the evaluation_function and report intermediate results (using tune.report()) accordingly. In the following code example, we use the ASHA scheduler by setting scheduler=\"asha\". We specify resource_attr, min_resource, min_resource and reduction_factor the same way as in the previous example (when \"flaml\" is used as the scheduler). We perform the evaluation in a customized schedule.Use ray backend or not? You can choose to use ray backend or not by specifying use_ray=True or use_ray=False. When ray backend is not used, i.e., use_ray=False, you also need to stop the evaluation function by explicitly catching the StopIteration exception, as shown in the end of the evaluation function obj_w_intermediate_report() in the following code example.def obj_w_intermediate_report(resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource, config): from lightgbm import LGBMClassifier from sklearn.metrics import accuracy_score # a customized schedule to perform the evaluation eval_schedule = [res for res in range(min_resource, max_resource, 5000)] + [max_resource] for resource in eval_schedule: sampled_X_train = X_train.iloc[:resource] sampled_y_train = y_train[:resource] # construct a LGBM model from the config model = LGBMClassifier(**config) model.fit(sampled_X_train, sampled_y_train) y_test_predict = model.predict(X_test) test_loss = 1.0 - accuracy_score(y_test, y_test_predict) # need to report the resource attribute used and the corresponding intermediate results try: tune.report(sample_size=resource, loss=test_loss) except (StopIteration, SystemExit): # do cleanup operation here returnresource_attr = \"sample_size\"min_resource = 1000max_resource = len(y_train)analysis = tune.run( partial(obj_w_intermediate_report, resource_attr, X_train, X_test, y_train, y_test, min_resource, max_resource), config={ \"n_estimators\": tune.lograndint(lower=4, upper=32768), \"learning_rate\": tune.loguniform(lower=1 / 1024, upper=1.0), }, metric=\"loss\", mode=\"min\", resource_attr=resource_attr, scheduler=\"asha\", max_resource=max_resource, min_resource=min_resource, reduction_factor=2, time_budget_s=10, num_samples = -1,)CopyIf you would like to do some cleanup opearation when the trial is stopped",
        "segment_9": "by the scheduler, you can do it when you catch the StopIteration (when not using ray) or SystemExit (when using ray) exception explicitly.Warm start\u200bRelated arguments:points_to_evaluate: A list of initial hyperparameter configurations to run first.evaluated_rewards: If you have previously evaluated the parameters passed in as points_to_evaluate , you can avoid re-running those trials by passing in the reward attributes as a list so the optimizer can be told the results without needing to re-compute the trial. Must be the same length or shorter length than points_to_evaluate.If you are aware of some good hyperparameter configurations, you are encouraged to provide them via points_to_evaluate. The search algorithm will try them first and use them to bootstrap the search.You can use previously evaluated configurations to warm-start your tuning.",
        "segment_10": "For example, the following code means that you know the reward for the two configs in",
        "segment_11": "points_to_evaluate are 3.99 and 1.99, respectively, and want to",
        "segment_12": "inform tune.run().def simple_obj(config): return config[\"a\"] + config[\"b\"]from flaml import tuneconfig_search_space = { \"a\": tune.uniform(lower=0, upper=0.99), \"b\": tune.uniform(lower=0, upper=3)}points_to_evaluate = [ {\"b\": .99, \"a\": 3}, {\"b\": .99, \"a\": 2}, {\"b\": .80, \"a\": 3}, {\"b\": .80, \"a\": 2},]evaluated_rewards = [3.99, 2.99]analysis = tune.run( simple_obj, config=config_search_space, mode=\"max\", points_to_evaluate=points_to_evaluate, evaluated_rewards=evaluated_rewards, time_budget_s=10, num_samples=-1,)CopyReproducibility\u200bBy default, there is randomness in our tuning process (for versions <= 0.9.1). If reproducibility is desired, you could manually set a random seed before calling tune.run(). For example, in the following code, we call np.random.seed(100) to set the random seed.",
        "segment_13": "With this random seed, running the following code multiple times will generate exactly the same search trajectory. The reproducibility can only be guaranteed in sequential tuning.import numpy as npnp.random.seed(100) # This line is not needed starting from version v0.9.2.analysis = tune.run( simple_obj, config=config_search_space, mode=\"max\", num_samples=10,)CopyLexicographic Objectives\u200bWe support tuning multiple objectives with lexicographic preference by providing argument lexico_objectives for tune.run().",
        "segment_14": "lexico_objectives is a dictionary that contains the following fields of key-value pairs:metrics: a list of optimization objectives with the orders reflecting the priorities/preferences of the objectives.modes: (optional) a list of optimization modes (each mode either \"min\" or \"max\") corresponding to the objectives in the metric list. If not provided, we use \"min\" as the default mode for all the objectives.tolerances: (optional) a dictionary to specify the optimality tolerances on objectives. The keys are the metric names (provided in \"metrics\"), and the values are the absolute/percentage tolerance in the form of numeric/string.targets: (optional) a dictionary to specify the optimization targets on the objectives. The keys are the metric names (provided in \"metric\"), and the values are the numerical target values.In the following example, we want to minimize val_loss and pred_time of the model where val_loss has high priority. The tolerances for val_loss and pre_time are 0.02 and 0 respectively. We do not set targets for these two objectives and we set them to -inf for both objectives.lexico_objectives = {}lexico_objectives[\"metrics\"] = [\"val_loss\", \"pred_time\"]lexico_objectives[\"modes\"] = [\"min\", \"min\"]lexico_objectives[\"tolerances\"] = {\"val_loss\": 0.02, \"pred_time\": 0.0}lexico_objectives[\"targets\"] = {\"val_loss\": -float('inf'), \"pred_time\": -float('inf')}# provide the lexico_objectives to tune.runtune.run(..., search_alg=None, lexico_objectives=lexico_objectives)CopyWe also supports providing percentage tolerance as shown below.lexico_objectives[\"tolerances\"] = {\"val_loss\": \"10%\", \"pred_time\": \"0%\"}CopyNOTE:When lexico_objectives is not None, the arguments metric, mode, will be invalid, and flaml's tune uses CFO as the search_alg, which makes the input (if provided) search_alg invalid.This is a new feature that will be released in version 1.1.0 and is subject to change in the future version.Hyperparameter Optimization Algorithm\u200bTo tune the hyperparameters toward your objective, you will want to use a hyperparameter optimization algorithm which can help suggest hyperparameters with better performance (regarding your objective). flaml offers two HPO methods: CFO and BlendSearch. flaml.tune uses BlendSearch by default when the option [blendsearch] is installed.CFO: Frugal Optimization for Cost-related Hyperparameters\u200bCFO uses the randomized direct search method FLOW2 with adaptive stepsize and random restart.",
        "segment_15": "It requires a low-cost initial point as input if such point exists.",
        "segment_16": "The search begins with the low-cost initial point and gradually move to",
        "segment_17": "high cost region if needed. The local search method has a provable convergence",
        "segment_18": "rate and bounded cost.About FLOW2: FLOW2 is a simple yet effective randomized direct search method.",
        "segment_19": "It is an iterative optimization method that can optimize for black-box functions.",
        "segment_20": "FLOW2 only requires pairwise comparisons between function values to perform iterative update. Comparing to existing HPO methods, FLOW2 has the following appealing properties:It is applicable to general black-box functions with a good convergence rate in terms of loss.It provides theoretical guarantees on the total evaluation cost incurred.The GIFs attached below demonstrate an example search trajectory of FLOW2 shown in the loss and evaluation cost (i.e., the training time ) space respectively. FLOW2 is used in tuning the # of leaves and the # of trees for XGBoost. The two background heatmaps show the loss and cost distribution of all configurations. The black dots are the points evaluated in FLOW2. Black dots connected by lines are points that yield better loss performance when evaluated.From the demonstration, we can see that (1) FLOW2 can quickly move toward the low-loss region, showing good convergence property and (2) FLOW2 tends to avoid exploring the high-cost region until necessary.Example:from flaml import CFOtune.run(... search_alg=CFO(low_cost_partial_config=low_cost_partial_config),)CopyRecommended scenario: There exist cost-related hyperparameters and a low-cost",
        "segment_21": "initial point is known before optimization.",
        "segment_22": "If the search space is complex and CFO gets trapped into local optima, consider",
        "segment_23": "using BlendSearch.BlendSearch: Economical Hyperparameter Optimization With Blended Search Strategy\u200bBlendSearch combines local search with global search. It leverages the frugality",
        "segment_24": "of CFO and the space exploration ability of global search methods such as",
        "segment_25": "Bayesian optimization. Like CFO, BlendSearch requires a low-cost initial point",
        "segment_26": "as input if such point exists, and starts the search from there. Different from",
        "segment_27": "CFO, BlendSearch will not wait for the local search to fully converge before",
        "segment_28": "trying new start points. The new start points are suggested by the global search",
        "segment_29": "method and filtered based on their distance to the existing points in the",
        "segment_30": "cost-related dimensions. BlendSearch still gradually increases the trial cost.",
        "segment_31": "It prioritizes among the global search thread and multiple local search threads",
        "segment_32": "based on optimism in face of uncertainty.Example:# require: pip install flaml[blendsearch]from flaml import BlendSearchtune.run(... search_alg=BlendSearch(low_cost_partial_config=low_cost_partial_config),)CopyRecommended scenario: Cost-related hyperparameters exist, a low-cost",
        "segment_33": "initial point is known, and the search space is complex such that local search",
        "segment_34": "is prone to be stuck at local optima.Suggestion about using larger search space in BlendSearch.",
        "segment_35": "In hyperparameter optimization, a larger search space is desirable because it is more likely to include the optimal configuration (or one of the optimal configurations) in hindsight. However the performance (especially anytime performance) of most existing HPO methods is undesirable if the cost of the configurations in the search space has a large variation. Thus hand-crafted small search spaces (with relatively homogeneous cost) are often used in practice for these methods, which is subject to idiosyncrasy. BlendSearch combines the benefits of local search and global search, which enables a smart (economical) way of deciding where to explore in the search space even though it is larger than necessary. This allows users to specify a larger search space in BlendSearch, which is often easier and a better practice than narrowing down the search space by hand.For more technical details, please check our papers.Frugal Optimization for Cost-related Hyperparameters. Qingyun Wu, Chi Wang, Silu Huang. AAAI 2021.@inproceedings{wu2021cfo, title={Frugal Optimization for Cost-related Hyperparameters}, author={Qingyun Wu and Chi Wang and Silu Huang}, year={2021}, booktitle={AAAI'21},}CopyEconomical Hyperparameter Optimization With Blended Search Strategy. Chi Wang, Qingyun Wu, Silu Huang, Amin Saied. ICLR 2021.@inproceedings{wang2021blendsearch, title={Economical Hyperparameter Optimization With Blended Search Strategy}, author={Chi Wang and Qingyun Wu and Silu Huang and Amin Saied}, year={2021}, booktitle={ICLR'21},}CopyTargeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives. Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu. ICLR 2023 (notable-top-5%).@inproceedings{zhang2023targeted, title={Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives}, author={Shaokun Zhang and Feiran Jia and Chi Wang and Qingyun Wu}, booktitle={International Conference on Learning Representations}, year={2023}, url={https://openreview.net/forum?id=0Ij9_q567Ma}}CopyEdit this pagePrevious\u00ab Task Oriented AutoMLNextZero Shot AutoML \u00bbBasic Tuning ProcedureTuning objectiveSearch spaceTuning constraintsPut togetherResult analysisAdvanced Tuning OptionsMore constraints on the tuningParallel tuningTrial schedulingWarm startReproducibilityLexicographic ObjectivesHyperparameter Optimization AlgorithmCFO: Frugal Optimization for Cost-related HyperparametersBlendSearch: Economical Hyperparameter Optimization With Blended Search StrategyCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "class MultimodalConversableAgent(ConversableAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name: str, system_message: Optional[Union[str, List]] = DEFAULT_LMM_SYS_MSG, is_termination_msg: str = None, *args, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - agent name.",
        "segment_7": "system_message str - system message for the OpenAIWrapper inference.",
        "segment_8": "Please override this attribute if you want to reprogram the agent.",
        "segment_9": "**kwargs dict - Please refer to other kwargs in",
        "segment_10": "ConversableAgent.",
        "segment_12": "update_system_message\u200b",
        "segment_13": "def update_system_message(system_message: Union[Dict, List, str])",
        "segment_14": "Update the system message.",
        "segment_15": "Arguments:",
        "segment_17": "system_message str - system message for the OpenAIWrapper inference."
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_62": "TL;DR:",
        "segment_64": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_65": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_66": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_68": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_69": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_70": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_71": "The MathChat Framework\u200b",
        "segment_72": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_73": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_76": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_79": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_81": "Writing a Python program to solve the problem directly.",
        "segment_82": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_83": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_87": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_90": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_91": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_92": "(The conversation is modified for readability.)",
        "segment_93": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_94": "Experiment Setup\u200b",
        "segment_95": "We evaluate the improvement brought by MathChat.",
        "segment_96": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_97": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_98": "We also evaluate the following methods for comparison:",
        "segment_101": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_104": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_107": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_110": "Experiment Results\u200b",
        "segment_111": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_113": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_114": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_115": "The code for experiments can be found at this repository.",
        "segment_116": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_117": "Future Directions\u200b",
        "segment_118": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_119": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_121": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_122": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_123": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_125": "For Further Reading\u200b",
        "segment_127": "Research paper of MathChat",
        "segment_128": "Documentation about autogen",
        "segment_130": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_131": "TL;DR:",
        "segment_133": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_135": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_136": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_137": "Observations\u200b",
        "segment_139": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_140": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_142": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_143": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_144": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_145": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_146": "What else can we do? We notice that:",
        "segment_147": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_148": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_149": "Solution\u200b",
        "segment_150": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_152": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_153": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_156": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_157": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_159": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_160": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_161": "GPT-4, n=1, temperature=0",
        "segment_162": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_163": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_165": "Experiment Results\u200b",
        "segment_166": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_167": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_168": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_170": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_172": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_174": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_175": "Solved by GPT-4, n=1, temperature=0:",
        "segment_177": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_179": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_181": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_183": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_185": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_186": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_188": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_189": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_191": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_192": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_193": "Discussion\u200b",
        "segment_194": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_195": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_197": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_198": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_200": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_201": "Different configurations have complementary effect.",
        "segment_205": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_206": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_207": "There are many directions of extensions in research and development:",
        "segment_209": "Generalize the way to provide feedback.",
        "segment_210": "Automate the process of optimizing the configurations.",
        "segment_211": "Build adaptive agents for different applications.",
        "segment_213": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_214": "For Further Reading\u200b",
        "segment_216": "Documentation about autogen and Research paper.",
        "segment_217": "Blog post about a related study for math.",
        "segment_218": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_219": "TL;DR:",
        "segment_221": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_222": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_223": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_225": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_226": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_227": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_228": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_229": "Experiment Setup\u200b",
        "segment_230": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_232": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_233": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_235": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_237": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_238": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_239": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_240": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_241": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_243": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_244": "Experiment Results\u200b",
        "segment_245": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_246": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_247": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_249": "However, the selected model changes on level 4 Algebra.",
        "segment_251": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_252": "On level 5 the result is similar.",
        "segment_254": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_255": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_256": "Analysis and Discussion\u200b",
        "segment_257": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_258": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_259": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_260": "For Further Reading\u200b",
        "segment_262": "Research paper about the tuning technique",
        "segment_263": "Documentation about inference tuning",
        "segment_265": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class ConversableAgent(Agent)",
        "segment_2": "(In preview) A class for generic conversable agents which can be configured as assistant or user proxy.",
        "segment_3": "After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.",
        "segment_4": "For example, AssistantAgent and UserProxyAgent are subclasses of this class,",
        "segment_5": "configured with different default settings.",
        "segment_6": "To modify auto reply, override generate_reply method.",
        "segment_7": "To disable/enable human response in every turn, set human_input_mode to \"NEVER\" or \"ALWAYS\".",
        "segment_8": "To modify the way to get human input, override get_human_input method.",
        "segment_9": "To modify the way to execute code blocks, single code block, or function call, override execute_code_blocks,",
        "segment_10": "run_code, and execute_function methods respectively.",
        "segment_11": "To customize the initial message when a conversation starts, override generate_init_message method.",
        "segment_12": "DEFAULT_CONFIG\u200b",
        "segment_13": "An empty configuration",
        "segment_14": "MAX_CONSECUTIVE_AUTO_REPLY\u200b",
        "segment_15": "maximum number of consecutive auto replies (subject to future change)",
        "segment_16": "__init__\u200b",
        "segment_17": "def __init__(name: str, system_message: Optional[Union[ str, List]] = \"You are a helpful AI Assistant.\", is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"TERMINATE\", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Union[Dict, Literal[False]] = False, llm_config: Optional[Union[Dict, Literal[False]]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = \"\", description: Optional[str] = None)",
        "segment_18": "Arguments:",
        "segment_20": "name str - name of the agent.",
        "segment_21": "system_message str or list - system message for the ChatCompletion inference.",
        "segment_22": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_23": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_24": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_25": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_26": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_27": "When set to 0, no auto reply will be generated.",
        "segment_28": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_29": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_30": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_31": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_32": "or when is_termination_msg is True and there is no human input.",
        "segment_33": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_34": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_35": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_36": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_37": "function_map dict[str, callable] - Mapping function names (passed to openai) to callable functions, also used for tool calls.",
        "segment_38": "code_execution_config dict or False - config for the code execution.",
        "segment_39": "To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:",
        "segment_41": "work_dir (Optional, str): The working directory for the code execution.",
        "segment_42": "If None, a default working directory will be used.",
        "segment_43": "The default working directory is the \"extensions\" directory under",
        "segment_44": "\"path_to_autogen\".",
        "segment_45": "use_docker (Optional, list, str or bool): The docker image to use for code execution.",
        "segment_46": "Default is True, which means the code will be executed in a docker container. A default list of images will be used.",
        "segment_47": "If a list or a str of image name(s) is provided, the code will be executed in a docker container",
        "segment_48": "with the first image successfully pulled.",
        "segment_49": "If False, the code will be executed in the current environment.",
        "segment_50": "We strongly recommend using docker for code execution.",
        "segment_51": "timeout (Optional, int): The maximum execution time in seconds.",
        "segment_52": "last_n_messages (Experimental, int or str): The number of messages to look back for code execution.",
        "segment_53": "If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)",
        "segment_56": "llm_config dict or False - llm inference configuration.",
        "segment_57": "Please refer to OpenAIWrapper.create",
        "segment_58": "for available options.",
        "segment_59": "To disable llm-based auto reply, set to False.",
        "segment_60": "default_auto_reply str or dict or None - default auto reply when no code execution or llm-based reply is generated.",
        "segment_61": "description str - a short description of the agent. This description is used by other agents",
        "segment_62": "(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)",
        "segment_64": "register_reply\u200b",
        "segment_65": "def register_reply(trigger: Union[Type[Agent], str, Agent, Callable[[Agent], bool], List], reply_func: Callable, position: int = 0, config: Optional[Any] = None, reset_config: Optional[Callable] = None, *, ignore_async_in_sync_chat: bool = False)",
        "segment_66": "Register a reply function.",
        "segment_67": "The reply function will be called when the trigger matches the sender.",
        "segment_68": "The function registered later will be checked earlier by default.",
        "segment_69": "To change the order, set the position to a positive integer.",
        "segment_70": "Both sync and async reply functions can be registered. The sync reply function will be triggered",
        "segment_71": "from both sync and async chats. However, an async reply function will only be triggered from async",
        "segment_72": "chats (initiated with ConversableAgent.a_initiate_chat). If an async reply function is registered",
        "segment_73": "and a chat is initialized with a sync function, ignore_async_in_sync_chat determines the behaviour as follows:",
        "segment_75": "if ignore_async_in_sync_chat is set to False (default value), an exception will be raised, and",
        "segment_76": "if ignore_async_in_sync_chat is set to True, the reply function will be ignored.",
        "segment_78": "Arguments:",
        "segment_80": "trigger Agent class, str, Agent instance, callable, or list - the trigger.",
        "segment_82": "If a class is provided, the reply function will be called when the sender is an instance of the class.",
        "segment_83": "If a string is provided, the reply function will be called when the sender's name matches the string.",
        "segment_84": "If an agent instance is provided, the reply function will be called when the sender is the agent instance.",
        "segment_85": "If a callable is provided, the reply function will be called when the callable returns True.",
        "segment_86": "If a list is provided, the reply function will be called when any of the triggers in the list is activated.",
        "segment_87": "If None is provided, the reply function will be called only when the sender is None.",
        "segment_90": "Note - Be sure to register None as a trigger if you would like to trigger an auto-reply function with non-empty messages and async0.",
        "segment_91": "async1 Callable - the reply function.",
        "segment_92": "The function takes a recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.",
        "segment_93": "async2 - the position of the reply function in the reply function list.",
        "segment_94": "async3 - the config to be passed to the reply function, see below.",
        "segment_95": "async4 - the function to reset the config, see below.",
        "segment_96": "ignore_async_in_sync_chat - whether to ignore the async reply function in sync chats. If False, an exception",
        "segment_97": "will be raised if an async reply function is registered and a chat is initialized with a sync",
        "segment_98": "function.",
        "segment_99": "async7",
        "segment_100": "async2 int - the position of the reply function in the reply function list.",
        "segment_101": "The function registered later will be checked earlier by default.",
        "segment_102": "To change the order, set the position to a positive integer.",
        "segment_103": "async3 Any - the config to be passed to the reply function.",
        "segment_104": "When an agent is reset, the config will be reset to the original value.",
        "segment_105": "async4 Callable - the function to reset the config.",
        "segment_106": "The function returns None. Signature: ignore_async_in_sync_chat1",
        "segment_108": "system_message\u200b",
        "segment_109": "@propertydef system_message() -> Union[str, List]",
        "segment_110": "Return the system message.",
        "segment_111": "update_system_message\u200b",
        "segment_112": "def update_system_message(system_message: Union[str, List])",
        "segment_113": "Update the system message.",
        "segment_114": "Arguments:",
        "segment_116": "system_message str or List - system message for the ChatCompletion inference.",
        "segment_118": "update_max_consecutive_auto_reply\u200b",
        "segment_119": "def update_max_consecutive_auto_reply(value: int, sender: Optional[Agent] = None)",
        "segment_120": "Update the maximum number of consecutive auto replies.",
        "segment_121": "Arguments:",
        "segment_123": "value int - the maximum number of consecutive auto replies.",
        "segment_124": "sender Agent - when the sender is provided, only update the max_consecutive_auto_reply for that sender.",
        "segment_126": "max_consecutive_auto_reply\u200b",
        "segment_127": "def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int",
        "segment_128": "The maximum number of consecutive auto replies.",
        "segment_129": "chat_messages\u200b",
        "segment_130": "@propertydef chat_messages() -> Dict[Agent, List[Dict]]",
        "segment_131": "A dictionary of conversations from agent to list of messages.",
        "segment_132": "last_message\u200b",
        "segment_133": "def last_message(agent: Optional[Agent] = None) -> Optional[Dict]",
        "segment_134": "The last message exchanged with the agent.",
        "segment_135": "Arguments:",
        "segment_137": "agent Agent - The agent in the conversation.",
        "segment_138": "If None and more than one agent's conversations are found, an error will be raised.",
        "segment_139": "If None and only one conversation is found, the last message of the only conversation will be returned.",
        "segment_141": "Returns:",
        "segment_142": "The last message exchanged with the agent.",
        "segment_143": "use_docker\u200b",
        "segment_144": "@propertydef use_docker() -> Union[bool, str, None]",
        "segment_145": "Bool value of whether to use docker to execute the code,",
        "segment_146": "or str value of the docker image name to use, or None when code execution is disabled.",
        "segment_147": "send\u200b",
        "segment_148": "def send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_149": "Send a message to another agent.",
        "segment_150": "Arguments:",
        "segment_152": "message dict or str - message to be sent.",
        "segment_153": "The message could contain the following fields:",
        "segment_155": "content (str or List): Required, the content of the message. (Can be None)",
        "segment_156": "function_call (str): the name of the function to be called.",
        "segment_157": "name (str): the name of the function to be called.",
        "segment_158": "role (str): the role of the message, any role that is not \"function\"",
        "segment_159": "will be modified to \"assistant\".",
        "segment_160": "context (dict): the context of the message, which will be passed to",
        "segment_161": "OpenAIWrapper.create.",
        "segment_162": "For example, one agent can send a message A as:",
        "segment_166": "{ \"content\": lambda context: context[\"use_tool_msg\"], \"context\": { \"use_tool_msg\": \"Use tool X if they are relevant.\" }}",
        "segment_167": "Next time, one agent can send a message B with a different \"use_tool_msg\".",
        "segment_168": "Then the content of message A will be refreshed to the new \"use_tool_msg\".",
        "segment_169": "So effectively, this provides a way for an agent to send a \"link\" and modify",
        "segment_170": "the content of the \"link\" later.",
        "segment_172": "recipient Agent - the recipient of the message.",
        "segment_173": "request_reply bool or None - whether to request a reply from the recipient.",
        "segment_174": "silent bool or None - (Experimental) whether to print the message sent.",
        "segment_176": "Raises:",
        "segment_178": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_180": "a_send\u200b",
        "segment_181": "async def a_send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_182": "(async) Send a message to another agent.",
        "segment_183": "Arguments:",
        "segment_185": "message dict or str - message to be sent.",
        "segment_186": "The message could contain the following fields:",
        "segment_188": "content (str or List): Required, the content of the message. (Can be None)",
        "segment_189": "function_call (str): the name of the function to be called.",
        "segment_190": "name (str): the name of the function to be called.",
        "segment_191": "role (str): the role of the message, any role that is not \"function\"",
        "segment_192": "will be modified to \"assistant\".",
        "segment_193": "context (dict): the context of the message, which will be passed to",
        "segment_194": "OpenAIWrapper.create.",
        "segment_195": "For example, one agent can send a message A as:",
        "segment_199": "{ \"content\": lambda context: context[\"use_tool_msg\"], \"context\": { \"use_tool_msg\": \"Use tool X if they are relevant.\" }}",
        "segment_200": "Next time, one agent can send a message B with a different \"use_tool_msg\".",
        "segment_201": "Then the content of message A will be refreshed to the new \"use_tool_msg\".",
        "segment_202": "So effectively, this provides a way for an agent to send a \"link\" and modify",
        "segment_203": "the content of the \"link\" later.",
        "segment_205": "recipient Agent - the recipient of the message.",
        "segment_206": "request_reply bool or None - whether to request a reply from the recipient.",
        "segment_207": "silent bool or None - (Experimental) whether to print the message sent.",
        "segment_209": "Raises:",
        "segment_211": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_213": "receive\u200b",
        "segment_214": "def receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_215": "Receive a message from another agent.",
        "segment_216": "Once a message is received, this function sends a reply to the sender or stop.",
        "segment_217": "The reply can be generated automatically or entered manually by a human.",
        "segment_218": "Arguments:",
        "segment_220": "message dict or str - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).",
        "segment_222": "\"content\": content of the message, can be None.",
        "segment_223": "\"function_call\": a dictionary containing the function name and arguments. (deprecated in favor of \"tool_calls\")",
        "segment_224": "\"tool_calls\": a list of dictionaries containing the function name and arguments.",
        "segment_225": "\"role\": role of the message, can be \"assistant\", \"user\", \"function\", \"tool\".",
        "segment_226": "This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".",
        "segment_227": "\"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.",
        "segment_228": "\"context\" (dict): the context of the message, which will be passed to",
        "segment_229": "OpenAIWrapper.create.",
        "segment_232": "sender - sender of an Agent instance.",
        "segment_233": "request_reply bool or None - whether a reply is requested from the sender.",
        "segment_234": "If None, the value is determined by self.reply_at_receive[sender].",
        "segment_235": "silent bool or None - (Experimental) whether to print the message received.",
        "segment_237": "Raises:",
        "segment_239": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_241": "a_receive\u200b",
        "segment_242": "async def a_receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_243": "(async) Receive a message from another agent.",
        "segment_244": "Once a message is received, this function sends a reply to the sender or stop.",
        "segment_245": "The reply can be generated automatically or entered manually by a human.",
        "segment_246": "Arguments:",
        "segment_248": "message dict or str - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).",
        "segment_250": "\"content\": content of the message, can be None.",
        "segment_251": "\"function_call\": a dictionary containing the function name and arguments. (deprecated in favor of \"tool_calls\")",
        "segment_252": "\"tool_calls\": a list of dictionaries containing the function name and arguments.",
        "segment_253": "\"role\": role of the message, can be \"assistant\", \"user\", \"function\".",
        "segment_254": "This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".",
        "segment_255": "\"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.",
        "segment_256": "\"context\" (dict): the context of the message, which will be passed to",
        "segment_257": "OpenAIWrapper.create.",
        "segment_260": "sender - sender of an Agent instance.",
        "segment_261": "request_reply bool or None - whether a reply is requested from the sender.",
        "segment_262": "If None, the value is determined by self.reply_at_receive[sender].",
        "segment_263": "silent bool or None - (Experimental) whether to print the message received.",
        "segment_265": "Raises:",
        "segment_267": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_269": "initiate_chat\u200b",
        "segment_270": "def initiate_chat(recipient: \"ConversableAgent\", clear_history: Optional[bool] = True, silent: Optional[bool] = False, cache: Optional[Cache] = None, **context)",
        "segment_271": "Initiate a chat with the recipient agent.",
        "segment_272": "Reset the consecutive auto reply counter.",
        "segment_273": "If clear_history is True, the chat history with the recipient agent will be cleared.",
        "segment_274": "generate_init_message is called to generate the initial message for the agent.",
        "segment_275": "Arguments:",
        "segment_277": "recipient - the recipient agent.",
        "segment_278": "clear_history bool - whether to clear the chat history with the agent.",
        "segment_279": "silent bool or None - (Experimental) whether to print the messages for this conversation.",
        "segment_280": "cache Cache or None - the cache client to be used for this conversation.",
        "segment_281": "**context - any context information.",
        "segment_282": "\"message\" needs to be provided if the generate_init_message method is not overridden.",
        "segment_283": "Otherwise, input() will be called to get the initial message.",
        "segment_285": "Raises:",
        "segment_287": "RuntimeError - if any async reply functions are registered and not ignored in sync chat.",
        "segment_289": "a_initiate_chat\u200b",
        "segment_290": "async def a_initiate_chat(recipient: \"ConversableAgent\", clear_history: Optional[bool] = True, silent: Optional[bool] = False, cache: Optional[Cache] = None, **context)",
        "segment_291": "(async) Initiate a chat with the recipient agent.",
        "segment_292": "Reset the consecutive auto reply counter.",
        "segment_293": "If clear_history is True, the chat history with the recipient agent will be cleared.",
        "segment_294": "a_generate_init_message is called to generate the initial message for the agent.",
        "segment_295": "Arguments:",
        "segment_297": "recipient - the recipient agent.",
        "segment_298": "clear_history bool - whether to clear the chat history with the agent.",
        "segment_299": "silent bool or None - (Experimental) whether to print the messages for this conversation.",
        "segment_300": "cache Cache or None - the cache client to be used for this conversation.",
        "segment_301": "**context - any context information.",
        "segment_302": "\"message\" needs to be provided if the a_generate_init_message method is not overridden.",
        "segment_303": "Otherwise, input() will be called to get the initial message.",
        "segment_305": "reset\u200b",
        "segment_306": "def reset()",
        "segment_307": "Reset the agent.",
        "segment_308": "stop_reply_at_receive\u200b",
        "segment_309": "def stop_reply_at_receive(sender: Optional[Agent] = None)",
        "segment_310": "Reset the reply_at_receive of the sender.",
        "segment_311": "reset_consecutive_auto_reply_counter\u200b",
        "segment_312": "def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None)",
        "segment_313": "Reset the consecutive_auto_reply_counter of the sender.",
        "segment_314": "clear_history\u200b",
        "segment_315": "def clear_history(recipient: Optional[Agent] = None, nr_messages_to_preserve: Optional[int] = None)",
        "segment_316": "Clear the chat history of the agent.",
        "segment_317": "Arguments:",
        "segment_319": "recipient - the agent with whom the chat history to clear. If None, clear the chat history with all agents.",
        "segment_320": "nr_messages_to_preserve - the number of newest messages to preserve in the chat history.",
        "segment_322": "generate_oai_reply\u200b",
        "segment_323": "def generate_oai_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_324": "Generate a reply using autogen.oai.",
        "segment_325": "a_generate_oai_reply\u200b",
        "segment_326": "async def a_generate_oai_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_327": "Generate a reply using autogen.oai asynchronously.",
        "segment_328": "generate_code_execution_reply\u200b",
        "segment_329": "def generate_code_execution_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Union[Dict, Literal[False]]] = None)",
        "segment_330": "Generate a reply using code execution.",
        "segment_331": "generate_function_call_reply\u200b",
        "segment_332": "def generate_function_call_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_333": "Generate a reply using function call.",
        "segment_334": "\"function_call\" replaced by \"tool_calls\" as of OpenAI API v1.1.0",
        "segment_335": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions",
        "segment_336": "a_generate_function_call_reply\u200b",
        "segment_337": "async def a_generate_function_call_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_338": "Generate a reply using async function call.",
        "segment_339": "\"function_call\" replaced by \"tool_calls\" as of OpenAI API v1.1.0",
        "segment_340": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions",
        "segment_341": "generate_tool_calls_reply\u200b",
        "segment_342": "def generate_tool_calls_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_343": "Generate a reply using tool call.",
        "segment_344": "a_generate_tool_calls_reply\u200b",
        "segment_345": "async def a_generate_tool_calls_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_346": "Generate a reply using async function call.",
        "segment_347": "check_termination_and_human_reply\u200b",
        "segment_348": "def check_termination_and_human_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, None]]",
        "segment_349": "Check if the conversation should be terminated, and if human reply is provided.",
        "segment_350": "This method checks for conditions that require the conversation to be terminated, such as reaching",
        "segment_351": "a maximum number of consecutive auto-replies or encountering a termination message. Additionally,",
        "segment_352": "it prompts for and processes human input based on the configured human input mode, which can be",
        "segment_353": "'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter",
        "segment_354": "for the conversation and prints relevant messages based on the human input received.",
        "segment_355": "Arguments:",
        "segment_357": "messages (Optional[List[Dict]]): A list of message dictionaries, representing the conversation history.",
        "segment_358": "sender (Optional[Agent]): The agent object representing the sender of the message.",
        "segment_359": "config (Optional[Any]): Configuration object, defaults to the current instance if not provided.",
        "segment_361": "Returns:",
        "segment_363": "Tuple[bool, Union[str, Dict, None]]: A tuple containing a boolean indicating if the conversation",
        "segment_364": "should be terminated, and a human reply which can be a string, a dictionary, or None.",
        "segment_366": "a_check_termination_and_human_reply\u200b",
        "segment_367": "async def a_check_termination_and_human_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, None]]",
        "segment_368": "(async) Check if the conversation should be terminated, and if human reply is provided.",
        "segment_369": "This method checks for conditions that require the conversation to be terminated, such as reaching",
        "segment_370": "a maximum number of consecutive auto-replies or encountering a termination message. Additionally,",
        "segment_371": "it prompts for and processes human input based on the configured human input mode, which can be",
        "segment_372": "'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter",
        "segment_373": "for the conversation and prints relevant messages based on the human input received.",
        "segment_374": "Arguments:",
        "segment_376": "messages (Optional[List[Dict]]): A list of message dictionaries, representing the conversation history.",
        "segment_377": "sender (Optional[Agent]): The agent object representing the sender of the message.",
        "segment_378": "config (Optional[Any]): Configuration object, defaults to the current instance if not provided.",
        "segment_380": "Returns:",
        "segment_382": "Tuple[bool, Union[str, Dict, None]]: A tuple containing a boolean indicating if the conversation",
        "segment_383": "should be terminated, and a human reply which can be a string, a dictionary, or None.",
        "segment_385": "generate_reply\u200b",
        "segment_386": "def generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_387": "Reply based on the conversation history and the sender.",
        "segment_388": "Either messages or sender must be provided.",
        "segment_389": "Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.",
        "segment_390": "Use registered auto reply functions to generate replies.",
        "segment_391": "By default, the following functions are checked in order:",
        "segment_393": "check_termination_and_human_reply",
        "segment_394": "generate_function_call_reply (deprecated in favor of tool_calls)",
        "segment_395": "generate_tool_calls_reply",
        "segment_396": "generate_code_execution_reply",
        "segment_397": "generate_oai_reply",
        "segment_398": "Every function returns a tuple (final, reply).",
        "segment_399": "When a function returns final=False, the next function will be checked.",
        "segment_400": "So by default, termination and human reply will be checked first.",
        "segment_401": "If not terminating and human reply is skipped, execute function or code and return the result.",
        "segment_402": "AI replies are generated only when no code execution is performed.",
        "segment_404": "Arguments:",
        "segment_406": "messages - a list of messages in the conversation history.",
        "segment_407": "default_reply str or dict - default reply.",
        "segment_408": "sender - sender of an Agent instance.",
        "segment_409": "exclude - a list of functions to exclude.",
        "segment_411": "Returns:",
        "segment_412": "str or dict or None: reply. None if no reply is generated.",
        "segment_413": "a_generate_reply\u200b",
        "segment_414": "async def a_generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_415": "(async) Reply based on the conversation history and the sender.",
        "segment_416": "Either messages or sender must be provided.",
        "segment_417": "Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.",
        "segment_418": "Use registered auto reply functions to generate replies.",
        "segment_419": "By default, the following functions are checked in order:",
        "segment_421": "check_termination_and_human_reply",
        "segment_422": "generate_function_call_reply",
        "segment_423": "generate_tool_calls_reply",
        "segment_424": "generate_code_execution_reply",
        "segment_425": "generate_oai_reply",
        "segment_426": "Every function returns a tuple (final, reply).",
        "segment_427": "When a function returns final=False, the next function will be checked.",
        "segment_428": "So by default, termination and human reply will be checked first.",
        "segment_429": "If not terminating and human reply is skipped, execute function or code and return the result.",
        "segment_430": "AI replies are generated only when no code execution is performed.",
        "segment_432": "Arguments:",
        "segment_434": "messages - a list of messages in the conversation history.",
        "segment_435": "default_reply str or dict - default reply.",
        "segment_436": "sender - sender of an Agent instance.",
        "segment_437": "exclude - a list of functions to exclude.",
        "segment_439": "Returns:",
        "segment_440": "str or dict or None: reply. None if no reply is generated.",
        "segment_441": "get_human_input\u200b",
        "segment_442": "def get_human_input(prompt: str) -> str",
        "segment_443": "Get human input.",
        "segment_444": "Override this method to customize the way to get human input.",
        "segment_445": "Arguments:",
        "segment_447": "prompt str - prompt for the human input.",
        "segment_449": "Returns:",
        "segment_451": "str - human input.",
        "segment_453": "a_get_human_input\u200b",
        "segment_454": "async def a_get_human_input(prompt: str) -> str",
        "segment_455": "(Async) Get human input.",
        "segment_456": "Override this method to customize the way to get human input.",
        "segment_457": "Arguments:",
        "segment_459": "prompt str - prompt for the human input.",
        "segment_461": "Returns:",
        "segment_463": "str - human input.",
        "segment_465": "run_code\u200b",
        "segment_466": "def run_code(code, **kwargs)",
        "segment_467": "Run the code and return the result.",
        "segment_468": "Override this function to modify the way to run the code.",
        "segment_469": "Arguments:",
        "segment_471": "code str - the code to be executed.",
        "segment_472": "**kwargs - other keyword arguments.",
        "segment_474": "Returns:",
        "segment_475": "A tuple of (exitcode, logs, image).",
        "segment_477": "exitcode int - the exit code of the code execution.",
        "segment_478": "logs str - the logs of the code execution.",
        "segment_479": "image str or None - the docker image used for the code execution.",
        "segment_481": "execute_code_blocks\u200b",
        "segment_482": "def execute_code_blocks(code_blocks)",
        "segment_483": "Execute the code blocks and return the result.",
        "segment_484": "execute_function\u200b",
        "segment_485": "def execute_function(func_call, verbose: bool = False) -> Tuple[bool, Dict[str, str]]",
        "segment_486": "Execute a function call and return the result.",
        "segment_487": "Override this function to modify the way to execute function and tool calls.",
        "segment_488": "Arguments:",
        "segment_490": "func_call - a dictionary extracted from openai message at \"function_call\" or \"tool_calls\" with keys \"name\" and \"arguments\".",
        "segment_492": "Returns:",
        "segment_493": "A tuple of (is_exec_success, result_dict).",
        "segment_496": "is_exec_success boolean - whether the execution is successful.",
        "segment_499": "result_dict - a dictionary with keys \"name\", \"role\", and \"content\". Value of \"role\" is \"function\".",
        "segment_500": "\"function_call\" deprecated as of OpenAI API v1.1.0",
        "segment_501": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_504": "a_execute_function\u200b",
        "segment_505": "async def a_execute_function(func_call)",
        "segment_506": "Execute an async function call and return the result.",
        "segment_507": "Override this function to modify the way async functions and tools are executed.",
        "segment_508": "Arguments:",
        "segment_510": "func_call - a dictionary extracted from openai message at key \"function_call\" or \"tool_calls\" with keys \"name\" and \"arguments\".",
        "segment_512": "Returns:",
        "segment_513": "A tuple of (is_exec_success, result_dict).",
        "segment_516": "is_exec_success boolean - whether the execution is successful.",
        "segment_519": "result_dict - a dictionary with keys \"name\", \"role\", and \"content\". Value of \"role\" is \"function\".",
        "segment_520": "\"function_call\" deprecated as of OpenAI API v1.1.0",
        "segment_521": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_524": "generate_init_message\u200b",
        "segment_525": "def generate_init_message(**context) -> Union[str, Dict]",
        "segment_526": "Generate the initial message for the agent.",
        "segment_527": "Override this function to customize the initial message based on user's request.",
        "segment_528": "If not overridden, \"message\" needs to be provided in the context.",
        "segment_529": "Arguments:",
        "segment_531": "**context - any context information, and \"message\" parameter needs to be provided.",
        "segment_532": "If message is not given, prompt for it via input()",
        "segment_534": "a_generate_init_message\u200b",
        "segment_535": "async def a_generate_init_message(**context) -> Union[str, Dict]",
        "segment_536": "Generate the initial message for the agent.",
        "segment_537": "Override this function to customize the initial message based on user's request.",
        "segment_538": "If not overridden, \"message\" needs to be provided in the context.",
        "segment_539": "Arguments:",
        "segment_541": "**context - any context information, and \"message\" parameter needs to be provided.",
        "segment_542": "If message is not given, prompt for it via input()",
        "segment_544": "register_function\u200b",
        "segment_545": "def register_function(function_map: Dict[str, Callable])",
        "segment_546": "Register functions to the agent.",
        "segment_547": "Arguments:",
        "segment_549": "function_map - a dictionary mapping function names to functions.",
        "segment_551": "update_function_signature\u200b",
        "segment_552": "def update_function_signature(func_sig: Union[str, Dict], is_remove: None)",
        "segment_553": "update a function_signature in the LLM configuration for function_call.",
        "segment_554": "Arguments:",
        "segment_557": "func_sig str or dict - description/name of the function to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions",
        "segment_560": "is_remove - whether removing the function from llm_config with name 'func_sig'",
        "segment_561": "Deprecated as of OpenAI API v1.1.0",
        "segment_562": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_565": "update_tool_signature\u200b",
        "segment_566": "def update_tool_signature(tool_sig: Union[str, Dict], is_remove: None)",
        "segment_567": "update a tool_signature in the LLM configuration for tool_call.",
        "segment_568": "Arguments:",
        "segment_570": "tool_sig str or dict - description/name of the tool to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools",
        "segment_571": "is_remove - whether removing the tool from llm_config with name 'tool_sig'",
        "segment_573": "can_execute_function\u200b",
        "segment_574": "def can_execute_function(name: Union[List[str], str]) -> bool",
        "segment_575": "Whether the agent can execute the function.",
        "segment_576": "function_map\u200b",
        "segment_577": "@propertydef function_map() -> Dict[str, Callable]",
        "segment_578": "Return the function map.",
        "segment_579": "register_for_llm\u200b",
        "segment_580": "def register_for_llm( *, name: Optional[str] = None, description: Optional[str] = None, api_style: Literal[\"function\", \"tool\"] = \"tool\") -> Callable[[F], F]",
        "segment_581": "Decorator factory for registering a function to be used by an agent.",
        "segment_582": "It's return value is used to decorate a function to be registered to the agent. The function uses type hints to",
        "segment_583": "specify the arguments and return type. The function name is used as the default name for the function,",
        "segment_584": "but a custom name can be provided. The function description is used to describe the function in the",
        "segment_585": "agent's configuration.",
        "segment_586": "Arguments:",
        "segment_587": "name (optional(str)): name of the function. If None, the function name will be used (default: None).",
        "segment_588": "description (optional(str)): description of the function (default: None). It is mandatory",
        "segment_589": "for the initial decorator, but the following ones can omit it.",
        "segment_591": "api_style - (literal): the API style for function call.",
        "segment_592": "For Azure OpenAI API, use version 2023-12-01-preview or later.",
        "segment_593": "\"function\" style will be deprecated. For earlier version use",
        "segment_594": "\"function\" if \"tool\" doesn't work.",
        "segment_595": "See Azure OpenAI documentation for details.",
        "segment_597": "Returns:",
        "segment_598": "The decorator for registering a function to be used by an agent.",
        "segment_599": "Examples:",
        "segment_600": "```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description=\"This is a very useful function\")def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14) -> str: return a + str(b * c)```",
        "segment_601": "For Azure OpenAI versions prior to 2023-12-01-preview, set api_style",
        "segment_602": "to \"function\" if \"tool\" doesn't work:",
        "segment_603": "@agent2.register_for_llm(api_style=\"function\") def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14) -> str: return a + str(b * c)",
        "segment_604": "register_for_execution\u200b",
        "segment_605": "def register_for_execution(name: Optional[str] = None) -> Callable[[F], F]",
        "segment_606": "Decorator factory for registering a function to be executed by an agent.",
        "segment_607": "It's return value is used to decorate a function to be registered to the agent.",
        "segment_608": "Arguments:",
        "segment_609": "name (optional(str)): name of the function. If None, the function name will be used (default: None).",
        "segment_610": "Returns:",
        "segment_611": "The decorator for registering a function to be used by an agent.",
        "segment_612": "Examples:",
        "segment_613": "```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description=\"This is a very useful function\")def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14): return a + str(b * c)```",
        "segment_614": "register_model_client\u200b",
        "segment_615": "def register_model_client(model_client_cls: ModelClient, **kwargs)",
        "segment_616": "Register a model client.",
        "segment_617": "Arguments:",
        "segment_619": "model_client_cls - A custom client class that follows the Client interface",
        "segment_620": "**kwargs - The kwargs for the custom client class to be initialized with",
        "segment_622": "register_hook\u200b",
        "segment_623": "def register_hook(hookable_method: Callable, hook: Callable)",
        "segment_624": "Registers a hook to be called by a hookable method, in order to add a capability to the agent.",
        "segment_625": "Registered hooks are kept in lists (one per hookable method), and are called in their order of registration.",
        "segment_626": "Arguments:",
        "segment_628": "hookable_method - A hookable method implemented by ConversableAgent.",
        "segment_629": "hook - A method implemented by a subclass of AgentCapability.",
        "segment_631": "process_last_message\u200b",
        "segment_632": "def process_last_message(messages)",
        "segment_633": "Calls any registered capability hooks to use and potentially modify the text of the last message,",
        "segment_634": "as long as the last message is not a function call or exit command.",
        "segment_635": "print_usage_summary\u200b",
        "segment_636": "def print_usage_summary( mode: Union[str, List[str]] = [\"actual\", \"total\"]) -> None",
        "segment_637": "Print the usage summary.",
        "segment_638": "get_actual_usage\u200b",
        "segment_639": "def get_actual_usage() -> Union[None, Dict[str, int]]",
        "segment_640": "Get the actual usage summary.",
        "segment_641": "get_total_usage\u200b",
        "segment_642": "def get_total_usage() -> Union[None, Dict[str, int]]",
        "segment_643": "Get the total usage summary.",
        "segment_644": "register_function\u200b",
        "segment_645": "def register_function(f: Callable[..., Any], *, caller: ConversableAgent, executor: ConversableAgent, name: Optional[str] = None, description: str) -> None",
        "segment_646": "Register a function to be proposed by an agent and executed for an executor.",
        "segment_647": "This function can be used instead of function decorators @ConversationAgent.register_for_llm and",
        "segment_648": "@ConversationAgent.register_for_execution.",
        "segment_649": "Arguments:",
        "segment_651": "f - the function to be registered.",
        "segment_652": "caller - the agent calling the function, typically an instance of ConversableAgent.",
        "segment_653": "executor - the agent executing the function, typically an instance of UserProxy.",
        "segment_654": "name - name of the function. If None, the function name will be used (default: None).",
        "segment_655": "description - description of the function. The description is used by LLM to decode whether the function",
        "segment_656": "is called. Make sure the description is properly describing what the function does or it might not be",
        "segment_657": "called by LLM when needed."
    },
    {
        "segment_1": "This project welcomes and encourages all forms of contributions, including but not limited to:",
        "segment_3": "Pushing patches.",
        "segment_4": "Code review of pull requests.",
        "segment_5": "Documentation, examples and test cases.",
        "segment_6": "Readability improvement, e.g., improvement on docstr and comments.",
        "segment_7": "Community participation in issues, discussions, discord, and twitter.",
        "segment_8": "Tutorials, blog posts, talks that promote the project.",
        "segment_9": "Sharing application scenarios and/or related research.",
        "segment_11": "Most contributions require you to agree to a",
        "segment_12": "Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us",
        "segment_13": "the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.",
        "segment_14": "If you are new to GitHub here is a detailed help source on getting involved with development on GitHub.",
        "segment_15": "When you submit a pull request, a CLA bot will automatically determine whether you need to provide",
        "segment_16": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions",
        "segment_17": "provided by the bot. You will only need to do this once across all repos using our CLA.",
        "segment_18": "This project has adopted the Microsoft Open Source Code of Conduct.",
        "segment_19": "For more information see the Code of Conduct FAQ or",
        "segment_20": "contact opencode@microsoft.com with any additional questions or comments.",
        "segment_21": "How to make a good bug report\u200b",
        "segment_22": "When you submit an issue to GitHub, please do your best to",
        "segment_23": "follow these guidelines! This will make it a lot easier to provide you with good",
        "segment_24": "feedback:",
        "segment_27": "The ideal bug report contains a short reproducible code snippet. This way",
        "segment_28": "anyone can try to reproduce the bug easily (see this for more details). If your snippet is",
        "segment_29": "longer than around 50 lines, please link to a gist or a GitHub repo.",
        "segment_32": "If an exception is raised, please provide the full traceback.",
        "segment_35": "Please include your operating system type and version number, as well as",
        "segment_36": "your Python, autogen, scikit-learn versions. The version of autogen",
        "segment_37": "can be found by running the following code snippet:",
        "segment_40": "import autogenprint(autogen.__version__)",
        "segment_42": "Please ensure all code snippets and error messages are formatted in",
        "segment_43": "appropriate code blocks. See Creating and highlighting code blocks",
        "segment_44": "for more details.",
        "segment_46": "Becoming a Reviewer\u200b",
        "segment_47": "There is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord.",
        "segment_48": "Guidance for Maintainers\u200b",
        "segment_49": "General\u200b",
        "segment_51": "Be a member of the community and treat everyone as a member. Be inclusive.",
        "segment_52": "Help each other and encourage mutual help.",
        "segment_53": "Actively post and respond.",
        "segment_54": "Keep open communication.",
        "segment_56": "Pull Requests\u200b",
        "segment_59": "For new PR, decide whether to close without review. If not, find the right reviewers. The default reviewer is microsoft/autogen. Ask users who can benefit from the PR to review it.",
        "segment_62": "For old PR, check the blocker: reviewer or PR creator. Try to unblock. Get additional help when needed.",
        "segment_65": "When requesting changes, make sure you can check back in time because it blocks merging.",
        "segment_68": "Make sure all the checks are passed.",
        "segment_71": "For changes that require running OpenAI tests, make sure the OpenAI tests pass too. Running these tests requires approval.",
        "segment_74": "In general, suggest small PRs instead of a giant PR.",
        "segment_77": "For documentation change, request snapshot of the compiled website, or compile by yourself to verify the format.",
        "segment_80": "For new contributors who have not signed the contributing agreement, remind them to sign before reviewing.",
        "segment_83": "For multiple PRs which may have conflict, coordinate them to figure out the right order.",
        "segment_86": "Pay special attention to:",
        "segment_88": "Breaking changes. Don\u2019t make breaking changes unless necessary. Don\u2019t merge to main until enough headsup is provided and a new release is ready.",
        "segment_89": "Test coverage decrease.",
        "segment_90": "Changes that may cause performance degradation. Do regression test when test suites are available.",
        "segment_91": "Discourage change to the core library when there is an alternative.",
        "segment_95": "Issues and Discussions\u200b",
        "segment_98": "For new issues, write a reply, apply a label if relevant. Ask on discord when necessary. For roadmap issues, add to the roadmap project and encourage community discussion. Mention relevant experts when necessary.",
        "segment_101": "For old issues, provide an update or close. Ask on discord when necessary. Encourage PR creation when relevant.",
        "segment_104": "Use \u201cgood first issue\u201d for easy fix suitable for first-time contributors.",
        "segment_107": "Use \u201ctask list\u201d for issues that require multiple PRs.",
        "segment_110": "For discussions, create an issue when relevant. Discuss on discord when appropriate.",
        "segment_113": "Docker for Development\u200b",
        "segment_114": "For developers contributing to the AutoGen project, we offer a specialized Docker environment. This setup is designed to streamline the development process, ensuring that all contributors work within a consistent and well-equipped environment.",
        "segment_115": "Autogen Developer Image (autogen_dev_img)\u200b",
        "segment_117": "Purpose: The autogen_dev_img is tailored for contributors to the AutoGen project. It includes a suite of tools and configurations that aid in the development and testing of new features or fixes.",
        "segment_118": "Usage: This image is recommended for developers who intend to contribute code or documentation to AutoGen.",
        "segment_119": "Forking the Project: It's advisable to fork the AutoGen GitHub project to your own repository. This allows you to make changes in a separate environment without affecting the main project.",
        "segment_120": "Updating Dockerfile: Modify your copy of Dockerfile in the dev folder as needed for your development work.",
        "segment_121": "Submitting Pull Requests: Once your changes are ready, submit a pull request from your branch to the upstream AutoGen GitHub project for review and integration. For more details on contributing, see the AutoGen Contributing page.",
        "segment_123": "Building the Developer Docker Image\u200b",
        "segment_126": "To build the developer Docker image (autogen_dev_img), use the following commands:",
        "segment_127": "docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git",
        "segment_130": "For building the developer image built from a specific Dockerfile in a branch other than main/master",
        "segment_131": "# clone the branch you want to work out ofgit clone --branch {branch-name} https://github.com/microsoft/autogen.git# cd to your new directorycd autogen# build your Docker imagedocker build -f .devcontainer/dev/Dockerfile -t autogen_dev-srv_img .",
        "segment_134": "Using the Developer Docker Image\u200b",
        "segment_135": "Once you have built the autogen_dev_img, you can run it using the standard Docker commands. This will place you inside the containerized development environment where you can run tests, develop code, and ensure everything is functioning as expected before submitting your contributions.",
        "segment_136": "docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_138": "Note that the pwd is shorthand for present working directory. Thus, any path after the pwd is relative to that. If you want a more verbose method you could remove the \"pwd/autogen-newcode\" and replace it with the full path to your directory",
        "segment_140": "docker run -it -p 8081:3000 -v /home/AutoGenDeveloper/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_141": "Develop in Remote Container\u200b",
        "segment_142": "If you use vscode, you can open the autogen folder in a Container.",
        "segment_143": "We have provided the configuration in devcontainer. They can be used in GitHub codespace too. Developing AutoGen in dev containers is recommended.",
        "segment_144": "Pre-commit\u200b",
        "segment_145": "Run pre-commit install to install pre-commit into your git hooks. Before you commit, run",
        "segment_146": "pre-commit run to check if you meet the pre-commit requirements. If you use Windows (without WSL) and can't commit after installing pre-commit, you can run pre-commit uninstall to uninstall the hook. In WSL or Linux this is supposed to work.",
        "segment_147": "Write tests\u200b",
        "segment_148": "Tests are automatically run via GitHub actions. There are two workflows:",
        "segment_150": "build.yml",
        "segment_151": "openai.yml",
        "segment_153": "The first workflow is required to pass for all PRs (and it doesn't do any OpenAI calls). The second workflow is required for changes that affect the OpenAI tests (and does actually call LLM). The second workflow requires approval to run. When writing tests that require OpenAI calls, please use pytest.mark.skipif to make them run in only when openai package is installed. If additional dependency for this test is required, install the dependency in the corresponding python version in openai.yml.",
        "segment_154": "Make sure all tests pass, this is required for build.yml checks to pass",
        "segment_155": "Running tests locally\u200b",
        "segment_156": "To run tests, install the [test] option:",
        "segment_157": "pip install -e.\"[test]\"",
        "segment_158": "Then you can run the tests from the test folder using the following command:",
        "segment_159": "pytest test",
        "segment_160": "Tests for the autogen.agentchat.contrib module may be skipped automatically if the",
        "segment_161": "required dependencies are not installed. Please consult the documentation for",
        "segment_162": "each contrib module to see what dependencies are required.",
        "segment_163": "Skip flags for tests\u200b",
        "segment_165": "--skip-openai for skipping tests that require access to OpenAI services.",
        "segment_166": "--skip-docker for skipping tests that explicitly use docker",
        "segment_167": "--skip-redis for skipping tests that require a Redis server",
        "segment_169": "For example, the following command will skip tests that require access to",
        "segment_170": "OpenAI and docker services:",
        "segment_171": "pytest test --skip-openai --skip-docker",
        "segment_172": "Coverage\u200b",
        "segment_173": "Any code you commit should not decrease coverage. To run all unit tests, install the [test] option:",
        "segment_174": "pip install -e.\"[test]\"coverage run -m pytest test",
        "segment_175": "Then you can see the coverage report by",
        "segment_176": "coverage report -m or coverage html.",
        "segment_177": "Documentation\u200b",
        "segment_178": "To build and test documentation locally, install Node.js. For example,",
        "segment_179": "nvm install --lts",
        "segment_180": "Then:",
        "segment_181": "npm install --global yarn # skip if you use the dev container we providedpip install pydoc-markdown # skip if you use the dev container we providedcd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownquarto render ./docsyarn start",
        "segment_182": "The last command starts a local development server and opens up a browser window.",
        "segment_183": "Most changes are reflected live without having to restart the server.",
        "segment_184": "To build and test documentation within a docker container. Use the Dockerfile in the dev folder as described above to build your image",
        "segment_185": "docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git",
        "segment_186": "Then start the container like so, this will log you in and ensure that Docker port 3000 is mapped to port 8081 on your local machine",
        "segment_187": "docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_188": "Once at the CLI in Docker run the following commands:",
        "segment_189": "cd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownquarto render ./docsyarn start --host 0.0.0.0 --port 3000",
        "segment_190": "Once done you should be able to access the documentation at http://127.0.0.1:8081/autogen",
        "segment_191": "Note:",
        "segment_192": "some tips in this guide are based off the contributor guide from flaml.Edit this pagePreviousEnhanced InferenceNextResearchHow to make a good bug reportBecoming a ReviewerGuidance for MaintainersGeneralPull RequestsIssues and DiscussionsDocker for DevelopmentAutogen Developer Image (autogen_dev_img)Building the Developer Docker ImageUsing the Developer Docker ImageDevelop in Remote ContainerPre-commitWrite testsCoverageDocumentationCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class AssistantAgent(ConversableAgent)",
        "segment_2": "(In preview) Assistant agent, designed to solve a task with LLM.",
        "segment_3": "AssistantAgent is a subclass of ConversableAgent configured with a default system message.",
        "segment_4": "The default system message is designed to solve a task with LLM,",
        "segment_5": "including suggesting python code blocks and debugging.",
        "segment_6": "human_input_mode is default to \"NEVER\"",
        "segment_7": "and code_execution_config is default to False.",
        "segment_8": "This agent doesn't execute code by default, and expects the user to execute the code.",
        "segment_9": "__init__\u200b",
        "segment_10": "def __init__(name: str, system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE, llm_config: Optional[Union[Dict, Literal[False]]] = None, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"NEVER\", description: Optional[str] = None, **kwargs)",
        "segment_11": "Arguments:",
        "segment_13": "name str - agent name.",
        "segment_14": "system_message str - system message for the ChatCompletion inference.",
        "segment_15": "Please override this attribute if you want to reprogram the agent.",
        "segment_16": "llm_config dict - llm inference configuration.",
        "segment_17": "Please refer to OpenAIWrapper.create",
        "segment_18": "for available options.",
        "segment_19": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_20": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_21": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_22": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_23": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_24": "The limit only plays a role when human_input_mode is not \"ALWAYS\".",
        "segment_25": "**kwargs dict - Please refer to other kwargs in",
        "segment_26": "ConversableAgent."
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_59": "TL;DR:",
        "segment_61": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_62": "allows retrieval-augmented generation, and its basic usage.",
        "segment_63": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_64": "split function and vector database.",
        "segment_65": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_66": "application with Gradio.",
        "segment_68": "Introduction\u200b",
        "segment_69": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_70": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_71": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_72": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_73": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_74": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_75": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_76": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_77": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_78": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_79": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_80": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_82": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_83": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_84": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_85": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_86": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_87": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_88": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_89": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_90": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_91": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_92": "Retrieval-Augmented Assistant.",
        "segment_93": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_94": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_95": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_96": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_97": "The conversation terminates if no more documents are available for the context.",
        "segment_99": "Basic Usage of RAG Agents\u200b",
        "segment_101": "Install dependencies",
        "segment_103": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_104": "pip install \"pyautogen[retrievechat]\"",
        "segment_105": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_106": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_107": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_108": "If you install unstructured",
        "segment_109": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_110": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_111": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_113": "Import Agents",
        "segment_115": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_117": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_119": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_121": "Initialize Chat and ask a question",
        "segment_123": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_124": "Output is like:",
        "segment_125": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_127": "Create a UserProxyAgent and ask the same question",
        "segment_129": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_130": "Output is like:",
        "segment_131": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_132": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_133": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_134": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_135": "Customizing RAG Agents\u200b",
        "segment_136": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_137": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_138": "function and vector database.",
        "segment_139": "Customizing Embedding Function\u200b",
        "segment_140": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_141": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_143": "OpenAI",
        "segment_145": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_147": "HuggingFace",
        "segment_149": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_150": "More examples can be found here.",
        "segment_151": "Customizing Text Split Function\u200b",
        "segment_152": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_153": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_154": "There are also some existing text split tools which are good to reuse.",
        "segment_155": "For example, you can use all the text splitters in langchain.",
        "segment_156": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_157": "Customizing Vector Database\u200b",
        "segment_158": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_159": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_160": "For example, you can use Qdrant as below:",
        "segment_161": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_162": "Advanced Usage of RAG Agents\u200b",
        "segment_163": "Integrate with other agents in a group chat\u200b",
        "segment_164": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_165": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_166": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_167": "you'll need to call it from a function.",
        "segment_168": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_169": "Build a Chat application with Gradio\u200b",
        "segment_170": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_172": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_173": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_174": "Read More\u200b",
        "segment_175": "You can check out more example notebooks for RAG use cases:",
        "segment_177": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_178": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_179": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_180": "Tags:LLMRAGCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Completion(openai_Completion)",
        "segment_2": "(openai<1) A class for OpenAI completion API.",
        "segment_3": "It also supports: ChatCompletion, Azure OpenAI API.",
        "segment_4": "set_cache\u200b",
        "segment_5": "@classmethoddef set_cache(cls, seed: Optional[int] = 41, cache_path_root: Optional[str] = \".cache\")",
        "segment_6": "Set cache path.",
        "segment_7": "Arguments:",
        "segment_9": "seed int, Optional - The integer identifier for the pseudo seed.",
        "segment_10": "Results corresponding to different seeds will be cached in different places.",
        "segment_11": "cache_path str, Optional - The root path for the cache.",
        "segment_12": "The complete cache path will be {cache_path_root}/{seed}.",
        "segment_14": "clear_cache\u200b",
        "segment_15": "@classmethoddef clear_cache(cls, seed: Optional[int] = None, cache_path_root: Optional[str] = \".cache\")",
        "segment_16": "Clear cache.",
        "segment_17": "Arguments:",
        "segment_19": "seed int, Optional - The integer identifier for the pseudo seed.",
        "segment_20": "If omitted, all caches under cache_path_root will be cleared.",
        "segment_21": "cache_path str, Optional - The root path for the cache.",
        "segment_22": "The complete cache path will be {cache_path_root}/{seed}.",
        "segment_24": "tune\u200b",
        "segment_25": "@classmethoddef tune(cls, data: List[Dict], metric: str, mode: str, eval_func: Callable, log_file_name: Optional[str] = None, inference_budget: Optional[float] = None, optimization_budget: Optional[float] = None, num_samples: Optional[int] = 1, logging_level: Optional[int] = logging.WARNING, **config)",
        "segment_26": "Tune the parameters for the OpenAI API call.",
        "segment_27": "TODO: support parallel tuning with ray or spark.",
        "segment_28": "TODO: support agg_method as in test",
        "segment_29": "Arguments:",
        "segment_31": "data list - The list of data points.",
        "segment_32": "metric str - The metric to optimize.",
        "segment_33": "mode str - The optimization mode, \"min\" or \"max.",
        "segment_34": "eval_func Callable - The evaluation function for responses.",
        "segment_35": "The function should take a list of responses and a data point as input,",
        "segment_36": "and return a dict of metrics. For example,",
        "segment_38": "def eval_func(responses, **data): solution = data[\"solution\"] success_list = [] n = len(responses) for i in range(n): response = responses[i] succeed = is_equiv_chain_of_thought(response, solution) success_list.append(succeed) return { \"expected_success\": 1 - pow(1 - sum(success_list) / n, n), \"success\": any(s for s in success_list), }",
        "segment_40": "log_file_name str, optional - The log file.",
        "segment_41": "inference_budget float, optional - The inference budget, dollar per instance.",
        "segment_42": "optimization_budget float, optional - The optimization budget, dollar in total.",
        "segment_43": "num_samples int, optional - The number of samples to evaluate.",
        "segment_44": "-1 means no hard restriction in the number of trials",
        "segment_45": "and the actual number is decided by optimization_budget. Defaults to 1.",
        "segment_46": "logging_level optional - logging level. Defaults to logging.WARNING.",
        "segment_47": "metric0 dict - The search space to update over the default search.",
        "segment_48": "For prompt, please provide a string/Callable or a list of strings/Callables.",
        "segment_50": "If prompt is provided for chat models, it will be converted to messages under role \"user\".",
        "segment_51": "Do not provide both prompt and messages for chat models, but provide either of them.",
        "segment_52": "A string template will be used to generate a prompt for each data instance",
        "segment_53": "using metric1.",
        "segment_54": "A callable template will be used to generate a prompt for each data instance",
        "segment_55": "using metric2.",
        "segment_56": "For stop, please provide a string, a list of strings, or a list of lists of strings.",
        "segment_57": "For messages (chat models only), please provide a list of messages (for a single chat prefix)",
        "segment_58": "or a list of lists of messages (for multiple choices of chat prefix to choose from).",
        "segment_59": "Each message should be a dict with keys \"role\" and \"content\". The value of \"content\" can be a string/Callable template.",
        "segment_63": "Returns:",
        "segment_65": "metric3 - The optimized hyperparameter setting.",
        "segment_66": "metric4 - The tuning results.",
        "segment_68": "create\u200b",
        "segment_69": "@classmethoddef create(cls, context: Optional[Dict] = None, use_cache: Optional[bool] = True, config_list: Optional[List[Dict]] = None, filter_func: Optional[Callable[[Dict, Dict], bool]] = None, raise_on_ratelimit_or_timeout: Optional[bool] = True, allow_format_str_template: Optional[bool] = False, **config)",
        "segment_70": "Make a completion for a given context.",
        "segment_71": "Arguments:",
        "segment_73": "context Dict, Optional - The context to instantiate the prompt.",
        "segment_74": "It needs to contain keys that are used by the prompt template or the filter function.",
        "segment_75": "E.g., prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}.",
        "segment_76": "The actual prompt will be:",
        "segment_77": "\"Complete the following sentence: Today I feel\".",
        "segment_78": "More examples can be found at templating.",
        "segment_79": "use_cache bool, Optional - Whether to use cached responses.",
        "segment_80": "config_list List, Optional - List of configurations for the completion to try.",
        "segment_81": "The first one that does not raise an error will be used.",
        "segment_82": "Only the differences from the default config need to be provided.",
        "segment_83": "E.g.,",
        "segment_85": "response = oai.Completion.create( config_list=[ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-03-15-preview\", }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"api_type\": \"openai\", \"base_url\": \"https://api.openai.com/v1\", }, { \"model\": \"llama-7B\", \"base_url\": \"http://127.0.0.1:8080\", \"api_type\": \"openai\", } ], prompt=\"Hi\",)",
        "segment_87": "filter_func Callable, Optional - A function that takes in the context and the response and returns a boolean to indicate whether the response is valid. E.g.,",
        "segment_89": "def yes_or_no_filter(context, config, response): return context.get(\"yes_or_no_choice\", False) is False or any( text in [\"Yes.\", \"No.\"] for text in oai.Completion.extract_text(response) )",
        "segment_91": "raise_on_ratelimit_or_timeout bool, Optional - Whether to raise RateLimitError or Timeout when all configs fail.",
        "segment_92": "When set to False, -1 will be returned when all configs fail.",
        "segment_93": "allow_format_str_template bool, Optional - Whether to allow format string template in the config.",
        "segment_94": "**config - Configuration for the openai API call. This is used as parameters for calling openai API.",
        "segment_95": "The \"prompt\" or \"messages\" parameter can contain a template (str or Callable) which will be instantiated with the context.",
        "segment_96": "Besides the parameters for the openai API call, it can also contain:",
        "segment_98": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}0 (int): the total time (in seconds) allowed for retrying failed requests.",
        "segment_99": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}1 (int): the time interval to wait (in seconds) before retrying a failed request.",
        "segment_100": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}2 (int) for the cache. This is useful when implementing \"controlled randomness\" for the completion.",
        "segment_104": "Returns:",
        "segment_105": "Responses from OpenAI API, with additional fields.",
        "segment_107": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}3: the total cost.",
        "segment_108": "When config_list is provided, the response will contain a few more fields:",
        "segment_109": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}5: the index of the config in the config_list that is used to generate the response.",
        "segment_110": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}6: whether the response passes the filter function. None if no filter is provided.",
        "segment_112": "test\u200b",
        "segment_113": "@classmethoddef test(cls, data, eval_func=None, use_cache=True, agg_method=\"avg\", return_responses_and_per_instance_result=False, logging_level=logging.WARNING, **config)",
        "segment_114": "Evaluate the responses created with the config for the OpenAI API call.",
        "segment_115": "Arguments:",
        "segment_117": "data list - The list of test data points.",
        "segment_118": "eval_func Callable - The evaluation function for responses per data instance.",
        "segment_119": "The function should take a list of responses and a data point as input,",
        "segment_120": "and return a dict of metrics. You need to either provide a valid callable",
        "segment_121": "eval_func; or do not provide one (set None) but call the test function after",
        "segment_122": "calling the tune function in which a eval_func is provided.",
        "segment_123": "In the latter case we will use the eval_func provided via tune function.",
        "segment_124": "Defaults to None.",
        "segment_126": "def eval_func(responses, **data): solution = data[\"solution\"] success_list = [] n = len(responses) for i in range(n): response = responses[i] succeed = is_equiv_chain_of_thought(response, solution) success_list.append(succeed) return { \"expected_success\": 1 - pow(1 - sum(success_list) / n, n), \"success\": any(s for s in success_list), }",
        "segment_128": "use_cache bool, Optional - Whether to use cached responses. Defaults to True.",
        "segment_129": "agg_method str, Callable or a dict of Callable - Result aggregation method (across",
        "segment_130": "multiple instances) for each of the metrics. Defaults to 'avg'.",
        "segment_131": "An example agg_method in str:",
        "segment_133": "agg_method = 'median'",
        "segment_134": "An example agg_method in a Callable:",
        "segment_135": "agg_method = np.median",
        "segment_136": "An example agg_method in a dict of Callable:",
        "segment_137": "agg_method={'median_success': np.median, 'avg_success': np.mean}",
        "segment_139": "return_responses_and_per_instance_result bool - Whether to also return responses",
        "segment_140": "and per instance results in addition to the aggregated results.",
        "segment_141": "logging_level optional - logging level. Defaults to logging.WARNING.",
        "segment_142": "eval_func0 dict - parameters passed to the openai api call eval_func1.",
        "segment_144": "Returns:",
        "segment_145": "None when no valid eval_func is provided in either test or tune;",
        "segment_146": "Otherwise, a dict of aggregated results, responses and per instance results if return_responses_and_per_instance_result is True;",
        "segment_147": "Otherwise, a dict of aggregated results (responses and per instance results are not returned).",
        "segment_148": "cost\u200b",
        "segment_149": "@classmethoddef cost(cls, response: dict)",
        "segment_150": "Compute the cost of an API call.",
        "segment_151": "Arguments:",
        "segment_153": "response dict - The response from OpenAI API.",
        "segment_155": "Returns:",
        "segment_156": "The cost in USD. 0 if the model is not supported.",
        "segment_157": "extract_text\u200b",
        "segment_158": "@classmethoddef extract_text(cls, response: dict) -> List[str]",
        "segment_159": "Extract the text from a completion or chat response.",
        "segment_160": "Arguments:",
        "segment_162": "response dict - The response from OpenAI API.",
        "segment_164": "Returns:",
        "segment_165": "A list of text in the responses.",
        "segment_166": "extract_text_or_function_call\u200b",
        "segment_167": "@classmethoddef extract_text_or_function_call(cls, response: dict) -> List[str]",
        "segment_168": "Extract the text or function calls from a completion or chat response.",
        "segment_169": "Arguments:",
        "segment_171": "response dict - The response from OpenAI API.",
        "segment_173": "Returns:",
        "segment_174": "A list of text or function calls in the responses.",
        "segment_175": "logged_history\u200b",
        "segment_176": "@classmethod@propertydef logged_history(cls) -> Dict",
        "segment_177": "Return the book keeping dictionary.",
        "segment_178": "print_usage_summary\u200b",
        "segment_179": "@classmethoddef print_usage_summary(cls) -> Dict",
        "segment_180": "Return the usage summary.",
        "segment_181": "start_logging\u200b",
        "segment_182": "@classmethoddef start_logging(cls, history_dict: Optional[Dict] = None, compact: Optional[bool] = True, reset_counter: Optional[bool] = True)",
        "segment_183": "Start book keeping.",
        "segment_184": "Arguments:",
        "segment_186": "history_dict Dict - A dictionary for book keeping.",
        "segment_187": "If no provided, a new one will be created.",
        "segment_188": "compact bool - Whether to keep the history dictionary compact.",
        "segment_189": "Compact history contains one key per conversation, and the value is a dictionary",
        "segment_190": "like:",
        "segment_192": "{ \"create_at\": [0, 1], \"cost\": [0.1, 0.2],}",
        "segment_193": "where \"created_at\" is the index of API calls indicating the order of all the calls,",
        "segment_194": "and \"cost\" is the cost of each call. This example shows that the conversation is based",
        "segment_195": "on two API calls. The compact format is useful for condensing the history of a conversation.",
        "segment_196": "If compact is False, the history dictionary will contain all the API calls: the key",
        "segment_197": "is the index of the API call, and the value is a dictionary like:",
        "segment_198": "{ \"request\": request_dict, \"response\": response_dict,}",
        "segment_199": "where request_dict is the request sent to OpenAI API, and response_dict is the response.",
        "segment_200": "For a conversation containing two API calls, the non-compact history dictionary will be like:",
        "segment_201": "{ 0: { \"request\": request_dict_0, \"response\": response_dict_0, }, 1: { \"request\": request_dict_1, \"response\": response_dict_1, },",
        "segment_202": "The first request's messages plus the response is equal to the second request's messages.",
        "segment_203": "For a conversation with many turns, the non-compact history dictionary has a quadratic size",
        "segment_204": "while the compact history dict has a linear size.",
        "segment_206": "reset_counter bool - whether to reset the counter of the number of API calls.",
        "segment_208": "stop_logging\u200b",
        "segment_209": "@classmethoddef stop_logging(cls)",
        "segment_210": "End book keeping.",
        "segment_211": "ChatCompletion Objects\u200b",
        "segment_212": "class ChatCompletion(Completion)",
        "segment_213": "(openai<1) A class for OpenAI API ChatCompletion. Share the same API as Completion.Edit this pagePreviousclientNextopenai_utilsCompletion ObjectsChatCompletion ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageGetting StartedFLAML is a lightweight Python library for efficient automation of machine",
        "segment_2": "learning and AI operations. It automates workflow based on large language models, machine learning models, etc.",
        "segment_3": "and optimizes their performance.Main Features\u200bFLAML enables building next-gen GPT-X applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation and optimization of a complex GPT-X workflow. It maximizes the performance of GPT-X models and augments their weakness.For common machine learning tasks like classification and regression, it quickly finds quality models for user-provided data with low computational resources. It is easy to customize or extend.It supports fast and economical automatic tuning, capable of handling large search space with heterogeneous evaluation cost and complex constraints/guidance/early stopping.FLAML is powered by a series of research studies from Microsoft Research and collaborators such as Penn State University, Stevens Institute of Technology, University of Washington, and University of Waterloo.Quickstart\u200bInstall FLAML from pip: pip install flaml. Find more options in Installation.There are several ways of using flaml:(New) AutoGen\u200bAutogen enables the next-gen GPT-X applications with a generic multi-agent conversation framework.",
        "segment_4": "It offers customizable and conversable agents which integrate LLMs, tools and human.",
        "segment_5": "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example,from flaml import autogenassistant = autogen.AssistantAgent(\"assistant\")user_proxy = autogen.UserProxyAgent(\"user_proxy\")user_proxy.initiate_chat(assistant, message=\"Show me the YTD gain of 10 largest technology companies as of today.\")# This initiates an automated chat between the two agents to solve the taskCopyAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers a drop-in replacement of openai.Completion or openai.ChatCompletion with powerful functionalites like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.# perform tuningconfig, analysis = autogen.Completion.tune( data=tune_data, metric=\"success\", mode=\"max\", eval_func=eval_func, inference_budget=0.05, optimization_budget=3, num_samples=-1,)# perform inference for a test instanceresponse = autogen.Completion.create(context=test_instance, **config)CopyTask-oriented AutoML\u200bWith three lines of code, you can start using this economical and fast AutoML engine as a scikit-learn style estimator.from flaml import AutoMLautoml = AutoML()automl.fit(X_train, y_train, task=\"classification\", time_budget=60)CopyIt automatically tunes the hyperparameters and selects the best model from default learners such as LightGBM, XGBoost, random forest etc. for the specified time budget 60 seconds. Customizing the optimization metrics, learners and search spaces etc. is very easy. For example,automl.add_learner(\"mylgbm\", MyLGBMEstimator)automl.fit(X_train, y_train, task=\"classification\", metric=custom_metric, estimator_list=[\"mylgbm\"], time_budget=60)CopyTune user-defined function\u200bYou can run generic hyperparameter tuning for a custom function (machine learning or beyond). For example,from flaml import tunefrom flaml.automl.model import LGBMEstimatordef train_lgbm(config: dict) -> dict: # convert config dict to lgbm params params = LGBMEstimator(**config).params # train the model train_set = lightgbm.Dataset(csv_file_name) model = lightgbm.train(params, train_set) # evaluate the model pred = model.predict(X_test) mse = mean_squared_error(y_test, pred) # return eval results as a dictionary return {\"mse\": mse}# load a built-in search space from flamlflaml_lgbm_search_space = LGBMEstimator.search_space(X_train.shape)# specify the search space as a dict from hp name to domain; you can define your own search space same wayconfig_search_space = {hp: space[\"domain\"] for hp, space in flaml_lgbm_search_space.items()}# give guidance about hp values corresponding to low training cost, i.e., {\"n_estimators\": 4, \"num_leaves\": 4}low_cost_partial_config = { hp: space[\"low_cost_init_value\"] for hp, space in flaml_lgbm_search_space.items() if \"low_cost_init_value\" in space}# run the tuning, minimizing mse, with total time budget 3 secondsanalysis = tune.run( train_lgbm, metric=\"mse\", mode=\"min\", config=config_search_space, low_cost_partial_config=low_cost_partial_config, time_budget_s=3, num_samples=-1,)CopyPlease see this script for the complete version of the above example.Zero-shot AutoML\u200bFLAML offers a unique, seamless and effortless way to leverage AutoML for the commonly used classifiers and regressors such as LightGBM and XGBoost. For example, if you are using lightgbm.LGBMClassifier as your current learner, all you need to do is to replace from lightgbm import LGBMClassifier by:from flaml.default import LGBMClassifierCopyThen, you can use it just like you use the original LGMBClassifier. Your other code can remain unchanged. When you call the fit() function from flaml.default.LGBMClassifier, it will automatically instantiate a good data-dependent hyperparameter configuration for your dataset, which is expected to work better than the default configuration.Where to Go Next?\u200bUnderstand the use cases for AutoGen, Task-oriented AutoML, Tune user-defined function and Zero-shot AutoML.Find code examples under \"Examples\": from AutoGen - AgentChat to Tune - PyTorch.Learn about research around FLAML and check blogposts.Chat on Discord.If you like our project, please give it a star on GitHub. If you are interested in contributing, please read Contributor's Guide.Edit this pageNextInstallation \u00bbMain FeaturesQuickstartWhere to Go Next?CommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_4": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_5": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_7": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_8": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_9": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_10": "The MathChat Framework\u200b",
        "segment_11": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_12": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_15": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_18": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_20": "Writing a Python program to solve the problem directly.",
        "segment_21": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_22": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_26": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_29": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_30": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_31": "(The conversation is modified for readability.)",
        "segment_32": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_33": "Experiment Setup\u200b",
        "segment_34": "We evaluate the improvement brought by MathChat.",
        "segment_35": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_36": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_37": "We also evaluate the following methods for comparison:",
        "segment_40": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_43": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_46": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_49": "Experiment Results\u200b",
        "segment_50": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_52": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_53": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_54": "The code for experiments can be found at this repository.",
        "segment_55": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_56": "Future Directions\u200b",
        "segment_57": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_58": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_60": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_61": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_62": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_64": "For Further Reading\u200b",
        "segment_66": "Research paper of MathChat",
        "segment_67": "Documentation about autogen",
        "segment_69": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostUse AutoGen for Local LLMsOlder PostAchieve More, Pay Less - Use GPT-4 SmartlyThe MathChat FrameworkExperiment SetupExperiment ResultsFuture DirectionsFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Automated Multi Agent Chat\u200b",
        "segment_2": "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation via multi-agent conversation.",
        "segment_3": "Please find documentation about this feature here.",
        "segment_4": "Links to notebook examples:",
        "segment_7": "Code Generation, Execution, and Debugging",
        "segment_9": "Automated Task Solving with Code Generation, Execution & Debugging - View Notebook",
        "segment_10": "Automated Code Generation and Question Answering with Retrieval Augmented Agents - View Notebook",
        "segment_11": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents - View Notebook",
        "segment_15": "Multi-Agent Collaboration (>3 Agents)",
        "segment_17": "Automated Task Solving by Group Chat (with 3 group member agents and 1 manager agent) - View Notebook",
        "segment_18": "Automated Data Visualization by Group Chat (with 3 group member agents and 1 manager agent) - View Notebook",
        "segment_19": "Automated Complex Task Solving by Group Chat (with 6 group member agents and 1 manager agent) - View Notebook",
        "segment_20": "Automated Task Solving with Coding & Planning Agents - View Notebook",
        "segment_21": "Automated Task Solving with transition paths specified in a graph - View Notebook",
        "segment_22": "Running a group chat as an inner-monolgue via the SocietyOfMindAgent - View Notebook",
        "segment_26": "Applications",
        "segment_28": "Automated Chess Game Playing & Chitchatting by GPT-4 Agents - View Notebook",
        "segment_29": "Automated Continual Learning from New Data - View Notebook",
        "segment_30": "OptiGuide - Coding, Tool Using, Safeguarding & Question Answering for Supply Chain Optimization",
        "segment_34": "Tool Use",
        "segment_36": "Web Search: Solve Tasks Requiring Web Info - View Notebook",
        "segment_37": "Use Provided Tools as Functions - View Notebook",
        "segment_38": "Use Tools via Sync and Async Function Calling - View Notebook",
        "segment_39": "Task Solving with Langchain Provided Tools as Functions - View Notebook",
        "segment_40": "RAG: Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) - View Notebook",
        "segment_41": "Function Inception: Enable AutoGen agents to update/remove functions during conversations. - View Notebook",
        "segment_42": "Agent Chat with Whisper - View Notebook",
        "segment_43": "Constrained Responses via Guidance - View Notebook",
        "segment_44": "Browse the Web with Agents - View Notebook",
        "segment_45": "SQL: Natural Language Text to SQL Query using the Spider Text-to-SQL Benchmark - View Notebook",
        "segment_49": "Human Involvement",
        "segment_51": "Simple example in ChatGPT style View example",
        "segment_52": "Auto Code Generation, Execution, Debugging and Human Feedback - View Notebook",
        "segment_53": "Automated Task Solving with GPT-4 + Multiple Human Users - View Notebook",
        "segment_54": "Agent Chat with Async Human Inputs - View Notebook",
        "segment_58": "Agent Teaching and Learning",
        "segment_60": "Teach Agents New Skills & Reuse via Automated Chat - View Notebook",
        "segment_61": "Teach Agents New Facts, User Preferences and Skills Beyond Coding - View Notebook",
        "segment_62": "Teach OpenAI Assistants Through GPTAssistantAgent - View Notebook",
        "segment_63": "Agent Optimizer: Train Agents in an Agentic Way - View Notebook",
        "segment_67": "Multi-Agent Chat with OpenAI Assistants in the loop",
        "segment_69": "Hello-World Chat with OpenAi Assistant in AutoGen - View Notebook",
        "segment_70": "Chat with OpenAI Assistant using Function Call - View Notebook",
        "segment_71": "Chat with OpenAI Assistant with Code Interpreter - View Notebook",
        "segment_72": "Chat with OpenAI Assistant with Retrieval Augmentation - View Notebook",
        "segment_73": "OpenAI Assistant in a Group Chat - View Notebook",
        "segment_77": "Multimodal Agent",
        "segment_79": "Multimodal Agent Chat with DALLE and GPT-4V - View Notebook",
        "segment_80": "Multimodal Agent Chat with Llava - View Notebook",
        "segment_81": "Multimodal Agent Chat with GPT-4V - View Notebook",
        "segment_85": "Long Context Handling",
        "segment_87": "Conversations with Chat History Compression Enabled - View Notebook",
        "segment_91": "Evaluation and Assessment",
        "segment_93": "AgentEval: A Multi-Agent System for Assess Utility of LLM-powered Applications - View Notebook",
        "segment_97": "Automatic Agent Building",
        "segment_99": "Automatically Build Multi-agent System with AgentBuilder - View Notebook",
        "segment_100": "Automatically Build Multi-agent System from Agent Library - View Notebook",
        "segment_104": "Enhanced Inferences\u200b",
        "segment_105": "Utilities\u200b",
        "segment_107": "API Unification - View Documentation with Code Example",
        "segment_108": "Utility Functions to Help Managing API configurations effectively - View Notebook",
        "segment_109": "Cost Calculation - View Notebook",
        "segment_111": "Inference Hyperparameters Tuning\u200b",
        "segment_112": "AutoGen offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. The research study finds that tuning hyperparameters can significantly improve the utility of them.",
        "segment_113": "Please find documentation about this feature here.",
        "segment_114": "Links to notebook examples:",
        "segment_116": "Optimize for Code Generation | Open in colab",
        "segment_117": "Optimize for Math | Open in colab",
        "segment_118": "Edit this pageAutomated Multi Agent ChatEnhanced InferencesUtilitiesInference Hyperparameters TuningCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR\u200b",
        "segment_2": "For just getting started with AutoGen you can use the following to",
        "segment_3": "define your LLM endpoint configuration:",
        "segment_4": "import autogenconfig_list = [{\"model\": \"gpt-4\", \"api_key\": \"YOUR_OPENAI_API_KEY\"}]",
        "segment_5": "dangerNever commit secrets into your code. Before committing, change the code",
        "segment_6": "to use a different way of providing your API keys as described below.",
        "segment_7": "In-depth\u200b",
        "segment_8": "Managing API configurations can be tricky, especially when dealing with",
        "segment_9": "multiple models and API versions. The provided utility functions assist",
        "segment_10": "users in managing these configurations effectively. Ensure your API keys",
        "segment_11": "and other sensitive data are stored securely. You might store keys in",
        "segment_12": ".txt or .env files or environment variables for local development.",
        "segment_13": "Never expose your API keys publicly. If you insist on storing your key",
        "segment_14": "files locally on your repo (you shouldn\u2019t), ensure the key file path is",
        "segment_15": "added to the .gitignore file.",
        "segment_16": "Steps:\u200b",
        "segment_18": "Obtain API keys from OpenAI and optionally from Azure OpenAI (or",
        "segment_19": "other provider).",
        "segment_20": "Store them securely using either:",
        "segment_22": "Environment Variables: export OPENAI_API_KEY='your-key' in",
        "segment_23": "your shell.",
        "segment_24": "Text File: Save the key in a key_openai.txt file.",
        "segment_25": "Env File: Save the key to a .env file eg:",
        "segment_26": "OPENAI_API_KEY=sk-********************",
        "segment_29": "Ensure pyautogen is installed",
        "segment_31": "There are many ways to generate a config_list depending on your use",
        "segment_32": "case:",
        "segment_34": "get_config_list: Generates configurations for API calls, primarily",
        "segment_35": "from provided API keys.",
        "segment_36": "config_list_openai_aoai: Constructs a list of configurations using",
        "segment_37": "both Azure OpenAI and OpenAI endpoints, sourcing API keys from",
        "segment_38": "environment variables or local files.",
        "segment_39": "config_list_from_json: Loads configurations from a JSON structure,",
        "segment_40": "either from an environment variable or a local JSON file, with the",
        "segment_41": "flexibility of filtering configurations based on given criteria.",
        "segment_42": "config_list_from_models: Creates configurations based on a",
        "segment_43": "provided list of models, useful when targeting specific models",
        "segment_44": "without manually specifying each configuration.",
        "segment_45": "config_list_from_dotenv: Constructs a configuration list from a",
        "segment_46": ".env file, offering a consolidated way to manage multiple API",
        "segment_47": "configurations and keys from a single file.",
        "segment_49": "If multiple models are provided, the Autogen client (OpenAIWrapper)",
        "segment_50": "and agents don\u2019t choose the \u201cbest model\u201d on any criteria - inference is",
        "segment_51": "done through the very first model and the next one is used only if the",
        "segment_52": "current model fails (e.g. API throttling by the provider or a filter",
        "segment_53": "condition is unsatisfied).",
        "segment_54": "What is a config_list?\u200b",
        "segment_55": "When instantiating an assistant, such as the example below, it is passed",
        "segment_56": "a config_list. This is used to tell the AssistantAgent which models",
        "segment_57": "or configurations it has access to:",
        "segment_58": "assistant = AssistantAgent( name=\"assistant\", llm_config={ \"timeout\": 600, \"cache_seed\": 42, \"config_list\": config_list, \"temperature\": 0, },)",
        "segment_59": "Consider an intelligent assistant that utilizes OpenAI\u2019s GPT models.",
        "segment_60": "Depending on user requests, it might need to:",
        "segment_62": "Generate creative content (using gpt-4).",
        "segment_63": "Answer general queries (using gpt-3.5-turbo).",
        "segment_65": "Different tasks may require different models, and the config_list aids",
        "segment_66": "in dynamically selecting the appropriate model configuration, managing",
        "segment_67": "API keys, endpoints, and versions for efficient operation of the",
        "segment_68": "intelligent assistant. In summary, the config_list helps the agents",
        "segment_69": "work efficiently, reliably, and optimally by managing various",
        "segment_70": "configurations and interactions with the OpenAI API - enhancing the",
        "segment_71": "adaptability and functionality of the agents.",
        "segment_72": "get_config_list\u200b",
        "segment_73": "Used to generate configurations for API calls.",
        "segment_74": "api_keys = [\"YOUR_OPENAI_API_KEY\"]base_urls = None # You can specify API base URLs if needed. eg: localhost:8000api_type = \"openai\" # Type of API, e.g., \"openai\" or \"aoai\".api_version = None # Specify API version if needed.config_list = autogen.get_config_list(api_keys, base_urls=base_urls, api_type=api_type, api_version=api_version)print(config_list)",
        "segment_75": "[{'api_key': 'YOUR_OPENAI_API_KEY', 'api_type': 'openai'}]",
        "segment_76": "config_list_openai_aoai\u200b",
        "segment_77": "This method creates a list of configurations using Azure OpenAI",
        "segment_78": "endpoints and OpenAI endpoints. It tries to extract API keys and bases",
        "segment_79": "from environment variables or local text files.",
        "segment_80": "Steps: - Store OpenAI API key in: - Environment variable:",
        "segment_81": "OPENAI_API_KEY - or Local file: key_openai.txt - Store Azure OpenAI",
        "segment_82": "API key in: - Environment variable: AZURE_OPENAI_API_KEY - or Local",
        "segment_83": "file: key_aoai.txt (Supports multiple keys, one per line) - Store",
        "segment_84": "Azure OpenAI API base in: - Environment variable:",
        "segment_85": "AZURE_OPENAI_API_BASE - or Local file: base_aoai.txt (Supports",
        "segment_86": "multiple bases, one per line)",
        "segment_87": "config_list = autogen.config_list_openai_aoai( key_file_path=\".\", openai_api_key_file=\"key_openai.txt\", aoai_api_key_file=\"key_aoai.txt\", aoai_api_base_file=\"base_aoai.txt\", exclude=None, # The API type to exclude, eg: \"openai\" or \"aoai\".)",
        "segment_88": "config_list_from_json\u200b",
        "segment_89": "This method loads configurations from an environment variable or a JSON",
        "segment_90": "file. It provides flexibility by allowing users to filter configurations",
        "segment_91": "based on certain criteria.",
        "segment_92": "Steps: - Setup the JSON Configuration: 1. Store configurations in an",
        "segment_93": "environment variable named OAI_CONFIG_LIST as a valid JSON string. 2.",
        "segment_94": "Alternatively, save configurations in a local JSON file named",
        "segment_95": "OAI_CONFIG_LIST.json 3. Add OAI_CONFIG_LIST to your .gitignore",
        "segment_96": "file on your local repository.",
        "segment_97": "Your JSON structure should look something like this:",
        "segment_98": "# OAI_CONFIG_LIST file example[ { \"model\": \"gpt-4\", \"api_key\": \"YOUR_OPENAI_API_KEY\" }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": \"YOUR_OPENAI_API_KEY\", \"api_version\": \"2023-03-01-preview\" }]",
        "segment_99": "config_list = autogen.config_list_from_json( env_or_file=\"OAI_CONFIG_LIST\", # or OAI_CONFIG_LIST.json if file extension is added filter_dict={ \"model\": { \"gpt-4\", \"gpt-3.5-turbo\", } },)",
        "segment_100": "What is filter_dict?\u200b",
        "segment_101": "The z parameter in autogen.config_list_from_json function is used to",
        "segment_102": "selectively filter the configurations loaded from the environment",
        "segment_103": "variable or JSON file based on specified criteria. It allows you to",
        "segment_104": "define criteria to select only those configurations that match the",
        "segment_105": "defined conditions.",
        "segment_106": "let\u2019s say you want to configure an assistant agent to only LLM type.",
        "segment_107": "Take the below example: even though we have \u201cgpt-3.5-turbo\u201d and \u201cgpt-4\u201d",
        "segment_108": "in our OAI_CONFIG_LIST, this agent would only be configured to use",
        "segment_109": "cheap_config_list = autogen.config_list_from_json( env_or_file=\"OAI_CONFIG_LIST\", filter_dict={ \"model\": { \"gpt-3.5-turbo\", } },)costly_config_list = autogen.config_list_from_json( env_or_file=\"OAI_CONFIG_LIST\", filter_dict={ \"model\": { \"gpt-4\", } },)# Assistant using GPT 3.5 Turboassistant_one = autogen.AssistantAgent( name=\"3.5-assistant\", llm_config={ \"timeout\": 600, \"cache_seed\": 42, \"config_list\": cheap_config_list, \"temperature\": 0, },)# Assistant using GPT 4assistant_two = autogen.AssistantAgent( name=\"4-assistant\", llm_config={ \"timeout\": 600, \"cache_seed\": 42, \"config_list\": costly_config_list, \"temperature\": 0, },)",
        "segment_110": "With the OAI_CONFIG_LIST we set earlier, there isn\u2019t much to filter",
        "segment_111": "on. But when the complexity of a project grows and you\u2019re managing",
        "segment_112": "multiple models for various purposes, you can see how filter_dict can",
        "segment_113": "be useful.",
        "segment_114": "A more complex filtering criteria could be the following: Assuming we",
        "segment_115": "have an OAI_CONFIG_LIST several models used to create various agents -",
        "segment_116": "Let\u2019s say we want to load configurations for gpt-4 using API version",
        "segment_117": "\"2023-03-01-preview\" and we want the api_type to be aoai, we can",
        "segment_118": "set up filter_dict as follows:",
        "segment_119": "config_list = autogen.config_list_from_json( env_or_file=\"OAI_CONFIG_LIST\", filter_dict={\"model\": {\"gpt-4\"}, \"api_version\": {\"2023-03-01-preview\"}, \"api_type\": [\"aoai\"]},)",
        "segment_120": "config_list_from_models\u200b",
        "segment_121": "This method creates configurations based on a provided list of models.",
        "segment_122": "It\u2019s useful when you have specific models in mind and don\u2019t want to",
        "segment_123": "manually specify each configuration. The",
        "segment_124": "config_list_from_models",
        "segment_125": "function tries to create a list of configurations using Azure OpenAI",
        "segment_126": "endpoints and OpenAI endpoints for the provided list of models. It",
        "segment_127": "assumes the api keys and api bases are stored in the corresponding",
        "segment_128": "environment variables or local txt files. It\u2019s okay to only have the",
        "segment_129": "OpenAI API key, OR only the Azure OpenAI API key + base. For Azure the",
        "segment_130": "model name refers to the OpenAI Studio deployment name.",
        "segment_131": "Steps: - Similar to method 1, store API keys and bases either in",
        "segment_132": "environment variables or .txt files.",
        "segment_133": "config_list = autogen.config_list_from_models( key_file_path=\".\", openai_api_key_file=\"key_openai.txt\", aoai_api_key_file=\"key_aoai.txt\", aoai_api_base_file=\"base_aoai.txt\", exclude=\"aoai\", model_list=[\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],)",
        "segment_134": "config_list_from_dotenv\u200b",
        "segment_135": "If you are interested in keeping all of your keys in a single location",
        "segment_136": "like a .env file rather than using a configuration specifically for",
        "segment_137": "OpenAI, you can use config_list_from_dotenv. This allows you to",
        "segment_138": "conveniently create a config list without creating a complex",
        "segment_139": "OAI_CONFIG_LIST file.",
        "segment_140": "The model_api_key_map parameter is a dictionary that maps model names",
        "segment_141": "to the environment variable names in the .env file where their",
        "segment_142": "respective API keys are stored. It lets the code know which API key to",
        "segment_143": "use for each model.",
        "segment_144": "If not provided, it defaults to using OPENAI_API_KEY for gpt-4 and",
        "segment_145": "OPENAI_API_KEY for gpt-3.5-turbo.",
        "segment_146": "# default key map model_api_key_map = { \"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": \"OPENAI_API_KEY\", }",
        "segment_147": "Here is an example .env file:",
        "segment_148": "OPENAI_API_KEY=sk-*********************HUGGING_FACE_API_KEY=**************************ANOTHER_API_KEY=1234567890234567890",
        "segment_149": "config_list = autogen.config_list_from_dotenv( dotenv_file_path=\".env\", # If None the function will try to find in the working directory filter_dict={ \"model\": { \"gpt-4\", \"gpt-3.5-turbo\", } },)config_list",
        "segment_150": "[{'api_key': 'sk-*********************', 'model': 'gpt-4'}, {'api_key': 'sk-*********************', 'model': 'gpt-3.5-turbo'}]",
        "segment_151": "# gpt-3.5-turbo will default to OPENAI_API_KEYconfig_list = autogen.config_list_from_dotenv( dotenv_file_path=\".env\", # If None the function will try to find in the working directory model_api_key_map={ \"gpt-4\": \"ANOTHER_API_KEY\", # String or dict accepted }, filter_dict={ \"model\": { \"gpt-4\", \"gpt-3.5-turbo\", } },)config_list",
        "segment_152": "[{'api_key': '1234567890234567890', 'model': 'gpt-4'}, {'api_key': 'sk-*********************', 'model': 'gpt-3.5-turbo'}]",
        "segment_153": "# example using different environment variable namesconfig_list = autogen.config_list_from_dotenv( dotenv_file_path=\".env\", model_api_key_map={ \"gpt-4\": \"OPENAI_API_KEY\", \"vicuna\": \"HUGGING_FACE_API_KEY\", }, filter_dict={ \"model\": { \"gpt-4\", \"vicuna\", } },)config_list",
        "segment_154": "[{'api_key': 'sk-*********************', 'model': 'gpt-4'}, {'api_key': '**************************', 'model': 'vicuna'}]",
        "segment_155": "You can also provide additional configurations for APIs, simply by",
        "segment_156": "replacing the string value with a dictionary expanding on the",
        "segment_157": "configurations. See the example below showing the example of using",
        "segment_158": "gpt-4 on openai by default, and using gpt-3.5-turbo with",
        "segment_159": "additional configurations for aoai.",
        "segment_160": "config_list = autogen.config_list_from_dotenv( dotenv_file_path=\".env\", model_api_key_map={ \"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": { \"api_key_env_var\": \"ANOTHER_API_KEY\", \"api_type\": \"aoai\", \"api_version\": \"v2\", \"base_url\": \"https://api.someotherapi.com\", }, }, filter_dict={ \"model\": { \"gpt-4\", \"gpt-3.5-turbo\", } },)config_list",
        "segment_161": "[{'api_key': 'sk-*********************', 'model': 'gpt-4'}, {'api_key': '1234567890234567890', 'base_url': 'https://api.someotherapi.com', 'api_type': 'aoai', 'api_version': 'v2', 'model': 'gpt-3.5-turbo'}]Edit this pagePreviousOptional DependenciesNextMulti-agent Conversation FrameworkTL;DRIn-depthSteps:What is a config_list?get_config_listconfig_list_openai_aoaiconfig_list_from_jsonconfig_list_from_modelsconfig_list_from_dotenvCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_2": "Introducing AgentOptimizer, a new class for training LLM agents in the era of LLMs as a service.",
        "segment_3": "AgentOptimizer is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.",
        "segment_4": "Checkout one implementation for AgentOptimizer on MATH dataset",
        "segment_5": "here.",
        "segment_6": "Paper is coming soon!",
        "segment_7": "Introduction\u200b",
        "segment_8": "In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?",
        "segment_9": "Here, we take an initial step towards the agent training.",
        "segment_10": "Inspired by the function calling capabilities provided by OpenAI,",
        "segment_11": "we draw an analogy between model weights and agent functions/skills, and update an agent\u2019s functions/skills based on its historical performance on a training set.",
        "segment_12": "Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents\u2019 functions as a set of function calls,",
        "segment_13": "to support iteratively adding, revising, and removing existing functions.",
        "segment_14": "As an agentic way of training an agent, our approach helps enhance the agents\u2019 abilities without requiring access to the LLMs weights.",
        "segment_15": "AgentOptimizer\u200b",
        "segment_16": "AgentOptimizer is a class designed to optimize the agents by improving their function calls.",
        "segment_17": "It contains two core methods:",
        "segment_19": "step(): step() takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).",
        "segment_21": "actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)",
        "segment_22": "It has two outputs actions and updated_functions. actions is a series of actions to manipulate the current functions. And updated_functions is the updated functions after the actions are applied (including code implementation).",
        "segment_24": "update_function_call():",
        "segment_25": "This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from step().",
        "segment_26": "For AssistantAgent, it first uses update_function_signature to update the function signatures.",
        "segment_27": "Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from step().",
        "segment_29": "Sometimes, the function signatures (JSON schema) returned by the step() may not be valid, and the generated code may also face syntax errors.",
        "segment_30": "AgentOptimizer includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.",
        "segment_31": "Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.",
        "segment_32": "Pseudocode for the optimization process\u200b",
        "segment_33": "The optimization process is as follows:",
        "segment_34": "for - in range(EPOCH): history, statistic, current_functions = solve_problems(train_problems) actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions) AgentOptimizer.update_function_call(actions)",
        "segment_35": "Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.",
        "segment_36": "The functions are then improved using AgentOptimizer.",
        "segment_37": "Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.",
        "segment_38": "After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks",
        "segment_39": "The implementation technology behind the AgentOptimizer\u200b",
        "segment_40": "To obtain stable and structured function signatures and code implementations from AgentOptimizer,",
        "segment_41": "we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.",
        "segment_42": "Specifically, we introduce three function calls to manipulate the current functions at each step: add_function, remove_function, and revise_function.",
        "segment_43": "These calls add, remove, and revise functions in the existing function list, respectively.",
        "segment_44": "This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.",
        "segment_45": "Below is the JSON schema of these function calls:",
        "segment_47": "add_function: Add one new function that may be used in the future tasks.",
        "segment_49": "ADD_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"add_function\", \"description\": \"Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_51": "revise_function: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.",
        "segment_53": "REVISE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"revise_function\", \"description\": \"Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_55": "remove_function: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.",
        "segment_57": "REMOVE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"remove_function\", \"description\": \"Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" } }, \"required\": [\"name\"] } }}",
        "segment_58": "Limitation & Future work\u200b",
        "segment_60": "Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.",
        "segment_61": "When the training epoch is small, the agent\u2019s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.",
        "segment_62": "The Current implementation of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.",
        "segment_63": "It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.",
        "segment_64": "Currently, it only supports optimizing the multi-agent system in solving problems from MATH dataset. We will integrate it into autogen with more general interface in the future.",
        "segment_65": "Tags:LLMresearchAgent AutoBuild - Automatically Building Multi-agent SystemsNovember 26, 2023 \u00b7 7 min readLinxin SongMS student at Waseda UniversityJieyu ZhangPhD student at University of Washington",
        "segment_66": "TL;DR:",
        "segment_67": "Introducing AutoBuild, building multi-agent system automatically, fast, and easily for complex tasks with minimal",
        "segment_68": "user prompt required, powered by a new designed class AgentBuilder. AgentBuilder also supports open-source LLMs by",
        "segment_69": "leveraging vLLM and FastChat.",
        "segment_70": "Checkout example notebooks and source code for reference:",
        "segment_72": "AutoBuild Examples",
        "segment_73": "AgentBuilder",
        "segment_75": "Introduction\u200b",
        "segment_76": "In this blog, we introduce AutoBuild, a pipeline that can automatically build multi-agent systems for complex tasks.",
        "segment_77": "Specifically, we design a new class called AgentBuilder, which will complete the generation of participant expert agents",
        "segment_78": "and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.",
        "segment_79": "AgentBuilder supports open-source models on Hugging Face powered by vLLM",
        "segment_80": "and FastChat. Once the user chooses to use open-source LLM, AgentBuilder will set",
        "segment_81": "up an endpoint server automatically without any user participation.",
        "segment_82": "Installation\u200b",
        "segment_84": "AutoGen:",
        "segment_86": "pip install pyautogen[autobuild]",
        "segment_88": "(Optional: if you want to use open-source LLMs) vLLM and FastChat",
        "segment_90": "pip install vllm fastchat",
        "segment_91": "Basic Example\u200b",
        "segment_92": "In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.",
        "segment_93": "Step 1: prepare configurations\u200b",
        "segment_94": "First, we need to prepare the Agent configurations.",
        "segment_95": "Specifically, a config path containing the model name and API key, and a default config for each agent, are required.",
        "segment_96": "config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST' # modify pathdefault_llm_config = { 'temperature': 0}",
        "segment_97": "Step 2: create an AgentBuilder instance\u200b",
        "segment_98": "Then, we create an AgentBuilder instance with the config path and default config.",
        "segment_99": "You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.",
        "segment_100": "from autogen.agentchat.contrib.agent_builder import AgentBuilderbuilder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')",
        "segment_101": "Step 3: specify the building task\u200b",
        "segment_102": "Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.",
        "segment_103": "Note that your building task should have a general description of the task. Adding some specific examples is better.",
        "segment_104": "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"",
        "segment_105": "Step 4: build group chat agents\u200b",
        "segment_106": "Use build() to let the build manager (with a builder_model as backbone) complete the group chat agents generation.",
        "segment_107": "If you think coding is necessary for your task, you can use coding=True to add a user proxy (a local code interpreter) into the agent list as:",
        "segment_108": "agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)",
        "segment_109": "If coding is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.",
        "segment_110": "The generated agent_list is a list of AssistantAgent instances.",
        "segment_111": "If coding is true, a user proxy (a UserProxyAssistant instance) will be added as the first element to the agent_list.",
        "segment_112": "agent_configs is a list of agent configurations including agent name, backbone LLM model, and system message.",
        "segment_113": "For example",
        "segment_114": "// an example of agent_configs. AgentBuilder will generate agents with the following configurations.[ { \"name\": \"ArXiv_Data_Scraper_Developer\", \"model\": \"gpt-4-1106-preview\", \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\", \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\" }, ...]",
        "segment_115": "Step 5: execute the task\u200b",
        "segment_116": "Let agents generated in build() complete the task collaboratively in a group chat.",
        "segment_117": "import autogendef start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\"]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task)start_task( execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\", agent_list=agent_list, llm_config=default_llm_config)",
        "segment_118": "Step 6 (Optional): clear all agents and prepare for the next task\u200b",
        "segment_119": "You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.",
        "segment_120": "builder.clear_all_agents(recycle_endpoint=True)",
        "segment_121": "If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.",
        "segment_122": "If necessary, you can use recycle_endpoint=False to retain the previous open-source LLM's endpoint server.",
        "segment_123": "Save and Load\u200b",
        "segment_124": "You can save all necessary information of the built group chat agents by",
        "segment_125": "saved_path = builder.save()",
        "segment_126": "Configurations will be saved in JSON format with the following content:",
        "segment_127": "// FILENAME: save_config_TASK_MD5.json{ \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"agent_configs\": [ { \"name\": \"...\", \"model\": \"...\", \"system_message\": \"...\", \"description\": \"...\" }, ... ], \"manager_system_message\": \"...\", \"code_execution_config\": {...}, \"default_llm_config\": {...}}",
        "segment_128": "You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename save_config_TASK_MD5.json.",
        "segment_129": "You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.",
        "segment_130": "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)agent_list, agent_config = new_builder.load(saved_path)start_task(...) # skip build()",
        "segment_131": "Use OpenAI Assistant\u200b",
        "segment_132": "Assistants API allows you to build AI assistants within your own applications.",
        "segment_133": "An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.",
        "segment_134": "AutoBuild also supports the assistant API by adding use_oai_assistant=True to build().",
        "segment_135": "# Transfer to the OpenAI Assistant API.agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)...",
        "segment_136": "(Experimental) Use Open-source LLM\u200b",
        "segment_137": "AutoBuild supports open-source LLM by vLLM and FastChat.",
        "segment_138": "Check the supported model list here.",
        "segment_139": "After satisfying the requirements, you can add an open-source LLM's huggingface repository to the config file,",
        "segment_140": "// Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.[ ... { \"model\": \"meta-llama/Llama-2-13b-chat-hf\", \"api_key\": \"EMPTY\" }]",
        "segment_141": "and specify it when initializing AgentBuilder.",
        "segment_142": "AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.",
        "segment_143": "Future work/Roadmap\u200b",
        "segment_145": "Let the builder select the best agents from a given library/database to solve the task.",
        "segment_147": "Summary\u200b",
        "segment_148": "We propose AutoBuild with a new class AgentBuilder.",
        "segment_149": "AutoBuild can help user solve their complex task with an automatically built multi-agent system.",
        "segment_150": "AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.",
        "segment_151": "More advanced features are coming soon.Tags:LLMresearchHow to Assess Utility of LLM-powered Applications?November 20, 2023 \u00b7 10 min readJulia KiselevaSenior Researcher at Microsoft ResearchNegar ArabzadehPhD student at the University of Waterloo",
        "segment_152": "Fig.1 illustrates the general flow of AgentEval",
        "segment_153": "TL;DR:",
        "segment_155": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_156": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_157": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_159": "Introduction\u200b",
        "segment_160": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_161": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_163": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_164": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_166": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_167": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_169": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_170": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_174": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_175": "AgentEval Framework\u200b",
        "segment_176": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_178": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_180": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_181": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_183": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_185": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_186": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_187": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_188": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_189": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_191": "AgentChat",
        "segment_192": "ReAct",
        "segment_193": "GPT-4 Vanilla Solver",
        "segment_195": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_197": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_198": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_199": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_200": "Limitations and Future Work\u200b",
        "segment_201": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_203": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_204": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_206": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_207": "Summary\u200b",
        "segment_208": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_209": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_210": "Previous Research\u200b",
        "segment_211": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_212": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityEcoAssistant - Using LLM Assistants More Accurately and AffordablyNovember 9, 2023 \u00b7 5 min readJieyu ZhangPhD student at University of Washington",
        "segment_213": "TL;DR:",
        "segment_215": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_216": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_217": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_218": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_220": "EcoAssistant\u200b",
        "segment_221": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_222": "Problem setup\u200b",
        "segment_223": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_224": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_225": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_226": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_227": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_228": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_229": "Leveraging external APIs\u200b",
        "segment_230": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_231": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_232": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_233": "A visualization of the two-agent system is shown below.",
        "segment_235": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_236": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_238": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_239": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_240": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_241": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_242": "Solution Demonstration\u200b",
        "segment_243": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_244": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_245": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_246": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_247": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_249": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_250": "Assistant Hierarchy\u200b",
        "segment_251": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_252": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_253": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_254": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_255": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_256": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_257": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_258": "A Synergistic Effect\u200b",
        "segment_259": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_260": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_261": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_262": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_263": "Experimental Results\u200b",
        "segment_264": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_265": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_267": "Further reading\u200b",
        "segment_268": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_269": "If you find this blog useful, please consider citing:",
        "segment_270": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessAutoGen's Teachable AgentsOctober 26, 2023 \u00b7 17 min readRicky LoyndSenior Research Engineer at Microsoft",
        "segment_271": "TL;DR:",
        "segment_273": "We introduce Teachable Agents so that users can teach their LLM-based assistants new facts, preferences, and skills.",
        "segment_274": "We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.",
        "segment_276": "Introduction\u200b",
        "segment_277": "Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.",
        "segment_278": "Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.",
        "segment_279": "Any instantiated agent that inherits from ConversableAgent can be made teachable by instantiating a Teachability object and calling its add_to_agent(agent) method.",
        "segment_280": "In order to make effective decisions about memo storage and retrieval, the Teachability object calls an instance of TextAnalyzerAgent (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.",
        "segment_281": "Run It Yourself\u200b",
        "segment_282": "AutoGen contains four code examples that use Teachability.",
        "segment_285": "Run chat_with_teachable_agent.py to converse with a teachable agent.",
        "segment_288": "Run test_teachable_agent.py for quick unit testing of a teachable agent.",
        "segment_291": "Use the Jupyter notebook agentchat_teachability.ipynb to step through examples discussed below.",
        "segment_294": "Use the Jupyter notebook agentchat_teachable_oai_assistants.ipynb to make arbitrary OpenAI Assistants teachable through GPTAssistantAgent.",
        "segment_297": "Basic Usage of Teachability\u200b",
        "segment_299": "Install dependencies",
        "segment_301": "Please install pyautogen with the [teachable] option before using Teachability.",
        "segment_302": "pip install \"pyautogen[teachable]\"",
        "segment_304": "Import agents",
        "segment_306": "from autogen import UserProxyAgent, config_list_from_jsonfrom autogen.agentchat.contrib.capabilities.teachability import Teachabilityfrom autogen import ConversableAgent # As an example",
        "segment_308": "Create llm_config",
        "segment_310": "# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_samplefilter_dict = {\"model\": [\"gpt-4\"]} # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", filter_dict=filter_dict)llm_config={\"config_list\": config_list, \"timeout\": 120}",
        "segment_312": "Create the agents",
        "segment_314": "# Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.teachable_agent = ConversableAgent( name=\"teachable_agent\", # The name can be anything. llm_config=llm_config)# Instantiate a Teachability object. Its parameters are all optional.teachability = Teachability( reset_db=False, # Use True to force-reset the memo DB, and False to use an existing DB. path_to_db_dir=\"./tmp/interactive/teachability_db\" # Can be any path, but teachable agents in a group chat require unique paths.)# Now add teachability to the agent.teachability.add_to_agent(teachable_agent)# For this test, create a user proxy agent as usual.user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")",
        "segment_316": "Chat with the teachable agent",
        "segment_318": "# This function will return once the user types 'exit'.teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")",
        "segment_319": "Example 1 - Learning user info\u200b",
        "segment_320": "A user can teach the agent facts about themselves.",
        "segment_321": "(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)",
        "segment_322": "Loading previous memory (if any) from disk.teachable_agent (to user):Greetings, I'm a teachable user assistant! What's on your mind today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Rickyuser (to teachable_agent):My name is Ricky-------------------------------------------------------------------------------->>>>>>>> USING AUTO REPLY...teachable_agent (to user):Hello Ricky! How can I assist you today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit",
        "segment_323": "In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)",
        "segment_324": "--------------------------------------------------------------------------------user (to teachable_agent):what's my name?--------------------------------------------------------------------------------teachable_agent (to user):Hi Ricky! How can I help you today?--------------------------------------------------------------------------------",
        "segment_325": "Example 2 - Learning new facts\u200b",
        "segment_326": "A user can teach the agent more complex, related facts.",
        "segment_327": "--------------------------------------------------------------------------------user (to teachable_agent):What is the Vicuna model?--------------------------------------------------------------------------------teachable_agent (to user):I'm sorry, but I don't have information about the \"Vicuna model\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?--------------------------------------------------------------------------------user (to teachable_agent):Vicuna is a 13B-parameter language model released by Meta.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.--------------------------------------------------------------------------------user (to teachable_agent):What is the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.--------------------------------------------------------------------------------user (to teachable_agent):Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.",
        "segment_328": "Then in a later chat the teachable agent can answer questions about the facts it has been taught.",
        "segment_329": "(Remember to first close the previous chat by typing 'exit'.)",
        "segment_330": "--------------------------------------------------------------------------------user (to teachable_agent):How does the Vicuna model compare to the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.--------------------------------------------------------------------------------",
        "segment_331": "Example 3 - Learning user preferences\u200b",
        "segment_332": "A user can teach the agent how they prefer to have things done.",
        "segment_333": "Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.",
        "segment_334": "Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.",
        "segment_335": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.--------------------------------------------------------------------------------Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points: the title, the innovation, and the key empirical results.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.--------------------------------------------------------------------------------",
        "segment_336": "Then in later chats the teacher doesn't need to reiterate their detailed preferences.",
        "segment_337": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.Sparks of Artificial General Intelligence: Early experiments with GPT-4S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi ZhangArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.--------------------------------------------------------------------------------teachable_agent (to user):- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.--------------------------------------------------------------------------------",
        "segment_338": "Example 4 - Learning new skills\u200b",
        "segment_339": "Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.",
        "segment_340": "The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.",
        "segment_341": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------Step 1: Identify the current equation and the target value on the right-hand side.Current equation: 9 * 4 + 6 * 6 = 72Target value: 99Step 2: Determine what numbers can be changed on the left-hand side.Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.9 -> If changed, it would create a larger product.4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.Step 4: Write down the solution.The solution is: 9 * 4 + 6 * 7--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".Here's some advice that may help:1. Let E denote the original number on the right.2. Let F denote the final number on the right.3. Calculate the difference between the two, G = F - E.4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.5. Calculate J = G / H. This is the number of times that H divides into G.6. Verify that J is an integer, and that H * J = G.7. Find the number on the left which is multiplied by H, and call it K.8. Change K to K + J.9. Recompute the value on the left, and verify that it equals F.Finally, write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Step 1: Identify the original number (E) and the final number (F) on the right-hand side.E = 72F = 99Step 2: Calculate the difference between the two, G = F - E.G = 99 - 72 = 27Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.9 (divides evenly into 27)4 (does not)6 (first occurrence, does not)6 (second occurrence, does not)Step 4: Calculate J = G / H. This is the number of times that H divides into G.H = 9 (since 9 divides evenly into 27)J = 27 / 9 = 3Step 5: Verify that J is an integer, and that H * J = G.H * J = 9 * 3 = 27J = 3 (which is an integer)Step 6: Find the number on the left which is multiplied by H, and call it K.Number on the leftside of the equation that is multiplied by 9: 4Therefore, K = 4Step 7: Change K to K + J.K + J = 4 + 3 = 7Step 8: Recompute the value on the left, and verify that it equals F.9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)Step 9: Write down the solution.The solution is: 9 * 7 + 6 * 6--------------------------------------------------------------------------------",
        "segment_342": "In a later chat the user doesn't need to repeat the detailed advice.",
        "segment_343": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:8 * 3 + 7 * 9 = 87Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:1. Let E denote the original number on the right: E = 87.2. Let F denote the final number on the right: F = 59.3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).5. Calculate J = G / H: J = -28 / 7 = -4.6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.8. Change K to K + J: K_new = 9 + (-4) = 5.9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.The solution is: 8 * 3 + 7 * 5--------------------------------------------------------------------------------",
        "segment_344": "Planned improvements\u200b",
        "segment_346": "Understanding user instructions distributed over multiple turns.",
        "segment_347": "Learning from the agent's own experience, to reduce dependence on explicit user teachings.",
        "segment_348": "Learning skills built on top of previously learned skills.",
        "segment_350": "Conclusion\u200b",
        "segment_351": "Teachability is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our Discord channel. We look forward to seeing how you and the rest of the community can use and improve teachable agents in AutoGen!Tags:LLMteachRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_352": "TL;DR:",
        "segment_354": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_355": "allows retrieval-augmented generation, and its basic usage.",
        "segment_356": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_357": "split function and vector database.",
        "segment_358": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_359": "application with Gradio.",
        "segment_361": "Introduction\u200b",
        "segment_362": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_363": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_364": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_365": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_366": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_367": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_368": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_369": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_370": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_371": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_372": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_373": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_375": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_376": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_377": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_378": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_379": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_380": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_381": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_382": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_383": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_384": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_385": "Retrieval-Augmented Assistant.",
        "segment_386": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_387": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_388": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_389": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_390": "The conversation terminates if no more documents are available for the context.",
        "segment_392": "Basic Usage of RAG Agents\u200b",
        "segment_394": "Install dependencies",
        "segment_396": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_397": "pip install \"pyautogen[retrievechat]\"",
        "segment_398": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_399": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_400": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_401": "If you install unstructured",
        "segment_402": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_403": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_404": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_406": "Import Agents",
        "segment_408": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_410": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_412": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_414": "Initialize Chat and ask a question",
        "segment_416": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_417": "Output is like:",
        "segment_418": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_420": "Create a UserProxyAgent and ask the same question",
        "segment_422": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_423": "Output is like:",
        "segment_424": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_425": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_426": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_427": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_428": "Customizing RAG Agents\u200b",
        "segment_429": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_430": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_431": "function and vector database.",
        "segment_432": "Customizing Embedding Function\u200b",
        "segment_433": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_434": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_436": "OpenAI",
        "segment_438": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_440": "HuggingFace",
        "segment_442": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_443": "More examples can be found here.",
        "segment_444": "Customizing Text Split Function\u200b",
        "segment_445": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_446": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_447": "There are also some existing text split tools which are good to reuse.",
        "segment_448": "For example, you can use all the text splitters in langchain.",
        "segment_449": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_450": "Customizing Vector Database\u200b",
        "segment_451": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_452": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_453": "For example, you can use Qdrant as below:",
        "segment_454": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_455": "Advanced Usage of RAG Agents\u200b",
        "segment_456": "Integrate with other agents in a group chat\u200b",
        "segment_457": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_458": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_459": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_460": "you'll need to call it from a function.",
        "segment_461": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_462": "Build a Chat application with Gradio\u200b",
        "segment_463": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_465": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_466": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_467": "Read More\u200b",
        "segment_468": "You can check out more example notebooks for RAG use cases:",
        "segment_470": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_471": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_472": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_473": "Tags:LLMRAGUse AutoGen for Local LLMsJuly 14, 2023 \u00b7 3 min readJiale LiuUndergraduate student at Xidian UniversityTL;DR:",
        "segment_474": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_475": "Preparations\u200b",
        "segment_476": "Clone FastChat\u200b",
        "segment_477": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_478": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_479": "Download checkpoint\u200b",
        "segment_480": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_481": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_482": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_483": "Initiate server\u200b",
        "segment_484": "First, launch the controller",
        "segment_485": "python -m fastchat.serve.controller",
        "segment_486": "Then, launch the model worker(s)",
        "segment_487": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_488": "Finally, launch the RESTful API server",
        "segment_489": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_490": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_491": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_492": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_493": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_494": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_495": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_496": "interacting with multiple local LLMs\u200b",
        "segment_497": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_498": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_499": "The inference code would be:",
        "segment_500": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_501": "For Further Reading\u200b",
        "segment_503": "Documentation about autogen.",
        "segment_504": "Documentation about FastChat.",
        "segment_505": "Tags:LLMMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_506": "TL;DR:",
        "segment_508": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_509": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_510": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_512": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_513": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_514": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_515": "The MathChat Framework\u200b",
        "segment_516": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_517": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_520": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_523": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_525": "Writing a Python program to solve the problem directly.",
        "segment_526": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_527": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_531": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_534": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_535": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_536": "(The conversation is modified for readability.)",
        "segment_537": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_538": "Experiment Setup\u200b",
        "segment_539": "We evaluate the improvement brought by MathChat.",
        "segment_540": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_541": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_542": "We also evaluate the following methods for comparison:",
        "segment_545": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_548": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_551": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_554": "Experiment Results\u200b",
        "segment_555": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_557": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_558": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_559": "The code for experiments can be found at this repository.",
        "segment_560": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_561": "Future Directions\u200b",
        "segment_562": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_563": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_565": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_566": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_567": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_569": "For Further Reading\u200b",
        "segment_571": "Research paper of MathChat",
        "segment_572": "Documentation about autogen",
        "segment_574": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_575": "TL;DR:",
        "segment_577": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_579": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_580": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_581": "Observations\u200b",
        "segment_583": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_584": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_586": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_587": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_588": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_589": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_590": "What else can we do? We notice that:",
        "segment_591": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_592": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_593": "Solution\u200b",
        "segment_594": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_596": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_597": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_600": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_601": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_603": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_604": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_605": "GPT-4, n=1, temperature=0",
        "segment_606": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_607": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_609": "Experiment Results\u200b",
        "segment_610": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_611": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_612": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_614": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_616": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_618": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_619": "Solved by GPT-4, n=1, temperature=0:",
        "segment_621": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_623": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_625": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_627": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_629": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_630": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_632": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_633": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_635": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_636": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_637": "Discussion\u200b",
        "segment_638": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_639": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_641": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_642": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_644": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_645": "Different configurations have complementary effect.",
        "segment_649": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_650": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_651": "There are many directions of extensions in research and development:",
        "segment_653": "Generalize the way to provide feedback.",
        "segment_654": "Automate the process of optimizing the configurations.",
        "segment_655": "Build adaptive agents for different applications.",
        "segment_657": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_658": "For Further Reading\u200b",
        "segment_660": "Documentation about autogen and Research paper.",
        "segment_661": "Blog post about a related study for math.",
        "segment_662": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_663": "TL;DR:",
        "segment_665": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_666": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_667": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_669": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_670": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_671": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_672": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_673": "Experiment Setup\u200b",
        "segment_674": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_676": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_677": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_679": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_681": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_682": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_683": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_684": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_685": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_687": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_688": "Experiment Results\u200b",
        "segment_689": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_690": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_691": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_693": "However, the selected model changes on level 4 Algebra.",
        "segment_695": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_696": "On level 5 the result is similar.",
        "segment_698": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_699": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_700": "Analysis and Discussion\u200b",
        "segment_701": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_702": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_703": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_704": "For Further Reading\u200b",
        "segment_706": "Research paper about the tuning technique",
        "segment_707": "Documentation about inference tuning",
        "segment_709": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen 0.2.8 enhances operational safety by making 'code execution inside a Docker container' the default setting, focusing on informing users about its operations and empowering them to make informed decisions regarding code execution.",
        "segment_2": "The new release introduces a breaking change where the use_docker argument is set to True by default in code executing agents. This change underscores our commitment to prioritizing security and safety in AutoGen.",
        "segment_3": "Introduction\u200b",
        "segment_4": "AutoGen has code-executing agents, usually defined as a UserProxyAgent, where code execution is by default ON. Until now, unless explicitly specified by the user, any code generated by other agents would be executed by code-execution agents locally, i.e. wherever AutoGen was being executed. If AutoGen happened to be run in a docker container then the risks of running code were minimized. However, if AutoGen runs outside of Docker, it's easy particularly for new users to overlook code-execution risks.",
        "segment_5": "AutoGen has now changed to by default execute any code inside a docker container (unless execution is already happening inside a docker container). It will launch a Docker image (either user-provided or default), execute the new code, and then terminate the image, preparing for the next code execution cycle.",
        "segment_6": "We understand that not everyone is concerned about this especially when playing around with AutoGen for the first time. We have provided easy ways to turn this requirement off. But we believe that making sure that the user is aware of the fact that code will be executed locally, and prompting them to think about the security implications of running code locally is the right step for AutoGen.",
        "segment_7": "Example\u200b",
        "segment_8": "The example shows the default behaviour which is that any code generated by assistant agent and executed by user_proxy agent, will attempt to use a docker container to execute the code. If docker is not running, it will throw an error. User can decide to activate docker or opt in for local code execution.",
        "segment_9": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_jsonassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")",
        "segment_10": "To opt out of from this default behaviour there are some options.",
        "segment_11": "Diasable code execution entirely\u200b",
        "segment_13": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_15": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config=False)",
        "segment_16": "Run code execution locally\u200b",
        "segment_18": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_19": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_21": "E.g.:",
        "segment_22": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})",
        "segment_23": "Related documentation\u200b",
        "segment_25": "Code execution with docker",
        "segment_26": "How to disable code execution in docker",
        "segment_28": "Conclusion\u200b",
        "segment_29": "AutoGen 0.2.8 now improves the code execution safety and is ensuring that the user is properly informed of what autogen is doing and can make decisions around code-execution.Tags:AutoGenNewer PostAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsOlder PostAll About Agent DescriptionsTLDRIntroductionExampleDiasable code execution entirelyRun code execution locallyRelated documentationConclusionCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class MultimodalConversableAgent(ConversableAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name: str, system_message: Optional[Union[str, List]] = DEFAULT_LMM_SYS_MSG, is_termination_msg: str = None, *args, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - agent name.",
        "segment_7": "system_message str - system message for the OpenAIWrapper inference.",
        "segment_8": "Please override this attribute if you want to reprogram the agent.",
        "segment_9": "**kwargs dict - Please refer to other kwargs in",
        "segment_10": "ConversableAgent.",
        "segment_12": "update_system_message\u200b",
        "segment_13": "def update_system_message(system_message: Union[Dict, List, str])",
        "segment_14": "Update the system message.",
        "segment_15": "Arguments:",
        "segment_17": "system_message str - system message for the OpenAIWrapper inference.",
        "segment_18": "Edit this pagePreviousmath_user_proxy_agentNextqdrant_retrieve_user_proxy_agentMultimodalConversableAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "These formats will be parsed by the 'unstructured' library, if installed.",
        "segment_2": "split_text_to_chunks\u200b",
        "segment_3": "def split_text_to_chunks(text: str, max_tokens: int = 4000, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, overlap: int = 10)",
        "segment_4": "Split a long text into chunks of max_tokens.",
        "segment_5": "extract_text_from_pdf\u200b",
        "segment_6": "def extract_text_from_pdf(file: str) -> str",
        "segment_7": "Extract text from PDF files",
        "segment_8": "split_files_to_chunks\u200b",
        "segment_9": "def split_files_to_chunks(files: list, max_tokens: int = 4000, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, custom_text_split_function: Callable = None)",
        "segment_10": "Split a list of files into chunks of max_tokens.",
        "segment_11": "get_files_from_dir\u200b",
        "segment_12": "def get_files_from_dir(dir_path: Union[str, List[str]], types: list = TEXT_FORMATS, recursive: bool = True)",
        "segment_13": "Return a list of all the files in a given directory, a url, a file path or a list of them.",
        "segment_14": "get_file_from_url\u200b",
        "segment_15": "def get_file_from_url(url: str, save_path: str = None)",
        "segment_16": "Download a file from a URL.",
        "segment_17": "is_url\u200b",
        "segment_18": "def is_url(string: str)",
        "segment_19": "Return True if the string is a valid URL.",
        "segment_20": "create_vector_db_from_dir\u200b",
        "segment_21": "def create_vector_db_from_dir(dir_path: Union[str, List[str]], max_tokens: int = 4000, client: API = None, db_path: str = \"/tmp/chromadb.db\", collection_name: str = \"all-my-documents\", get_or_create: bool = False, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, embedding_model: str = \"all-MiniLM-L6-v2\", embedding_function: Callable = None, custom_text_split_function: Callable = None, custom_text_types: List[str] = TEXT_FORMATS, recursive: bool = True, extra_docs: bool = False) -> API",
        "segment_22": "Create a vector db from all the files in a given directory, the directory can also be a single file or a url to",
        "segment_23": "a single file. We support chromadb compatible APIs to create the vector db, this function is not required if",
        "segment_24": "you prepared your own vector db.",
        "segment_25": "Arguments:",
        "segment_27": "dir_path Union[str, List[str]] - the path to the directory, file, url or a list of them.",
        "segment_28": "max_tokens Optional, int - the maximum number of tokens per chunk. Default is 4000.",
        "segment_29": "client Optional, API - the chromadb client. Default is None.",
        "segment_30": "db_path Optional, str - the path to the chromadb. Default is \"/tmp/chromadb.db\".",
        "segment_31": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_32": "get_or_create Optional, bool - Whether to get or create the collection. Default is False. If True, the collection",
        "segment_33": "will be returned if it already exists. Will raise ValueError if the collection already exists and get_or_create is False.",
        "segment_34": "chunk_mode Optional, str - the chunk mode. Default is \"multi_lines\".",
        "segment_35": "must_break_at_empty_line Optional, bool - Whether to break at empty line. Default is True.",
        "segment_36": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if",
        "segment_37": "embedding_function is not None.",
        "segment_38": "embedding_function Optional, Callable - the embedding function to use. Default is None, SentenceTransformer with",
        "segment_39": "the given embedding_model will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding",
        "segment_40": "functions, you can pass it here, follow the examples in max_tokens1.",
        "segment_41": "max_tokens2 Optional, Callable - a custom function to split a string into a list of strings.",
        "segment_42": "Default is None, will use the default function in max_tokens3.",
        "segment_43": "max_tokens4 Optional, List[str] - a list of file types to be processed. Default is TEXT_FORMATS.",
        "segment_44": "max_tokens5 Optional, bool - whether to search documents recursively in the dir_path. Default is True.",
        "segment_45": "max_tokens6 Optional, bool - whether to add more documents in the collection. Default is False",
        "segment_47": "Returns:",
        "segment_49": "max_tokens7 - the chromadb client.",
        "segment_51": "query_vector_db\u200b",
        "segment_52": "def query_vector_db(query_texts: List[str], n_results: int = 10, client: API = None, db_path: str = \"/tmp/chromadb.db\", collection_name: str = \"all-my-documents\", search_string: str = \"\", embedding_model: str = \"all-MiniLM-L6-v2\", embedding_function: Callable = None) -> QueryResult",
        "segment_53": "Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db",
        "segment_54": "and query function.",
        "segment_55": "Arguments:",
        "segment_57": "query_texts List[str] - the list of strings which will be used to query the vector db.",
        "segment_58": "n_results Optional, int - the number of results to return. Default is 10.",
        "segment_59": "client Optional, API - the chromadb compatible client. Default is None, a chromadb client will be used.",
        "segment_60": "db_path Optional, str - the path to the vector db. Default is \"/tmp/chromadb.db\".",
        "segment_61": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_62": "search_string Optional, str - the search string. Only docs that contain an exact match of this string will be retrieved. Default is \"\".",
        "segment_63": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if",
        "segment_64": "embedding_function is not None.",
        "segment_65": "embedding_function Optional, Callable - the embedding function to use. Default is None, SentenceTransformer with",
        "segment_66": "the given embedding_model will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding",
        "segment_67": "functions, you can pass it here, follow the examples in https://docs.trychroma.com/embeddings.",
        "segment_69": "Returns:",
        "segment_71": "n_results0 - the query result. The format is:",
        "segment_72": "class QueryResult(TypedDict):",
        "segment_73": "n_results1 - List[IDs]",
        "segment_74": "n_results2 - Optional[List[List[Embedding]]]",
        "segment_75": "n_results3 - Optional[List[List[Document]]]",
        "segment_76": "n_results4 - Optional[List[List[Metadata]]]",
        "segment_77": "n_results5 - Optional[List[List[float]]]",
        "segment_78": "Edit this pagePreviousmath_utilsNexttoken_count_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_0": "t(null!==e?e:\"light\")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith(\"docusaurus-data-\")){var a=t.replace(\"docusaurus-data-\",\"data-\");document.documentElement.setAttribute(a,e)}}catch(t){}}()Skip to main contentAutoGenDocsSDKBlogFAQExamplesResourcesEcosystemGalleryGitHubctrlKRecent postsAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsCode execution is now by default inside docker containerAll About Agent DescriptionsAgentOptimizer - An Agentic Way to Train Your LLM AgentAutoGen Studio: Interactively Explore Multi-Agent WorkflowsAgent AutoBuild - Automatically Building Multi-agent SystemsHow to Assess Utility of LLM-powered Applications?AutoGen Meets GPTsEcoAssistant - Using LLM Assistants More Accurately and AffordablyMultimodal with GPT-4V and LLaVAAutoGen's Teachable AgentsRetrieval-Augmented Generation (RAG) Applications with AutoGenUse AutoGen for Local LLMsMathChat - An Conversational Framework to Solve Math ProblemsAchieve More, Pay Less - Use GPT-4 SmartlyDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHMultimodal with GPT-4V and LLaVANovember 6, 2023 \u00b7 3 min readBeibin LiSenior Research Engineer at Microsoft",
        "segment_1": "In Brief:",
        "segment_3": "Introducing the Multimodal Conversable Agent and the LLaVA Agent to enhance LMM functionalities.",
        "segment_4": "Users can input text and images simultaneously using the tag to specify image loading.",
        "segment_5": "Demonstrated through the GPT-4V notebook.",
        "segment_6": "Demonstrated through the LLaVA notebook.",
        "segment_8": "Introduction\u200b",
        "segment_9": "Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.",
        "segment_10": "This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.",
        "segment_11": "We support the gpt-4-vision-preview model from OpenAI and LLaVA model from Microsoft now.",
        "segment_12": "Here, we emphasize the Multimodal Conversable Agent and the LLaVA Agent due to their growing popularity.",
        "segment_13": "GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.",
        "segment_14": "Installation\u200b",
        "segment_15": "Incorporate the lmm feature during AutoGen installation:",
        "segment_16": "pip install \"pyautogen[lmm]\"",
        "segment_17": "Subsequently, import the Multimodal Conversable Agent or LLaVA Agent from AutoGen:",
        "segment_18": "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent # for GPT-4Vfrom autogen.agentchat.contrib.llava_agent import LLaVAAgent # for LLaVA",
        "segment_19": "Usage\u200b",
        "segment_20": "A simple syntax has been defined to incorporate both messages and images within a single string.",
        "segment_21": "Example of an in-context learning prompt:",
        "segment_22": "prompt = \"\"\"You are now an image classifier for facial expressions. Here aresome examples. depicts a happy expression. represents a sad expression. portrays a neutral expression.Now, identify the facial expression of this individual: \"\"\"agent = MultimodalConversableAgent()user = UserProxyAgent()user.initiate_chat(agent, message=prompt)",
        "segment_23": "The MultimodalConversableAgent interprets the input prompt, extracting images from local or internet sources.",
        "segment_24": "Advanced Usage\u200b",
        "segment_25": "Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.",
        "segment_26": "For example, the FigureCreator in our GPT-4V notebook and LLaVA notebook integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).",
        "segment_27": "The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.",
        "segment_28": "With human_input_mode=ALWAYS, you can also contribute suggestions for better visualizations.",
        "segment_29": "Reference\u200b",
        "segment_31": "GPT-4V System Card",
        "segment_32": "LLaVA GitHub",
        "segment_34": "Future Enhancements\u200b",
        "segment_35": "For further inquiries or suggestions, please open an issue in the AutoGen repository or contact me directly at beibin.li@microsoft.com.",
        "segment_36": "AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.Tags:LMMmultimodalAutoGen's Teachable AgentsOctober 26, 2023 \u00b7 17 min readRicky LoyndSenior Research Engineer at Microsoft",
        "segment_37": "TL;DR:",
        "segment_39": "We introduce Teachable Agents so that users can teach their LLM-based assistants new facts, preferences, and skills.",
        "segment_40": "We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.",
        "segment_42": "Introduction\u200b",
        "segment_43": "Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.",
        "segment_44": "Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.",
        "segment_45": "Any instantiated agent that inherits from ConversableAgent can be made teachable by instantiating a Teachability object and calling its add_to_agent(agent) method.",
        "segment_46": "In order to make effective decisions about memo storage and retrieval, the Teachability object calls an instance of TextAnalyzerAgent (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.",
        "segment_47": "Run It Yourself\u200b",
        "segment_48": "AutoGen contains four code examples that use Teachability.",
        "segment_51": "Run chat_with_teachable_agent.py to converse with a teachable agent.",
        "segment_54": "Run test_teachable_agent.py for quick unit testing of a teachable agent.",
        "segment_57": "Use the Jupyter notebook agentchat_teachability.ipynb to step through examples discussed below.",
        "segment_60": "Use the Jupyter notebook agentchat_teachable_oai_assistants.ipynb to make arbitrary OpenAI Assistants teachable through GPTAssistantAgent.",
        "segment_63": "Basic Usage of Teachability\u200b",
        "segment_65": "Install dependencies",
        "segment_67": "Please install pyautogen with the [teachable] option before using Teachability.",
        "segment_68": "pip install \"pyautogen[teachable]\"",
        "segment_70": "Import agents",
        "segment_72": "from autogen import UserProxyAgent, config_list_from_jsonfrom autogen.agentchat.contrib.capabilities.teachability import Teachabilityfrom autogen import ConversableAgent # As an example",
        "segment_74": "Create llm_config",
        "segment_76": "# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_samplefilter_dict = {\"model\": [\"gpt-4\"]} # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", filter_dict=filter_dict)llm_config={\"config_list\": config_list, \"timeout\": 120}",
        "segment_78": "Create the agents",
        "segment_80": "# Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.teachable_agent = ConversableAgent( name=\"teachable_agent\", # The name can be anything. llm_config=llm_config)# Instantiate a Teachability object. Its parameters are all optional.teachability = Teachability( reset_db=False, # Use True to force-reset the memo DB, and False to use an existing DB. path_to_db_dir=\"./tmp/interactive/teachability_db\" # Can be any path, but teachable agents in a group chat require unique paths.)# Now add teachability to the agent.teachability.add_to_agent(teachable_agent)# For this test, create a user proxy agent as usual.user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")",
        "segment_82": "Chat with the teachable agent",
        "segment_84": "# This function will return once the user types 'exit'.teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")",
        "segment_85": "Example 1 - Learning user info\u200b",
        "segment_86": "A user can teach the agent facts about themselves.",
        "segment_87": "(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)",
        "segment_88": "Loading previous memory (if any) from disk.teachable_agent (to user):Greetings, I'm a teachable user assistant! What's on your mind today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Rickyuser (to teachable_agent):My name is Ricky-------------------------------------------------------------------------------->>>>>>>> USING AUTO REPLY...teachable_agent (to user):Hello Ricky! How can I assist you today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit",
        "segment_89": "In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)",
        "segment_90": "--------------------------------------------------------------------------------user (to teachable_agent):what's my name?--------------------------------------------------------------------------------teachable_agent (to user):Hi Ricky! How can I help you today?--------------------------------------------------------------------------------",
        "segment_91": "Example 2 - Learning new facts\u200b",
        "segment_92": "A user can teach the agent more complex, related facts.",
        "segment_93": "--------------------------------------------------------------------------------user (to teachable_agent):What is the Vicuna model?--------------------------------------------------------------------------------teachable_agent (to user):I'm sorry, but I don't have information about the \"Vicuna model\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?--------------------------------------------------------------------------------user (to teachable_agent):Vicuna is a 13B-parameter language model released by Meta.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.--------------------------------------------------------------------------------user (to teachable_agent):What is the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.--------------------------------------------------------------------------------user (to teachable_agent):Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.",
        "segment_94": "Then in a later chat the teachable agent can answer questions about the facts it has been taught.",
        "segment_95": "(Remember to first close the previous chat by typing 'exit'.)",
        "segment_96": "--------------------------------------------------------------------------------user (to teachable_agent):How does the Vicuna model compare to the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.--------------------------------------------------------------------------------",
        "segment_97": "Example 3 - Learning user preferences\u200b",
        "segment_98": "A user can teach the agent how they prefer to have things done.",
        "segment_99": "Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.",
        "segment_100": "Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.",
        "segment_101": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.--------------------------------------------------------------------------------Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points: the title, the innovation, and the key empirical results.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.--------------------------------------------------------------------------------",
        "segment_102": "Then in later chats the teacher doesn't need to reiterate their detailed preferences.",
        "segment_103": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.Sparks of Artificial General Intelligence: Early experiments with GPT-4S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi ZhangArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.--------------------------------------------------------------------------------teachable_agent (to user):- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.--------------------------------------------------------------------------------",
        "segment_104": "Example 4 - Learning new skills\u200b",
        "segment_105": "Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.",
        "segment_106": "The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.",
        "segment_107": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------Step 1: Identify the current equation and the target value on the right-hand side.Current equation: 9 * 4 + 6 * 6 = 72Target value: 99Step 2: Determine what numbers can be changed on the left-hand side.Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.9 -> If changed, it would create a larger product.4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.Step 4: Write down the solution.The solution is: 9 * 4 + 6 * 7--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".Here's some advice that may help:1. Let E denote the original number on the right.2. Let F denote the final number on the right.3. Calculate the difference between the two, G = F - E.4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.5. Calculate J = G / H. This is the number of times that H divides into G.6. Verify that J is an integer, and that H * J = G.7. Find the number on the left which is multiplied by H, and call it K.8. Change K to K + J.9. Recompute the value on the left, and verify that it equals F.Finally, write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Step 1: Identify the original number (E) and the final number (F) on the right-hand side.E = 72F = 99Step 2: Calculate the difference between the two, G = F - E.G = 99 - 72 = 27Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.9 (divides evenly into 27)4 (does not)6 (first occurrence, does not)6 (second occurrence, does not)Step 4: Calculate J = G / H. This is the number of times that H divides into G.H = 9 (since 9 divides evenly into 27)J = 27 / 9 = 3Step 5: Verify that J is an integer, and that H * J = G.H * J = 9 * 3 = 27J = 3 (which is an integer)Step 6: Find the number on the left which is multiplied by H, and call it K.Number on the leftside of the equation that is multiplied by 9: 4Therefore, K = 4Step 7: Change K to K + J.K + J = 4 + 3 = 7Step 8: Recompute the value on the left, and verify that it equals F.9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)Step 9: Write down the solution.The solution is: 9 * 7 + 6 * 6--------------------------------------------------------------------------------",
        "segment_108": "In a later chat the user doesn't need to repeat the detailed advice.",
        "segment_109": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:8 * 3 + 7 * 9 = 87Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:1. Let E denote the original number on the right: E = 87.2. Let F denote the final number on the right: F = 59.3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).5. Calculate J = G / H: J = -28 / 7 = -4.6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.8. Change K to K + J: K_new = 9 + (-4) = 5.9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.The solution is: 8 * 3 + 7 * 5--------------------------------------------------------------------------------",
        "segment_110": "Planned improvements\u200b",
        "segment_112": "Understanding user instructions distributed over multiple turns.",
        "segment_113": "Learning from the agent's own experience, to reduce dependence on explicit user teachings.",
        "segment_114": "Learning skills built on top of previously learned skills.",
        "segment_116": "Conclusion\u200b",
        "segment_117": "Teachability is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our Discord channel. We look forward to seeing how you and the rest of the community can use and improve teachable agents in AutoGen!Tags:LLMteachRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_118": "TL;DR:",
        "segment_120": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_121": "allows retrieval-augmented generation, and its basic usage.",
        "segment_122": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_123": "split function and vector database.",
        "segment_124": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_125": "application with Gradio.",
        "segment_127": "Introduction\u200b",
        "segment_128": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_129": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_130": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_131": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_132": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_133": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_134": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_135": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_136": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_137": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_138": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_139": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_141": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_142": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_143": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_144": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_145": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_146": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_147": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_148": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_149": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_150": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_151": "Retrieval-Augmented Assistant.",
        "segment_152": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_153": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_154": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_155": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_156": "The conversation terminates if no more documents are available for the context.",
        "segment_158": "Basic Usage of RAG Agents\u200b",
        "segment_160": "Install dependencies",
        "segment_162": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_163": "pip install \"pyautogen[retrievechat]\"",
        "segment_164": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_165": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_166": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_167": "If you install unstructured",
        "segment_168": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_169": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_170": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_172": "Import Agents",
        "segment_174": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_176": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_178": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_180": "Initialize Chat and ask a question",
        "segment_182": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_183": "Output is like:",
        "segment_184": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_186": "Create a UserProxyAgent and ask the same question",
        "segment_188": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_189": "Output is like:",
        "segment_190": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_191": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_192": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_193": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_194": "Customizing RAG Agents\u200b",
        "segment_195": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_196": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_197": "function and vector database.",
        "segment_198": "Customizing Embedding Function\u200b",
        "segment_199": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_200": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_202": "OpenAI",
        "segment_204": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_206": "HuggingFace",
        "segment_208": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_209": "More examples can be found here.",
        "segment_210": "Customizing Text Split Function\u200b",
        "segment_211": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_212": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_213": "There are also some existing text split tools which are good to reuse.",
        "segment_214": "For example, you can use all the text splitters in langchain.",
        "segment_215": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_216": "Customizing Vector Database\u200b",
        "segment_217": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_218": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_219": "For example, you can use Qdrant as below:",
        "segment_220": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_221": "Advanced Usage of RAG Agents\u200b",
        "segment_222": "Integrate with other agents in a group chat\u200b",
        "segment_223": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_224": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_225": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_226": "you'll need to call it from a function.",
        "segment_227": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_228": "Build a Chat application with Gradio\u200b",
        "segment_229": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_231": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_232": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_233": "Read More\u200b",
        "segment_234": "You can check out more example notebooks for RAG use cases:",
        "segment_236": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_237": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_238": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_239": "Tags:LLMRAGUse AutoGen for Local LLMsJuly 14, 2023 \u00b7 3 min readJiale LiuUndergraduate student at Xidian UniversityTL;DR:",
        "segment_240": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_241": "Preparations\u200b",
        "segment_242": "Clone FastChat\u200b",
        "segment_243": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_244": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_245": "Download checkpoint\u200b",
        "segment_246": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_247": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_248": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_249": "Initiate server\u200b",
        "segment_250": "First, launch the controller",
        "segment_251": "python -m fastchat.serve.controller",
        "segment_252": "Then, launch the model worker(s)",
        "segment_253": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_254": "Finally, launch the RESTful API server",
        "segment_255": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_256": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_257": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_258": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_259": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_260": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_261": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_262": "interacting with multiple local LLMs\u200b",
        "segment_263": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_264": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_265": "The inference code would be:",
        "segment_266": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_267": "For Further Reading\u200b",
        "segment_269": "Documentation about autogen.",
        "segment_270": "Documentation about FastChat.",
        "segment_271": "Tags:LLMMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_272": "TL;DR:",
        "segment_274": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_275": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_276": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_278": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_279": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_280": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_281": "The MathChat Framework\u200b",
        "segment_282": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_283": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_286": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_289": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_291": "Writing a Python program to solve the problem directly.",
        "segment_292": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_293": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_297": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_300": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_301": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_302": "(The conversation is modified for readability.)",
        "segment_303": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_304": "Experiment Setup\u200b",
        "segment_305": "We evaluate the improvement brought by MathChat.",
        "segment_306": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_307": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_308": "We also evaluate the following methods for comparison:",
        "segment_311": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_314": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_317": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_320": "Experiment Results\u200b",
        "segment_321": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_323": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_324": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_325": "The code for experiments can be found at this repository.",
        "segment_326": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_327": "Future Directions\u200b",
        "segment_328": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_329": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_331": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_332": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_333": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_335": "For Further Reading\u200b",
        "segment_337": "Research paper of MathChat",
        "segment_338": "Documentation about autogen",
        "segment_340": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_341": "TL;DR:",
        "segment_343": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_345": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_346": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_347": "Observations\u200b",
        "segment_349": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_350": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_352": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_353": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_354": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_355": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_356": "What else can we do? We notice that:",
        "segment_357": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_358": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_359": "Solution\u200b",
        "segment_360": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_362": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_363": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_366": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_367": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_369": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_370": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_371": "GPT-4, n=1, temperature=0",
        "segment_372": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_373": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_375": "Experiment Results\u200b",
        "segment_376": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_377": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_378": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_380": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_382": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_384": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_385": "Solved by GPT-4, n=1, temperature=0:",
        "segment_387": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_389": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_391": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_393": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_395": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_396": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_398": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_399": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_401": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_402": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_403": "Discussion\u200b",
        "segment_404": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_405": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_407": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_408": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_410": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_411": "Different configurations have complementary effect.",
        "segment_415": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_416": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_417": "There are many directions of extensions in research and development:",
        "segment_419": "Generalize the way to provide feedback.",
        "segment_420": "Automate the process of optimizing the configurations.",
        "segment_421": "Build adaptive agents for different applications.",
        "segment_423": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_424": "For Further Reading\u200b",
        "segment_426": "Documentation about autogen and Research paper.",
        "segment_427": "Blog post about a related study for math.",
        "segment_428": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_429": "TL;DR:",
        "segment_431": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_432": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_433": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_435": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_436": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_437": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_438": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_439": "Experiment Setup\u200b",
        "segment_440": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_442": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_443": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_445": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_447": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_448": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_449": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_450": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_451": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_453": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_454": "Experiment Results\u200b",
        "segment_455": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_456": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_457": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_459": "However, the selected model changes on level 4 Algebra.",
        "segment_461": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_462": "On level 5 the result is similar.",
        "segment_464": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_465": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_466": "Analysis and Discussion\u200b",
        "segment_467": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_468": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_469": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_470": "For Further Reading\u200b",
        "segment_472": "Research paper about the tuning technique",
        "segment_473": "Documentation about inference tuning",
        "segment_475": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer EntriesCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class UserProxyAgent(ConversableAgent)",
        "segment_2": "(In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents.",
        "segment_3": "UserProxyAgent is a subclass of ConversableAgent configured with human_input_mode to ALWAYS",
        "segment_4": "and llm_config to False. By default, the agent will prompt for human input every time a message is received.",
        "segment_5": "Code execution is enabled by default. LLM-based auto reply is disabled by default.",
        "segment_6": "To modify auto reply, register a method with register_reply.",
        "segment_7": "To modify the way to get human input, override get_human_input method.",
        "segment_8": "To modify the way to execute code blocks, single code block, or function call, override execute_code_blocks,",
        "segment_9": "run_code, and execute_function methods respectively.",
        "segment_10": "To customize the initial message when a conversation starts, override generate_init_message method.",
        "segment_11": "__init__\u200b",
        "segment_12": "def __init__(name: str, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"ALWAYS\", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Optional[Union[Dict, Literal[False]]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = \"\", llm_config: Optional[Union[Dict, Literal[False]]] = False, system_message: Optional[Union[str, List]] = \"\", description: Optional[str] = None)",
        "segment_13": "Arguments:",
        "segment_15": "name str - name of the agent.",
        "segment_16": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_17": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_18": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_19": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_20": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_21": "The limit only plays a role when human_input_mode is not \"ALWAYS\".",
        "segment_22": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_23": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_24": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_25": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_26": "or when is_termination_msg is True and there is no human input.",
        "segment_27": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_28": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_29": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_30": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_31": "function_map dict[str, callable] - Mapping function names (passed to openai) to callable functions.",
        "segment_32": "code_execution_config dict or False - config for the code execution.",
        "segment_33": "To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:",
        "segment_35": "work_dir (Optional, str): The working directory for the code execution.",
        "segment_36": "If None, a default working directory will be used.",
        "segment_37": "The default working directory is the \"extensions\" directory under",
        "segment_38": "\"path_to_autogen\".",
        "segment_39": "use_docker (Optional, list, str or bool): The docker image to use for code execution.",
        "segment_40": "Default is True, which means the code will be executed in a docker container. A default list of images will be used.",
        "segment_41": "If a list or a str of image name(s) is provided, the code will be executed in a docker container",
        "segment_42": "with the first image successfully pulled.",
        "segment_43": "If False, the code will be executed in the current environment.",
        "segment_44": "We strongly recommend using docker for code execution.",
        "segment_45": "timeout (Optional, int): The maximum execution time in seconds.",
        "segment_46": "last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.",
        "segment_49": "default_auto_reply str or dict or None - the default auto reply message when no code execution or llm based reply is generated.",
        "segment_50": "llm_config dict or False - llm inference configuration.",
        "segment_51": "Please refer to OpenAIWrapper.create",
        "segment_52": "for available options.",
        "segment_53": "Default to false, which disables llm-based auto reply.",
        "segment_54": "system_message str or List - system message for ChatCompletion inference.",
        "segment_55": "Only used when llm_config is not False. Use it to reprogram the agent.",
        "segment_56": "description str - a short description of the agent. This description is used by other agents",
        "segment_57": "(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)"
    },
    {
        "segment_1": "class CacheFactory()",
        "segment_2": "cache_factory\u200b",
        "segment_3": "@staticmethoddef cache_factory(seed, redis_url=None, cache_path_root=\".cache\")",
        "segment_4": "Factory function for creating cache instances.",
        "segment_5": "Based on the provided redis_url, this function decides whether to create a RedisCache",
        "segment_6": "or DiskCache instance. If RedisCache is available and redis_url is provided,",
        "segment_7": "a RedisCache instance is created. Otherwise, a DiskCache instance is used.",
        "segment_8": "Arguments:",
        "segment_10": "seed str - A string used as a seed or namespace for the cache.",
        "segment_11": "This could be useful for creating distinct cache instances",
        "segment_12": "or for namespacing keys in the cache.",
        "segment_13": "redis_url str or None - The URL for the Redis server. If this is None",
        "segment_14": "or if RedisCache is not available, a DiskCache instance is created.",
        "segment_16": "Returns:",
        "segment_17": "An instance of either RedisCache or DiskCache, depending on the availability of RedisCache",
        "segment_18": "and the provided redis_url.",
        "segment_19": "Examples:",
        "segment_20": "Creating a Redis cache",
        "segment_21": "> redis_cache = cache_factory(\"myseed\", \"redis://localhost:6379/0\")",
        "segment_22": "Creating a Disk cache",
        "segment_23": "> disk_cache = cache_factory(\"myseed\", None)Edit this pagePreviouscacheNextdisk_cacheCacheFactory ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def solve_problem(problem: str, **config) -> str",
        "segment_2": "(openai<1) Solve the math problem.",
        "segment_3": "Arguments:",
        "segment_5": "problem str - The problem statement.",
        "segment_6": "config Optional, dict - The configuration for the API call.",
        "segment_8": "Returns:",
        "segment_10": "str - The solution to the problem.",
        "segment_12": "remove_boxed\u200b",
        "segment_13": "def remove_boxed(string: str) -> Optional[str]",
        "segment_14": "Source: https://github.com/hendrycks/math",
        "segment_15": "Extract the text within a \\boxed{...} environment.",
        "segment_16": "Example:",
        "segment_17": "> remove_boxed(\"\\boxed{\\frac{2}{3}}\")",
        "segment_18": "\\frac{2}{3}",
        "segment_19": "last_boxed_only_string\u200b",
        "segment_20": "def last_boxed_only_string(string: str) -> Optional[str]",
        "segment_21": "Source: https://github.com/hendrycks/math",
        "segment_22": "Extract the last \\boxed{...} or \\fbox{...} element from a string.",
        "segment_23": "is_equiv\u200b",
        "segment_24": "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float",
        "segment_25": "Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in",
        "segment_27": "units",
        "segment_28": "fractions",
        "segment_29": "square roots",
        "segment_30": "superfluous LaTeX.",
        "segment_31": "Source: https://github.com/hendrycks/math",
        "segment_33": "is_equiv_chain_of_thought\u200b",
        "segment_34": "def is_equiv_chain_of_thought(str1: str, str2: str) -> float",
        "segment_35": "Strips the solution first before calling is_equiv.",
        "segment_36": "eval_math_responses\u200b",
        "segment_37": "def eval_math_responses(responses, solution=None, **args)",
        "segment_38": "Select a response for a math problem using voting, and check if the response is correct if the solution is provided.",
        "segment_39": "Arguments:",
        "segment_41": "responses list - The list of responses.",
        "segment_42": "solution str - The canonical solution.",
        "segment_44": "Returns:",
        "segment_46": "dict - The success metrics.",
        "segment_47": "Edit this pagePreviousgraph_utilsNextretrieve_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class CompressibleAgent(ConversableAgent)",
        "segment_2": "(Experimental) CompressibleAgent agent. While this agent retains all the default functionalities of the AssistantAgent,",
        "segment_3": "it also provides the added feature of compression when activated through the compress_config setting.",
        "segment_4": "compress_config is set to False by default, making this agent equivalent to the AssistantAgent.",
        "segment_5": "This agent does not work well in a GroupChat: The compressed messages will not be sent to all the agents in the group.",
        "segment_6": "The default system message is the same as AssistantAgent.",
        "segment_7": "human_input_mode is default to \"NEVER\"",
        "segment_8": "and code_execution_config is default to False.",
        "segment_9": "This agent doesn't execute code or function call by default.",
        "segment_10": "__init__\u200b",
        "segment_11": "def __init__(name: str, system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"NEVER\", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Optional[Union[Dict, bool]] = False, llm_config: Optional[Union[Dict, bool]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = \"\", compress_config: Optional[Dict] = False, description: Optional[str] = None, **kwargs)",
        "segment_12": "Arguments:",
        "segment_14": "name str - agent name.",
        "segment_15": "system_message str - system message for the ChatCompletion inference.",
        "segment_16": "Please override this attribute if you want to reprogram the agent.",
        "segment_17": "llm_config dict - llm inference configuration.",
        "segment_18": "Please refer to OpenAIWrapper.create",
        "segment_19": "for available options.",
        "segment_20": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_21": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_22": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_23": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_24": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_25": "The limit only plays a role when human_input_mode is not \"ALWAYS\".",
        "segment_26": "compress_config dict or True/False - config for compression before oai_reply. Default to False.",
        "segment_27": "You should contain the following keys:",
        "segment_29": "\"mode\" (Optional, str, default to \"TERMINATE\"): Choose from [\"COMPRESS\", \"TERMINATE\", \"CUSTOMIZED\"].",
        "segment_32": "\"TERMINATE\" - terminate the conversation ONLY when token count exceeds the max limit of current model.",
        "segment_33": "trigger_count is NOT used in this mode.",
        "segment_34": "\"COMPRESS\" - compress the messages when the token count exceeds the limit.",
        "segment_35": "\"CUSTOMIZED\" - pass in a customized function to compress the messages.",
        "segment_37": "\"compress_function\" (Optional, callable, default to None): Must be provided when mode is \"CUSTOMIZED\".",
        "segment_38": "The function should takes a list of messages and returns a tuple of (is_compress_success: bool, compressed_messages: List[Dict]).",
        "segment_39": "\"trigger_count\" (Optional, float, int, default to 0.7): the threshold to trigger compression.",
        "segment_40": "If a float between (0, 1], it is the percentage of token used. if a int, it is the number of tokens used.",
        "segment_41": "\"async\" (Optional, bool, default to False): whether to compress asynchronously.",
        "segment_42": "\"broadcast\" (Optional, bool, default to True): whether to update the compressed message history to sender.",
        "segment_43": "\"verbose\" (Optional, bool, default to False): Whether to print the content before and after compression. Used when mode=\"COMPRESS\".",
        "segment_44": "\"leave_last_n\" (Optional, int, default to 0): If provided, the last n messages will not be compressed. Used when mode=\"COMPRESS\".",
        "segment_47": "system_message0 str - a short description of the agent. This description is used by other agents",
        "segment_48": "(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)",
        "segment_49": "system_message1 dict - Please refer to other kwargs in",
        "segment_50": "ConversableAgent.",
        "segment_52": "generate_reply\u200b",
        "segment_53": "def generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_54": "Adding to line 202:",
        "segment_55": "if messages is not None and messages != self._oai_messages[sender]: messages = self._oai_messages[sender]",
        "segment_56": "on_oai_token_limit\u200b",
        "segment_57": "def on_oai_token_limit( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_58": "(Experimental) Compress previous messages when a threshold of tokens is reached.",
        "segment_59": "TODO: async compress",
        "segment_60": "TODO: maintain a list for old oai messages (messages before compression)",
        "segment_61": "compress_messages\u200b",
        "segment_62": "def compress_messages( messages: Optional[List[Dict]] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None, List]]",
        "segment_63": "Compress a list of messages into one message.",
        "segment_64": "The first message (the initial prompt) will not be compressed.",
        "segment_65": "The rest of the messages will be compressed into one message, the model is asked to distinguish the role of each message: USER, ASSISTANT, FUNCTION_CALL, FUNCTION_RETURN.",
        "segment_66": "Check out the compress_sys_msg.",
        "segment_67": "TODO: model used in compression agent is different from assistant agent: For example, if original model used by is gpt-4; we start compressing at 70% of usage, 70% of 8092 = 5664; and we use gpt 3.5 here max_toke = 4096, it will raise error. choosinng model automatically?Edit this pagePreviousagent_builderNextgpt_assistant_agentCompressibleAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityNewer PostAgent AutoBuild - Automatically Building Multi-agent SystemsOlder PostAutoGen Meets GPTsIntroductionAgentEval FrameworkAgentEval Results based on Math Problems DatasetLimitations and Future WorkSummaryPrevious ResearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_2": "Introducing AgentOptimizer, a new class for training LLM agents in the era of LLMs as a service.",
        "segment_3": "AgentOptimizer is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.",
        "segment_4": "Checkout one implementation for AgentOptimizer on MATH dataset",
        "segment_5": "here.",
        "segment_6": "Paper is coming soon!",
        "segment_7": "Introduction\u200b",
        "segment_8": "In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?",
        "segment_9": "Here, we take an initial step towards the agent training.",
        "segment_10": "Inspired by the function calling capabilities provided by OpenAI,",
        "segment_11": "we draw an analogy between model weights and agent functions/skills, and update an agent\u2019s functions/skills based on its historical performance on a training set.",
        "segment_12": "Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents\u2019 functions as a set of function calls,",
        "segment_13": "to support iteratively adding, revising, and removing existing functions.",
        "segment_14": "As an agentic way of training an agent, our approach helps enhance the agents\u2019 abilities without requiring access to the LLMs weights.",
        "segment_15": "AgentOptimizer\u200b",
        "segment_16": "AgentOptimizer is a class designed to optimize the agents by improving their function calls.",
        "segment_17": "It contains two core methods:",
        "segment_19": "step(): step() takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).",
        "segment_21": "actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)",
        "segment_22": "It has two outputs actions and updated_functions. actions is a series of actions to manipulate the current functions. And updated_functions is the updated functions after the actions are applied (including code implementation).",
        "segment_24": "update_function_call():",
        "segment_25": "This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from step().",
        "segment_26": "For AssistantAgent, it first uses update_function_signature to update the function signatures.",
        "segment_27": "Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from step().",
        "segment_29": "Sometimes, the function signatures (JSON schema) returned by the step() may not be valid, and the generated code may also face syntax errors.",
        "segment_30": "AgentOptimizer includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.",
        "segment_31": "Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.",
        "segment_32": "Pseudocode for the optimization process\u200b",
        "segment_33": "The optimization process is as follows:",
        "segment_34": "for - in range(EPOCH): history, statistic, current_functions = solve_problems(train_problems) actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions) AgentOptimizer.update_function_call(actions)",
        "segment_35": "Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.",
        "segment_36": "The functions are then improved using AgentOptimizer.",
        "segment_37": "Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.",
        "segment_38": "After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks",
        "segment_39": "The implementation technology behind the AgentOptimizer\u200b",
        "segment_40": "To obtain stable and structured function signatures and code implementations from AgentOptimizer,",
        "segment_41": "we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.",
        "segment_42": "Specifically, we introduce three function calls to manipulate the current functions at each step: add_function, remove_function, and revise_function.",
        "segment_43": "These calls add, remove, and revise functions in the existing function list, respectively.",
        "segment_44": "This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.",
        "segment_45": "Below is the JSON schema of these function calls:",
        "segment_47": "add_function: Add one new function that may be used in the future tasks.",
        "segment_49": "ADD_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"add_function\", \"description\": \"Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_51": "revise_function: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.",
        "segment_53": "REVISE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"revise_function\", \"description\": \"Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_55": "remove_function: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.",
        "segment_57": "REMOVE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"remove_function\", \"description\": \"Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" } }, \"required\": [\"name\"] } }}",
        "segment_58": "Limitation & Future work\u200b",
        "segment_60": "Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.",
        "segment_61": "When the training epoch is small, the agent\u2019s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.",
        "segment_62": "The Current implementation of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.",
        "segment_63": "It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.",
        "segment_64": "Currently, it only supports optimizing the multi-agent system in solving problems from MATH dataset. We will integrate it into autogen with more general interface in the future.",
        "segment_65": "Tags:LLMresearchAgent AutoBuild - Automatically Building Multi-agent SystemsNovember 26, 2023 \u00b7 7 min readLinxin SongMS student at Waseda UniversityJieyu ZhangPhD student at University of Washington",
        "segment_66": "TL;DR:",
        "segment_67": "Introducing AutoBuild, building multi-agent system automatically, fast, and easily for complex tasks with minimal",
        "segment_68": "user prompt required, powered by a new designed class AgentBuilder. AgentBuilder also supports open-source LLMs by",
        "segment_69": "leveraging vLLM and FastChat.",
        "segment_70": "Checkout example notebooks and source code for reference:",
        "segment_72": "AutoBuild Examples",
        "segment_73": "AgentBuilder",
        "segment_75": "Introduction\u200b",
        "segment_76": "In this blog, we introduce AutoBuild, a pipeline that can automatically build multi-agent systems for complex tasks.",
        "segment_77": "Specifically, we design a new class called AgentBuilder, which will complete the generation of participant expert agents",
        "segment_78": "and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.",
        "segment_79": "AgentBuilder supports open-source models on Hugging Face powered by vLLM",
        "segment_80": "and FastChat. Once the user chooses to use open-source LLM, AgentBuilder will set",
        "segment_81": "up an endpoint server automatically without any user participation.",
        "segment_82": "Installation\u200b",
        "segment_84": "AutoGen:",
        "segment_86": "pip install pyautogen[autobuild]",
        "segment_88": "(Optional: if you want to use open-source LLMs) vLLM and FastChat",
        "segment_90": "pip install vllm fastchat",
        "segment_91": "Basic Example\u200b",
        "segment_92": "In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.",
        "segment_93": "Step 1: prepare configurations\u200b",
        "segment_94": "First, we need to prepare the Agent configurations.",
        "segment_95": "Specifically, a config path containing the model name and API key, and a default config for each agent, are required.",
        "segment_96": "config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST' # modify pathdefault_llm_config = { 'temperature': 0}",
        "segment_97": "Step 2: create an AgentBuilder instance\u200b",
        "segment_98": "Then, we create an AgentBuilder instance with the config path and default config.",
        "segment_99": "You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.",
        "segment_100": "from autogen.agentchat.contrib.agent_builder import AgentBuilderbuilder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')",
        "segment_101": "Step 3: specify the building task\u200b",
        "segment_102": "Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.",
        "segment_103": "Note that your building task should have a general description of the task. Adding some specific examples is better.",
        "segment_104": "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"",
        "segment_105": "Step 4: build group chat agents\u200b",
        "segment_106": "Use build() to let the build manager (with a builder_model as backbone) complete the group chat agents generation.",
        "segment_107": "If you think coding is necessary for your task, you can use coding=True to add a user proxy (a local code interpreter) into the agent list as:",
        "segment_108": "agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)",
        "segment_109": "If coding is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.",
        "segment_110": "The generated agent_list is a list of AssistantAgent instances.",
        "segment_111": "If coding is true, a user proxy (a UserProxyAssistant instance) will be added as the first element to the agent_list.",
        "segment_112": "agent_configs is a list of agent configurations including agent name, backbone LLM model, and system message.",
        "segment_113": "For example",
        "segment_114": "// an example of agent_configs. AgentBuilder will generate agents with the following configurations.[ { \"name\": \"ArXiv_Data_Scraper_Developer\", \"model\": \"gpt-4-1106-preview\", \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\", \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\" }, ...]",
        "segment_115": "Step 5: execute the task\u200b",
        "segment_116": "Let agents generated in build() complete the task collaboratively in a group chat.",
        "segment_117": "import autogendef start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\"]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task)start_task( execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\", agent_list=agent_list, llm_config=default_llm_config)",
        "segment_118": "Step 6 (Optional): clear all agents and prepare for the next task\u200b",
        "segment_119": "You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.",
        "segment_120": "builder.clear_all_agents(recycle_endpoint=True)",
        "segment_121": "If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.",
        "segment_122": "If necessary, you can use recycle_endpoint=False to retain the previous open-source LLM's endpoint server.",
        "segment_123": "Save and Load\u200b",
        "segment_124": "You can save all necessary information of the built group chat agents by",
        "segment_125": "saved_path = builder.save()",
        "segment_126": "Configurations will be saved in JSON format with the following content:",
        "segment_127": "// FILENAME: save_config_TASK_MD5.json{ \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"agent_configs\": [ { \"name\": \"...\", \"model\": \"...\", \"system_message\": \"...\", \"description\": \"...\" }, ... ], \"manager_system_message\": \"...\", \"code_execution_config\": {...}, \"default_llm_config\": {...}}",
        "segment_128": "You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename save_config_TASK_MD5.json.",
        "segment_129": "You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.",
        "segment_130": "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)agent_list, agent_config = new_builder.load(saved_path)start_task(...) # skip build()",
        "segment_131": "Use OpenAI Assistant\u200b",
        "segment_132": "Assistants API allows you to build AI assistants within your own applications.",
        "segment_133": "An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.",
        "segment_134": "AutoBuild also supports the assistant API by adding use_oai_assistant=True to build().",
        "segment_135": "# Transfer to the OpenAI Assistant API.agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)...",
        "segment_136": "(Experimental) Use Open-source LLM\u200b",
        "segment_137": "AutoBuild supports open-source LLM by vLLM and FastChat.",
        "segment_138": "Check the supported model list here.",
        "segment_139": "After satisfying the requirements, you can add an open-source LLM's huggingface repository to the config file,",
        "segment_140": "// Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.[ ... { \"model\": \"meta-llama/Llama-2-13b-chat-hf\", \"api_key\": \"EMPTY\" }]",
        "segment_141": "and specify it when initializing AgentBuilder.",
        "segment_142": "AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.",
        "segment_143": "Future work/Roadmap\u200b",
        "segment_145": "Let the builder select the best agents from a given library/database to solve the task.",
        "segment_147": "Summary\u200b",
        "segment_148": "We propose AutoBuild with a new class AgentBuilder.",
        "segment_149": "AutoBuild can help user solve their complex task with an automatically built multi-agent system.",
        "segment_150": "AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.",
        "segment_151": "More advanced features are coming soon.Tags:LLMresearchHow to Assess Utility of LLM-powered Applications?November 20, 2023 \u00b7 10 min readJulia KiselevaSenior Researcher at Microsoft ResearchNegar ArabzadehPhD student at the University of Waterloo",
        "segment_152": "Fig.1 illustrates the general flow of AgentEval",
        "segment_153": "TL;DR:",
        "segment_155": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_156": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_157": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_159": "Introduction\u200b",
        "segment_160": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_161": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_163": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_164": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_166": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_167": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_169": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_170": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_174": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_175": "AgentEval Framework\u200b",
        "segment_176": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_178": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_180": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_181": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_183": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_185": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_186": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_187": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_188": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_189": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_191": "AgentChat",
        "segment_192": "ReAct",
        "segment_193": "GPT-4 Vanilla Solver",
        "segment_195": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_197": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_198": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_199": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_200": "Limitations and Future Work\u200b",
        "segment_201": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_203": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_204": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_206": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_207": "Summary\u200b",
        "segment_208": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_209": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_210": "Previous Research\u200b",
        "segment_211": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_212": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityEcoAssistant - Using LLM Assistants More Accurately and AffordablyNovember 9, 2023 \u00b7 5 min readJieyu ZhangPhD student at University of Washington",
        "segment_213": "TL;DR:",
        "segment_215": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_216": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_217": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_218": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_220": "EcoAssistant\u200b",
        "segment_221": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_222": "Problem setup\u200b",
        "segment_223": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_224": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_225": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_226": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_227": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_228": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_229": "Leveraging external APIs\u200b",
        "segment_230": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_231": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_232": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_233": "A visualization of the two-agent system is shown below.",
        "segment_235": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_236": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_238": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_239": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_240": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_241": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_242": "Solution Demonstration\u200b",
        "segment_243": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_244": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_245": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_246": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_247": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_249": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_250": "Assistant Hierarchy\u200b",
        "segment_251": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_252": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_253": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_254": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_255": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_256": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_257": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_258": "A Synergistic Effect\u200b",
        "segment_259": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_260": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_261": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_262": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_263": "Experimental Results\u200b",
        "segment_264": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_265": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_267": "Further reading\u200b",
        "segment_268": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_269": "If you find this blog useful, please consider citing:",
        "segment_270": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessAutoGen's Teachable AgentsOctober 26, 2023 \u00b7 17 min readRicky LoyndSenior Research Engineer at Microsoft",
        "segment_271": "TL;DR:",
        "segment_273": "We introduce Teachable Agents so that users can teach their LLM-based assistants new facts, preferences, and skills.",
        "segment_274": "We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.",
        "segment_276": "Introduction\u200b",
        "segment_277": "Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.",
        "segment_278": "Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.",
        "segment_279": "Any instantiated agent that inherits from ConversableAgent can be made teachable by instantiating a Teachability object and calling its add_to_agent(agent) method.",
        "segment_280": "In order to make effective decisions about memo storage and retrieval, the Teachability object calls an instance of TextAnalyzerAgent (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.",
        "segment_281": "Run It Yourself\u200b",
        "segment_282": "AutoGen contains four code examples that use Teachability.",
        "segment_285": "Run chat_with_teachable_agent.py to converse with a teachable agent.",
        "segment_288": "Run test_teachable_agent.py for quick unit testing of a teachable agent.",
        "segment_291": "Use the Jupyter notebook agentchat_teachability.ipynb to step through examples discussed below.",
        "segment_294": "Use the Jupyter notebook agentchat_teachable_oai_assistants.ipynb to make arbitrary OpenAI Assistants teachable through GPTAssistantAgent.",
        "segment_297": "Basic Usage of Teachability\u200b",
        "segment_299": "Install dependencies",
        "segment_301": "Please install pyautogen with the [teachable] option before using Teachability.",
        "segment_302": "pip install \"pyautogen[teachable]\"",
        "segment_304": "Import agents",
        "segment_306": "from autogen import UserProxyAgent, config_list_from_jsonfrom autogen.agentchat.contrib.capabilities.teachability import Teachabilityfrom autogen import ConversableAgent # As an example",
        "segment_308": "Create llm_config",
        "segment_310": "# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_samplefilter_dict = {\"model\": [\"gpt-4\"]} # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", filter_dict=filter_dict)llm_config={\"config_list\": config_list, \"timeout\": 120}",
        "segment_312": "Create the agents",
        "segment_314": "# Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.teachable_agent = ConversableAgent( name=\"teachable_agent\", # The name can be anything. llm_config=llm_config)# Instantiate a Teachability object. Its parameters are all optional.teachability = Teachability( reset_db=False, # Use True to force-reset the memo DB, and False to use an existing DB. path_to_db_dir=\"./tmp/interactive/teachability_db\" # Can be any path, but teachable agents in a group chat require unique paths.)# Now add teachability to the agent.teachability.add_to_agent(teachable_agent)# For this test, create a user proxy agent as usual.user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")",
        "segment_316": "Chat with the teachable agent",
        "segment_318": "# This function will return once the user types 'exit'.teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")",
        "segment_319": "Example 1 - Learning user info\u200b",
        "segment_320": "A user can teach the agent facts about themselves.",
        "segment_321": "(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)",
        "segment_322": "Loading previous memory (if any) from disk.teachable_agent (to user):Greetings, I'm a teachable user assistant! What's on your mind today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Rickyuser (to teachable_agent):My name is Ricky-------------------------------------------------------------------------------->>>>>>>> USING AUTO REPLY...teachable_agent (to user):Hello Ricky! How can I assist you today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit",
        "segment_323": "In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)",
        "segment_324": "--------------------------------------------------------------------------------user (to teachable_agent):what's my name?--------------------------------------------------------------------------------teachable_agent (to user):Hi Ricky! How can I help you today?--------------------------------------------------------------------------------",
        "segment_325": "Example 2 - Learning new facts\u200b",
        "segment_326": "A user can teach the agent more complex, related facts.",
        "segment_327": "--------------------------------------------------------------------------------user (to teachable_agent):What is the Vicuna model?--------------------------------------------------------------------------------teachable_agent (to user):I'm sorry, but I don't have information about the \"Vicuna model\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?--------------------------------------------------------------------------------user (to teachable_agent):Vicuna is a 13B-parameter language model released by Meta.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.--------------------------------------------------------------------------------user (to teachable_agent):What is the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.--------------------------------------------------------------------------------user (to teachable_agent):Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.",
        "segment_328": "Then in a later chat the teachable agent can answer questions about the facts it has been taught.",
        "segment_329": "(Remember to first close the previous chat by typing 'exit'.)",
        "segment_330": "--------------------------------------------------------------------------------user (to teachable_agent):How does the Vicuna model compare to the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.--------------------------------------------------------------------------------",
        "segment_331": "Example 3 - Learning user preferences\u200b",
        "segment_332": "A user can teach the agent how they prefer to have things done.",
        "segment_333": "Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.",
        "segment_334": "Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.",
        "segment_335": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.--------------------------------------------------------------------------------Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points: the title, the innovation, and the key empirical results.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.--------------------------------------------------------------------------------",
        "segment_336": "Then in later chats the teacher doesn't need to reiterate their detailed preferences.",
        "segment_337": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.Sparks of Artificial General Intelligence: Early experiments with GPT-4S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi ZhangArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.--------------------------------------------------------------------------------teachable_agent (to user):- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.--------------------------------------------------------------------------------",
        "segment_338": "Example 4 - Learning new skills\u200b",
        "segment_339": "Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.",
        "segment_340": "The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.",
        "segment_341": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------Step 1: Identify the current equation and the target value on the right-hand side.Current equation: 9 * 4 + 6 * 6 = 72Target value: 99Step 2: Determine what numbers can be changed on the left-hand side.Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.9 -> If changed, it would create a larger product.4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.Step 4: Write down the solution.The solution is: 9 * 4 + 6 * 7--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".Here's some advice that may help:1. Let E denote the original number on the right.2. Let F denote the final number on the right.3. Calculate the difference between the two, G = F - E.4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.5. Calculate J = G / H. This is the number of times that H divides into G.6. Verify that J is an integer, and that H * J = G.7. Find the number on the left which is multiplied by H, and call it K.8. Change K to K + J.9. Recompute the value on the left, and verify that it equals F.Finally, write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Step 1: Identify the original number (E) and the final number (F) on the right-hand side.E = 72F = 99Step 2: Calculate the difference between the two, G = F - E.G = 99 - 72 = 27Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.9 (divides evenly into 27)4 (does not)6 (first occurrence, does not)6 (second occurrence, does not)Step 4: Calculate J = G / H. This is the number of times that H divides into G.H = 9 (since 9 divides evenly into 27)J = 27 / 9 = 3Step 5: Verify that J is an integer, and that H * J = G.H * J = 9 * 3 = 27J = 3 (which is an integer)Step 6: Find the number on the left which is multiplied by H, and call it K.Number on the leftside of the equation that is multiplied by 9: 4Therefore, K = 4Step 7: Change K to K + J.K + J = 4 + 3 = 7Step 8: Recompute the value on the left, and verify that it equals F.9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)Step 9: Write down the solution.The solution is: 9 * 7 + 6 * 6--------------------------------------------------------------------------------",
        "segment_342": "In a later chat the user doesn't need to repeat the detailed advice.",
        "segment_343": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:8 * 3 + 7 * 9 = 87Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:1. Let E denote the original number on the right: E = 87.2. Let F denote the final number on the right: F = 59.3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).5. Calculate J = G / H: J = -28 / 7 = -4.6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.8. Change K to K + J: K_new = 9 + (-4) = 5.9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.The solution is: 8 * 3 + 7 * 5--------------------------------------------------------------------------------",
        "segment_344": "Planned improvements\u200b",
        "segment_346": "Understanding user instructions distributed over multiple turns.",
        "segment_347": "Learning from the agent's own experience, to reduce dependence on explicit user teachings.",
        "segment_348": "Learning skills built on top of previously learned skills.",
        "segment_350": "Conclusion\u200b",
        "segment_351": "Teachability is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our Discord channel. We look forward to seeing how you and the rest of the community can use and improve teachable agents in AutoGen!Tags:LLMteachRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_352": "TL;DR:",
        "segment_354": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_355": "allows retrieval-augmented generation, and its basic usage.",
        "segment_356": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_357": "split function and vector database.",
        "segment_358": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_359": "application with Gradio.",
        "segment_361": "Introduction\u200b",
        "segment_362": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_363": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_364": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_365": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_366": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_367": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_368": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_369": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_370": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_371": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_372": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_373": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_375": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_376": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_377": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_378": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_379": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_380": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_381": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_382": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_383": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_384": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_385": "Retrieval-Augmented Assistant.",
        "segment_386": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_387": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_388": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_389": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_390": "The conversation terminates if no more documents are available for the context.",
        "segment_392": "Basic Usage of RAG Agents\u200b",
        "segment_394": "Install dependencies",
        "segment_396": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_397": "pip install \"pyautogen[retrievechat]\"",
        "segment_398": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_399": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_400": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_401": "If you install unstructured",
        "segment_402": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_403": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_404": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_406": "Import Agents",
        "segment_408": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_410": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_412": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_414": "Initialize Chat and ask a question",
        "segment_416": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_417": "Output is like:",
        "segment_418": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_420": "Create a UserProxyAgent and ask the same question",
        "segment_422": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_423": "Output is like:",
        "segment_424": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_425": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_426": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_427": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_428": "Customizing RAG Agents\u200b",
        "segment_429": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_430": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_431": "function and vector database.",
        "segment_432": "Customizing Embedding Function\u200b",
        "segment_433": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_434": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_436": "OpenAI",
        "segment_438": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_440": "HuggingFace",
        "segment_442": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_443": "More examples can be found here.",
        "segment_444": "Customizing Text Split Function\u200b",
        "segment_445": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_446": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_447": "There are also some existing text split tools which are good to reuse.",
        "segment_448": "For example, you can use all the text splitters in langchain.",
        "segment_449": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_450": "Customizing Vector Database\u200b",
        "segment_451": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_452": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_453": "For example, you can use Qdrant as below:",
        "segment_454": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_455": "Advanced Usage of RAG Agents\u200b",
        "segment_456": "Integrate with other agents in a group chat\u200b",
        "segment_457": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_458": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_459": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_460": "you'll need to call it from a function.",
        "segment_461": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_462": "Build a Chat application with Gradio\u200b",
        "segment_463": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_465": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_466": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_467": "Read More\u200b",
        "segment_468": "You can check out more example notebooks for RAG use cases:",
        "segment_470": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_471": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_472": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_473": "Tags:LLMRAGUse AutoGen for Local LLMsJuly 14, 2023 \u00b7 3 min readJiale LiuUndergraduate student at Xidian UniversityTL;DR:",
        "segment_474": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_475": "Preparations\u200b",
        "segment_476": "Clone FastChat\u200b",
        "segment_477": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_478": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_479": "Download checkpoint\u200b",
        "segment_480": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_481": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_482": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_483": "Initiate server\u200b",
        "segment_484": "First, launch the controller",
        "segment_485": "python -m fastchat.serve.controller",
        "segment_486": "Then, launch the model worker(s)",
        "segment_487": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_488": "Finally, launch the RESTful API server",
        "segment_489": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_490": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_491": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_492": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_493": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_494": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_495": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_496": "interacting with multiple local LLMs\u200b",
        "segment_497": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_498": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_499": "The inference code would be:",
        "segment_500": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_501": "For Further Reading\u200b",
        "segment_503": "Documentation about autogen.",
        "segment_504": "Documentation about FastChat.",
        "segment_505": "Tags:LLMMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_506": "TL;DR:",
        "segment_508": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_509": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_510": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_512": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_513": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_514": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_515": "The MathChat Framework\u200b",
        "segment_516": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_517": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_520": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_523": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_525": "Writing a Python program to solve the problem directly.",
        "segment_526": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_527": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_531": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_534": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_535": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_536": "(The conversation is modified for readability.)",
        "segment_537": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_538": "Experiment Setup\u200b",
        "segment_539": "We evaluate the improvement brought by MathChat.",
        "segment_540": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_541": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_542": "We also evaluate the following methods for comparison:",
        "segment_545": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_548": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_551": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_554": "Experiment Results\u200b",
        "segment_555": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_557": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_558": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_559": "The code for experiments can be found at this repository.",
        "segment_560": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_561": "Future Directions\u200b",
        "segment_562": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_563": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_565": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_566": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_567": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_569": "For Further Reading\u200b",
        "segment_571": "Research paper of MathChat",
        "segment_572": "Documentation about autogen",
        "segment_574": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_575": "TL;DR:",
        "segment_577": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_579": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_580": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_581": "Observations\u200b",
        "segment_583": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_584": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_586": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_587": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_588": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_589": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_590": "What else can we do? We notice that:",
        "segment_591": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_592": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_593": "Solution\u200b",
        "segment_594": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_596": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_597": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_600": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_601": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_603": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_604": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_605": "GPT-4, n=1, temperature=0",
        "segment_606": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_607": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_609": "Experiment Results\u200b",
        "segment_610": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_611": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_612": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_614": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_616": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_618": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_619": "Solved by GPT-4, n=1, temperature=0:",
        "segment_621": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_623": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_625": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_627": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_629": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_630": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_632": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_633": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_635": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_636": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_637": "Discussion\u200b",
        "segment_638": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_639": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_641": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_642": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_644": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_645": "Different configurations have complementary effect.",
        "segment_649": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_650": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_651": "There are many directions of extensions in research and development:",
        "segment_653": "Generalize the way to provide feedback.",
        "segment_654": "Automate the process of optimizing the configurations.",
        "segment_655": "Build adaptive agents for different applications.",
        "segment_657": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_658": "For Further Reading\u200b",
        "segment_660": "Documentation about autogen and Research paper.",
        "segment_661": "Blog post about a related study for math.",
        "segment_662": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_663": "TL;DR:",
        "segment_665": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_666": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_667": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_669": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_670": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_671": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_672": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_673": "Experiment Setup\u200b",
        "segment_674": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_676": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_677": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_679": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_681": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_682": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_683": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_684": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_685": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_687": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_688": "Experiment Results\u200b",
        "segment_689": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_690": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_691": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_693": "However, the selected model changes on level 4 Algebra.",
        "segment_695": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_696": "On level 5 the result is similar.",
        "segment_698": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_699": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_700": "Analysis and Discussion\u200b",
        "segment_701": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_702": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_703": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_704": "For Further Reading\u200b",
        "segment_706": "Research paper about the tuning technique",
        "segment_707": "Documentation about inference tuning",
        "segment_709": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class LLaVAAgent(MultimodalConversableAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name: str, system_message: Optional[Tuple[str, List]] = DEFAULT_LLAVA_SYS_MSG, *args, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - agent name.",
        "segment_7": "system_message str - system message for the ChatCompletion inference.",
        "segment_8": "Please override this attribute if you want to reprogram the agent.",
        "segment_9": "**kwargs dict - Please refer to other kwargs in",
        "segment_10": "ConversableAgent.",
        "segment_12": "llava_call\u200b",
        "segment_13": "def llava_call(prompt: str, llm_config: dict) -> str",
        "segment_14": "Makes a call to the LLaVA service to generate text based on a given promptEdit this pagePreviousimg_utilsNextmath_user_proxy_agentLLaVAAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.",
        "segment_3": "Main Features\u200b",
        "segment_5": "AutoGen enables building next-gen LLM applications based on multi-agent conversations with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.",
        "segment_6": "It supports diverse conversation patterns for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,",
        "segment_7": "the number of agents, and agent conversation topology.",
        "segment_8": "It provides a collection of working systems with different complexities. These systems span a wide range of applications from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.",
        "segment_9": "AutoGen provides enhanced LLM inference. It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.",
        "segment_11": "AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and University of Washington.",
        "segment_12": "Quickstart\u200b",
        "segment_13": "Install from pip: pip install pyautogen. Find more options in Installation.",
        "segment_14": "For code execution, we strongly recommend installing the python docker package, and using docker.",
        "segment_15": "Multi-Agent Conversation Framework\u200b",
        "segment_16": "Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.",
        "segment_17": "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example,",
        "segment_18": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommendeduser_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")# This initiates an automated chat between the two agents to solve the task",
        "segment_19": "The figure below shows an example conversation flow with AutoGen.",
        "segment_22": "Code examples.",
        "segment_23": "Documentation.",
        "segment_25": "Enhanced LLM Inferences\u200b",
        "segment_26": "Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets.",
        "segment_27": "# perform tuning for openai<1config, analysis = autogen.Completion.tune( data=tune_data, metric=\"success\", mode=\"max\", eval_func=eval_func, inference_budget=0.05, optimization_budget=3, num_samples=-1,)# perform inference for a test instanceresponse = autogen.Completion.create(context=test_instance, **config)",
        "segment_29": "Code examples.",
        "segment_30": "Documentation.",
        "segment_32": "Where to Go Next ?\u200b",
        "segment_34": "Understand the use cases for multi-agent conversation and enhanced LLM inference.",
        "segment_35": "Find code examples.",
        "segment_36": "Read SDK.",
        "segment_37": "Learn about research around AutoGen.",
        "segment_38": "Roadmap",
        "segment_39": "Chat on Discord.",
        "segment_40": "Follow on Twitter.",
        "segment_42": "If you like our project, please give it a star on GitHub. If you are interested in contributing, please read Contributor's Guide.",
        "segment_43": "Edit this pageNextInstallationMain FeaturesQuickstartWhere to Go Next ?CommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_4": "allows retrieval-augmented generation, and its basic usage.",
        "segment_5": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_6": "split function and vector database.",
        "segment_7": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_8": "application with Gradio.",
        "segment_10": "Introduction\u200b",
        "segment_11": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_12": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_13": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_14": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_15": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_16": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_17": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_18": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_19": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_20": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_21": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_22": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_24": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_25": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_26": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_27": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_28": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_29": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_30": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_31": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_32": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_33": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_34": "Retrieval-Augmented Assistant.",
        "segment_35": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_36": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_37": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_38": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_39": "The conversation terminates if no more documents are available for the context.",
        "segment_41": "Basic Usage of RAG Agents\u200b",
        "segment_43": "Install dependencies",
        "segment_45": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_46": "pip install \"pyautogen[retrievechat]\"",
        "segment_47": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_48": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_49": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_50": "If you install unstructured",
        "segment_51": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_52": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_53": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_55": "Import Agents",
        "segment_57": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_59": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_61": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_63": "Initialize Chat and ask a question",
        "segment_65": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_66": "Output is like:",
        "segment_67": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_69": "Create a UserProxyAgent and ask the same question",
        "segment_71": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_72": "Output is like:",
        "segment_73": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_74": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_75": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_76": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_77": "Customizing RAG Agents\u200b",
        "segment_78": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_79": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_80": "function and vector database.",
        "segment_81": "Customizing Embedding Function\u200b",
        "segment_82": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_83": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_85": "OpenAI",
        "segment_87": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_89": "HuggingFace",
        "segment_91": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_92": "More examples can be found here.",
        "segment_93": "Customizing Text Split Function\u200b",
        "segment_94": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_95": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_96": "There are also some existing text split tools which are good to reuse.",
        "segment_97": "For example, you can use all the text splitters in langchain.",
        "segment_98": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_99": "Customizing Vector Database\u200b",
        "segment_100": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_101": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_102": "For example, you can use Qdrant as below:",
        "segment_103": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_104": "Advanced Usage of RAG Agents\u200b",
        "segment_105": "Integrate with other agents in a group chat\u200b",
        "segment_106": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_107": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_108": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_109": "you'll need to call it from a function.",
        "segment_110": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_111": "Build a Chat application with Gradio\u200b",
        "segment_112": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_114": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_115": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_116": "Read More\u200b",
        "segment_117": "You can check out more example notebooks for RAG use cases:",
        "segment_119": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_120": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_121": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_122": "Tags:LLMRAGNewer PostAutoGen's Teachable AgentsOlder PostUse AutoGen for Local LLMsIntroductionBasic Usage of RAG AgentsCustomizing RAG AgentsCustomizing Embedding FunctionCustomizing Text Split FunctionCustomizing Vector DatabaseAdvanced Usage of RAG AgentsIntegrate with other agents in a group chatBuild a Chat application with GradioRead MoreCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "This page contains a list of demos that use AutoGen in various applications from the community.",
        "segment_2": "Contribution guide:",
        "segment_3": "Built something interesting with AutoGen? Submit a PR to add it to the list! See the Contribution Guide below for more details.",
        "segment_4": "Filter by tagsAutoGen Group Chat Playgroundlg...A huggingface space to explore AutoGen group chat, build agents with zero-code, and access source code for reuse.lg...uiA Stateful Dev Environment Powered by Jupyter Notebooklg...An AutoGen Teams Powered by Jupyter notebook.lg...extensiontoolsSolving Security Vulnerabilities with AutoGenlg...An article discussing the use of AutoGen to tackle security vulnerabilities.lg...appResearch Agents via AutoGenlg...A guide to building a team of AI research agents using AutoGen.lg...groupchattoolsAutoGen with Ollama and LiteLLMlg...A demonstration of integrating Ollama, LiteLLM, and AutoGen.lg...extensionAutoGen Engineerlg...Join the AutoGen Engineer group chat to collaborate and build with others.lg...groupchatappAutoGen with Obsidianlg...Learn how to integrate AutoGen with Obsidian for note-taking and management.lg...toolsappAutoGen Builder GPTlg...A platform for building conversational AI agents with AutoGen Builder GPT.lg...groupchatuiAutoGen Multi-Round Human Interaction Chatbot with Gradio 4.0lg...Experience a multi-round human interaction chatbot built with AutoGen and Gradio 4.0.lg...uiappAgentcloud.dev (UI for AutoGen)lg...Agentcloud.dev provides a user interface for managing and collaborating with AutoGen agents.lg...uiNext.js + FASTAPI Based UI for AutoGenlg...A project featuring a UI for AutoGen built with Next.js and FastAPI.lg...uiFull Function UI for AutoGen Powered by Panellg...A UI allows users to directly interact with AI agents in real-time during a group chat scenariolg...uiAutoGen Monitoring and Observabilitylg...Documentation on monitoring and observability features for AutoGen.lg...extensionPostgres Data Analytics AI Agent with AutoGenlg...Utilizing AutoGen to speak directly to Postgres Database.lg...toolsappAutoGen with Local LLMslg...An article on deploying multiple AI agents using local LLMs with AutoGen.lg...extensionAutoGen with FastApi backend and React Frontendlg...A demonstration of using AutoGen with a FastAPI backend and React frontend.lg...uiTalk to AutoGen Agents Using Whisper and Gradiolg...Interact with AutoGen agents using Whisper and Gradio interfaces.lg...uiAutoGen + LangChain + ChromaDB = You Super AI Assistantlg...Create a super AI assistant combining AutoGen, LangChain, and ChromaDB.lg...appAutoGen + Flowise = Super AI Agents on No-Code Platformlg...Build super AI agents on a no-code platform using AutoGen and Flowise.lg...appAutoGen with RunPod and TextGen WebUIlg...Learn how to use AutoGen with RunPod and TextGen WebUI for enhanced AI agent integration.lg...uiextensionJarvis Collaborates with AutoGen for Tweet Analysislg...Explore how Jarvis collaborates with AutoGen for tweet analysis.lg...toolsappAutoGen + LangChain + PlayHT = Super AI Agent that Speakslg...Combine AutoGen, LangChain, and PlayHT to create a speaking super AI agent.lg...toolsappAutoGen Flow with FastAPI and Nextjslg...A development flow using AutoGen with FastAPI and Next.js.lg...uiBuild Vision-Enabled AI Agents with AutoGen + Llavalg...Tutorial on building vision-enabled AI agents using AutoGen and llava.lg...toolsappAutoGen + Chainlit chat interface with multi-agent conversationlg...Chainlit chat interface with multi-agent conversation between agents to complete a taskslg...uiXForce IDE: Build AutoGen based workforces from a drag and drop UIlg...X-Force IDE is a low-code, agent-as-a-service UI framework that lets you create AutoGen-based workforces from a drag-and-drop-like user interface.lg...ui",
        "segment_5": "Contributing\u200b",
        "segment_6": "To contribute, please open a PR that adds an entry to the data/gallery.json file in the src directory. The entry should be an object with the following properties:",
        "segment_7": "{ \"title\": \"AutoGen Playground\", \"link\": \"https://huggingface.co/spaces/thinkall/AutoGen_Playground\", \"description\": \"A space to explore the capabilities of AutoGen.\", \"image\": \"default.png\", \"tags\": [\"ui\"] }",
        "segment_8": "The image property should be the name of a file in the static/img/gallery directory.",
        "segment_9": "The tags property should be an array of strings that describe the demo. We recommend using no more than two tags for clarity.",
        "segment_10": "Here are the meanings of several tags for reference:",
        "segment_12": "app: Using Autogen for specific applications.",
        "segment_13": "extension: Enhancing AutoGen beyond the features in current version.",
        "segment_14": "ui: Building user interface for AutoGen.",
        "segment_15": "tool: Strengthening AutoGen Agents with external tools.",
        "segment_16": "groupchat: Solving complex tasks with a group of Agents.",
        "segment_18": "if the existing ones do not precisely portray your own demos, new tags are also encouraged to add.Edit this pageContributingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_0": "t(null!==e?e:\"light\")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith(\"docusaurus-data-\")){var a=t.replace(\"docusaurus-data-\",\"data-\");document.documentElement.setAttribute(a,e)}}catch(t){}}()Skip to main contentAutoGenDocsSDKBlogFAQExamplesResourcesEcosystemGalleryGitHubctrlKRecent postsAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsCode execution is now by default inside docker containerAll About Agent DescriptionsAgentOptimizer - An Agentic Way to Train Your LLM AgentAutoGen Studio: Interactively Explore Multi-Agent WorkflowsAgent AutoBuild - Automatically Building Multi-agent SystemsHow to Assess Utility of LLM-powered Applications?AutoGen Meets GPTsEcoAssistant - Using LLM Assistants More Accurately and AffordablyMultimodal with GPT-4V and LLaVAAutoGen's Teachable AgentsRetrieval-Augmented Generation (RAG) Applications with AutoGenUse AutoGen for Local LLMsMathChat - An Conversational Framework to Solve Math ProblemsAchieve More, Pay Less - Use GPT-4 SmartlyDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismJanuary 26, 2024 \u00b7 6 min readOlga VrousgouSenior Software Engineer at Microsoft ResearchTL;DR\u200b",
        "segment_1": "AutoGen now supports custom models! This feature empowers users to define and load their own models, allowing for a more flexible and personalized inference mechanism. By adhering to a specific protocol, you can integrate your custom model for use with AutoGen and respond to prompts any way needed by using any model/API call/hardcoded response you want.",
        "segment_2": "NOTE: Depending on what model you use, you may need to play with the default prompts of the Agent's",
        "segment_3": "Quickstart\u200b",
        "segment_4": "An interactive and easy way to get started is by following the notebook here which loads a local model from HuggingFace into AutoGen and uses it for inference, and making changes to the class provided.",
        "segment_5": "Step 1: Create the custom model client class\u200b",
        "segment_6": "To get started with using custom models in AutoGen, you need to create a model client class that adheres to the ModelClient protocol defined in client.py. The new model client class should implement these methods:",
        "segment_8": "create(): Returns a response object that implements the ModelClientResponseProtocol (more details in the Protocol section).",
        "segment_9": "message_retrieval(): Processes the response object and returns a list of strings or a list of message objects (more details in the Protocol section).",
        "segment_10": "cost(): Returns the cost of the response.",
        "segment_11": "get_usage(): Returns a dictionary with keys from RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"].",
        "segment_13": "E.g. of a bare bones dummy custom class:",
        "segment_14": "class CustomModelClient: def __init__(self, config, **kwargs): print(f\"CustomModelClient config: {config}\") def create(self, params): num_of_responses = params.get(\"n\", 1) # can create my own data response class # here using SimpleNamespace for simplicity # as long as it adheres to the ModelClientResponseProtocol response = SimpleNamespace() response.choices = [] response.model = \"model_name\" # should match the OAI_CONFIG_LIST registration for _ in range(num_of_responses): text = \"this is a dummy text response\" choice = SimpleNamespace() choice.message = SimpleNamespace() choice.message.content = text choice.message.function_call = None response.choices.append(choice) return response def message_retrieval(self, response): choices = response.choices return [choice.message.content for choice in choices] def cost(self, response) -> float: response.cost = 0 return 0 @staticmethod def get_usage(response): return {}",
        "segment_15": "Step 2: Add the configuration to the OAI_CONFIG_LIST\u200b",
        "segment_16": "The field that is necessary is setting model_client_cls to the name of the new class (as a string) \"model_client_cls\":\"CustomModelClient\". Any other fields will be forwarded to the class constructor, so you have full control over what parameters to specify and how to use them. E.g.:",
        "segment_17": "{ \"model\": \"Open-Orca/Mistral-7B-OpenOrca\", \"model_client_cls\": \"CustomModelClient\", \"device\": \"cuda\", \"n\": 1, \"params\": { \"max_length\": 1000, }}",
        "segment_18": "Step 3: Register the new custom model to the agent that will use it\u200b",
        "segment_19": "If a configuration with the field \"model_client_cls\":\"\" has been added to an Agent's config list, then the corresponding model with the desired class must be registered after the agent is created and before the conversation is initialized:",
        "segment_20": "my_agent.register_model_client(model_client_cls=CustomModelClient, [other args that will be forwarded to CustomModelClient constructor])",
        "segment_21": "model_client_cls=CustomModelClient arg matches the one specified in the OAI_CONFIG_LIST and CustomModelClient is the class that adheres to the ModelClient protocol (more details on the protocol below).",
        "segment_22": "If the new model client is in the config list but not registered by the time the chat is initialized, then an error will be raised.",
        "segment_23": "Protocol details\u200b",
        "segment_24": "A custom model class can be created in many ways, but needs to adhere to the ModelClient protocol and response structure which is defined in client.py and shown below.",
        "segment_25": "The response protocol is currently using the minimum required fields from the autogen codebase that match the OpenAI response structure. Any response protocol that matches the OpenAI response structure will probably be more resilient to future changes, but we are starting off with minimum requirements to make adpotion of this feature easier.",
        "segment_26": "class ModelClient(Protocol): \"\"\" A client class must implement the following methods: - create must return a response object that implements the ModelClientResponseProtocol - cost must return the cost of the response - get_usage must return a dict with the following keys: - prompt_tokens - completion_tokens - total_tokens - cost - model This class is used to create a client that can be used by OpenAIWrapper. The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed. The message_retrieval method must be implemented to return a list of str or a list of messages from the response. \"\"\" RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"] class ModelClientResponseProtocol(Protocol): class Choice(Protocol): class Message(Protocol): content: Optional[str] message: Message choices: List[Choice] model: str def create(self, params) -> ModelClientResponseProtocol: ... def message_retrieval( self, response: ModelClientResponseProtocol ) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]: \"\"\" Retrieve and return a list of strings or a list of Choice.Message from the response. NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used. \"\"\" ... def cost(self, response: ModelClientResponseProtocol) -> float: ... @staticmethod def get_usage(response: ModelClientResponseProtocol) -> Dict: \"\"\"Return usage summary of the response using RESPONSE_USAGE_KEYS.\"\"\" ...",
        "segment_27": "Troubleshooting steps\u200b",
        "segment_28": "If something doesn't work then run through the checklist:",
        "segment_30": "Make sure you have followed the client protocol and client response protocol when creating the custom model class",
        "segment_32": "create() method: ModelClientResponseProtocol must be followed when returning an inference response during create call.",
        "segment_33": "message_retrieval() method: returns a list of strings or a list of message objects. If a list of message objects is returned, they currently must contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.",
        "segment_34": "cost()method: returns an integer, and if you don't care about cost tracking you can just return 0.",
        "segment_35": "get_usage(): returns a dictionary, and if you don't care about usage tracking you can just return an empty dictionary {}.",
        "segment_38": "Make sure you have a corresponding entry in the OAI_CONFIG_LIST and that that entry has the \"model_client_cls\":\"\" field.",
        "segment_39": "Make sure you have registered the client using the corresponding config entry and your new class agent.register_model_client(model_client_cls=, [other optional args])",
        "segment_40": "Make sure that all of the custom models defined in the OAI_CONFIG_LIST have been registered.",
        "segment_41": "Any other troubleshooting might need to be done in the custom code itself.",
        "segment_43": "Conclusion\u200b",
        "segment_44": "With the ability to use custom models, AutoGen now offers even more flexibility and power for your AI applications. Whether you've trained your own model or want to use a specific pre-trained model, AutoGen can accommodate your needs. Happy coding!Tags:AutoGenAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsJanuary 25, 2024 \u00b7 7 min readAdam FourneyPrincipal Researcher Microsoft ResearchQingyun WuAssistant Professor at the Pennsylvania State University",
        "segment_45": "AutoGenBench is a standalone tool for evaluating AutoGen agents and workflows on common benchmarks.",
        "segment_46": "TLDR\u200b",
        "segment_47": "Today we are releasing AutoGenBench \u2013 a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.",
        "segment_48": "AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.",
        "segment_50": "See the AutoGenBench README for information on installation and running benchmarks.",
        "segment_51": "See the AutoGenBench CONTRIBUTING guide for information on developing or contributing benchmark datasets.",
        "segment_53": "Quick Start\u200b",
        "segment_54": "Get started quickly by running the following commands in a bash terminal.",
        "segment_55": "Note: You may need to adjust the path to the OAI_CONFIG_LIST, as appropriate.",
        "segment_56": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)pip install autogenbenchautogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate Results/human_eval_two_agents",
        "segment_57": "Introduction\u200b",
        "segment_58": "Measurement and evaluation are core components of every major AI or ML research project. The same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to AgentEval. In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluation; and conclude with an open call for contributions.",
        "segment_59": "Design Principles\u200b",
        "segment_60": "AutoGenBench is designed around three core design principles. Knowing these principles will help you understand the tool, its operation and its output. These three principles are:",
        "segment_63": "Repetition: LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.",
        "segment_66": "Isolation: Agents interact with their worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to ordering effects that can impact future measurements. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in its own Docker container. This ensures that all runs start with the same initial conditions. (Docker is also a much safer way to run agent-produced code, in general.)",
        "segment_69": "Instrumentation: While top-line metrics are great for comparing agents or models, we often want much more information about how the agents are performing, where they are getting stuck, and how they can be improved. We may also later think of new research questions that require computing a different set of metrics. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like AgentEval.",
        "segment_72": "Installing and Running AutoGenBench\u200b",
        "segment_73": "As noted above, isolation is a key design principle, and so AutoGenBench must be run in an environment where Docker is available (desktop or Engine). It will not run in GitHub codespaces, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see https://www.docker.com/products/docker-desktop/.",
        "segment_74": "Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With pip, installation can be achieved as follows:",
        "segment_75": "pip install autogenbench",
        "segment_76": "After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter.",
        "segment_77": "If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:",
        "segment_78": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)",
        "segment_79": "A Typical Session\u200b",
        "segment_80": "Once AutoGenBench and necessary keys are installed, a typical session will look as follows:",
        "segment_81": "autogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate results/human_eval_two_agents",
        "segment_82": "Where:",
        "segment_84": "autogenbench clone HumanEval downloads and expands the HumanEval benchmark scenario.",
        "segment_85": "cd HumanEval; cat README.md navigates to the benchmark directory, and prints the README (which you should always read!)",
        "segment_86": "autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl",
        "segment_87": "runs a 10% subsample of the tasks defined in Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.",
        "segment_88": "autogenbench tabulate results/human_eval_two_agents tabulates the results of the run.",
        "segment_90": "After running the above tabulate command, you should see output similar to the following:",
        "segment_91": "Trial 0 Trial 1 Trial 2Task Id Success Success Success------------- --------- --------- ---------HumanEval_107 False True TrueHumanEval_22 True True TrueHumanEval_43 True True TrueHumanEval_88 True True TrueHumanEval_14 True True TrueHumanEval_157 True True TrueHumanEval_141 True True TrueHumanEval_57 True True TrueHumanEval_154 True True TrueHumanEval_153 True True TrueHumanEval_93 False True FalseHumanEval_137 True True TrueHumanEval_143 True True TrueHumanEval_13 True True TrueHumanEval_49 True True TrueHumanEval_95 True True True------------- --------- --------- ---------Successes 14 16 15Failures 2 0 1Missing 0 0 0Total 16 16 16CAUTION: 'autogenbench tabulate' is in early preview.Please do not cite these values in academic work without first inspecting and verifying the results in the logs yourself.",
        "segment_92": "From this output we can see the results of the three separate repetitions of each task, and final summary statistics of each run. In this case, the results were generated via GPT-4 (as defined in the OAI_CONFIG_LIST that was provided), and used the TwoAgents template. It is important to remember that AutoGenBench evaluates specific end-to-end configurations of agents (as opposed to evaluating a model or cognitive framework more generally).",
        "segment_93": "Finally, complete execution traces and logs can be found in the Results folder. See the AutoGenBench README for more details about command-line options and output formats. Each of these commands also offers extensive in-line help via:",
        "segment_95": "autogenbench --help",
        "segment_96": "autogenbench clone --help",
        "segment_97": "autogenbench run --help",
        "segment_98": "autogenbench tabulate --help",
        "segment_100": "Roadmap\u200b",
        "segment_101": "While we are announcing AutoGenBench, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:",
        "segment_103": "Onboard many additional benchmarks beyond those shipping today",
        "segment_104": "Greatly improve logging and telemetry",
        "segment_105": "Introduce new core metrics including total costs, task completion time, conversation turns, etc.",
        "segment_106": "Provide tighter integration with AgentEval and AutoGen Studio",
        "segment_108": "For an up to date tracking of our work items on this project, please see AutoGenBench Work Items",
        "segment_109": "Call for Participation\u200b",
        "segment_110": "Finally, we want to end this blog post with an open call for contributions. AutoGenBench is still nascent, and has much opportunity for improvement. New benchmarks are constantly being published, and will need to be added. Everyone may have their own distinct set of metrics that they care most about optimizing, and these metrics should be onboarded. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the contributor\u2019s guide and join our Discord discussion in the #autogenbench channel!Tags:AutoGenCode execution is now by default inside docker containerJanuary 23, 2024 \u00b7 3 min readOlga VrousgouSenior Software Engineer at Microsoft ResearchTLDR\u200b",
        "segment_111": "AutoGen 0.2.8 enhances operational safety by making 'code execution inside a Docker container' the default setting, focusing on informing users about its operations and empowering them to make informed decisions regarding code execution.",
        "segment_112": "The new release introduces a breaking change where the use_docker argument is set to True by default in code executing agents. This change underscores our commitment to prioritizing security and safety in AutoGen.",
        "segment_113": "Introduction\u200b",
        "segment_114": "AutoGen has code-executing agents, usually defined as a UserProxyAgent, where code execution is by default ON. Until now, unless explicitly specified by the user, any code generated by other agents would be executed by code-execution agents locally, i.e. wherever AutoGen was being executed. If AutoGen happened to be run in a docker container then the risks of running code were minimized. However, if AutoGen runs outside of Docker, it's easy particularly for new users to overlook code-execution risks.",
        "segment_115": "AutoGen has now changed to by default execute any code inside a docker container (unless execution is already happening inside a docker container). It will launch a Docker image (either user-provided or default), execute the new code, and then terminate the image, preparing for the next code execution cycle.",
        "segment_116": "We understand that not everyone is concerned about this especially when playing around with AutoGen for the first time. We have provided easy ways to turn this requirement off. But we believe that making sure that the user is aware of the fact that code will be executed locally, and prompting them to think about the security implications of running code locally is the right step for AutoGen.",
        "segment_117": "Example\u200b",
        "segment_118": "The example shows the default behaviour which is that any code generated by assistant agent and executed by user_proxy agent, will attempt to use a docker container to execute the code. If docker is not running, it will throw an error. User can decide to activate docker or opt in for local code execution.",
        "segment_119": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_jsonassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")",
        "segment_120": "To opt out of from this default behaviour there are some options.",
        "segment_121": "Diasable code execution entirely\u200b",
        "segment_123": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_125": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config=False)",
        "segment_126": "Run code execution locally\u200b",
        "segment_128": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_129": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_131": "E.g.:",
        "segment_132": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})",
        "segment_133": "Related documentation\u200b",
        "segment_135": "Code execution with docker",
        "segment_136": "How to disable code execution in docker",
        "segment_138": "Conclusion\u200b",
        "segment_139": "AutoGen 0.2.8 now improves the code execution safety and is ensuring that the user is properly informed of what autogen is doing and can make decisions around code-execution.Tags:AutoGenAll About Agent DescriptionsDecember 29, 2023 \u00b7 9 min readAdam FourneyPrincipal Researcher Microsoft ResearchTLDR\u200b",
        "segment_140": "AutoGen 0.2.2 introduces a description field to ConversableAgent (and all subclasses), and changes GroupChat so that it uses agent descriptions rather than system_messages when choosing which agents should speak next.",
        "segment_141": "This is expected to simplify GroupChat\u2019s job, improve orchestration, and make it easier to implement new GroupChat or GroupChat-like alternatives.",
        "segment_142": "If you are a developer, and things were already working well for you, no action is needed -- backward compatibility is ensured because the description field defaults to the system_message when no description is provided.",
        "segment_143": "However, if you were struggling with getting GroupChat to work, you can now try updating the description field.",
        "segment_144": "Introduction\u200b",
        "segment_145": "As AutoGen matures and developers build increasingly complex combinations of agents, orchestration is becoming an important capability. At present, GroupChat and the GroupChatManager are the main built-in tools for orchestrating conversations between 3 or more agents. For orchestrators like GroupChat to work well, they need to know something about each agent so that they can decide who should speak and when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's system_message and name to learn about each participating agent. This is likely fine when the system prompt is short and sweet, but can lead to problems when the instructions are very long (e.g., with the AssistantAgent), or non-existent (e.g., with the UserProxyAgent).",
        "segment_146": "AutoGen 0.2.2 introduces a description field to all agents, and replaces the use of the system_message for orchestration in GroupChat and all future orchestrators. The description field defaults to the system_message to ensure backwards compatibility, so you may not need to change anything with your code if things are working well for you. However, if you were struggling with GroupChat, give setting the description field a try.",
        "segment_147": "The remainder of this post provides an example of how using the description field simplifies GroupChat's job, provides some evidence of its effectiveness, and provides tips for writing good descriptions.",
        "segment_148": "Example\u200b",
        "segment_149": "The current GroupChat orchestration system prompt has the following template:",
        "segment_150": "You are in a role play game. The following roles are available:{self._participant_roles(agents)}.Read the following conversation.Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.",
        "segment_151": "Suppose that you wanted to include 3 agents: A UserProxyAgent, an AssistantAgent, and perhaps a GuardrailsAgent.",
        "segment_152": "Prior to 0.2.2, this template would expand to:",
        "segment_153": "You are in a role play game. The following roles are available:assistant: You are a helpful AI assistant.Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply \"TERMINATE\" in the end when everything is done.user_proxy:guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:- You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.- You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.- You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.- You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.- You MUST TERMINATE the conversation if it involves seeking or giving advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice- You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.- You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.- You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.- You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.- You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word \"TERMINATE\". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_154": "As you can see, this description is super confusing:",
        "segment_156": "It is hard to make out where each agent's role-description ends",
        "segment_157": "You appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)",
        "segment_158": "It takes a lot of tokens!",
        "segment_160": "Consequently, it's not hard to see why the GroupChat manager sometimes struggles with this orchestration task.",
        "segment_161": "With AutoGen 0.2.2 onward, GroupChat instead relies on the description field. With a description field the orchestration prompt becomes:",
        "segment_162": "You are in a role play game. The following roles are available:assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_163": "This is much easier to parse and understand, and it doesn't use nearly as many tokens. Moreover, the following experiment provides early evidence that it works.",
        "segment_164": "An Experiment with Distraction\u200b",
        "segment_165": "To illustrate the impact of the description field, we set up a three-agent experiment with a reduced 26-problem subset of the HumanEval benchmark. Here, three agents were added to a GroupChat to solve programming problems. The three agents were:",
        "segment_167": "Coder (default Assistant prompt)",
        "segment_168": "UserProxy (configured to execute code)",
        "segment_169": "ExecutiveChef (added as a distraction)",
        "segment_171": "The Coder and UserProxy used the AssistantAgent and UserProxy defaults (provided above), while the ExecutiveChef was given the system prompt:",
        "segment_172": "You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.",
        "segment_173": "The ExecutiveChef is clearly the distractor here -- given that no HumanEval problems are food-related, the GroupChat should rarely consult with the chef. However, when configured with GPT-3.5-turbo-16k, we can clearly see the GroupChat struggling with orchestration:",
        "segment_174": "With versions prior to 0.2.2, using system_message:\u200b",
        "segment_176": "The Agents solve 3 out of 26 problems on their first turn",
        "segment_177": "The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)",
        "segment_179": "With version 0.2.2, using description:\u200b",
        "segment_181": "The Agents solve 7 out of 26 problems on the first turn",
        "segment_182": "The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)",
        "segment_184": "Using the description field doubles performance on this task and halves the incidence of calling upon the distractor agent.",
        "segment_185": "Tips for Writing Good Descriptions\u200b",
        "segment_186": "Since descriptions serve a different purpose than system_messages, it is worth reviewing what makes a good agent description. While descriptions are new, the following tips appear to lead to good results:",
        "segment_188": "Avoid using the 1st or 2nd person perspective. Descriptions should not contain \"I\" or \"You\", unless perhaps \"You\" is in reference to the GroupChat / orchestrator",
        "segment_189": "Include any details that might help the orchestrator know when to call upon the agent",
        "segment_190": "Keep descriptions short (e.g., \"A helpful AI assistant with strong natural language and Python coding skills.\").",
        "segment_192": "The main thing to remember is that the description is for the benefit of the GroupChatManager, not for the Agent's own use or instruction.",
        "segment_193": "Conclusion\u200b",
        "segment_194": "AutoGen 0.2.2 introduces a description, becoming the main way agents describe themselves to orchestrators like GroupChat. Since the description defaults to the system_message, there's nothing you need to change if you were already satisfied with how your group chats were working. However, we expect this feature to generally improve orchestration, so please consider experimenting with the description field if you are struggling with GroupChat or want to boost performance.Tags:AutoGenAgentOptimizer - An Agentic Way to Train Your LLM AgentDecember 23, 2023 \u00b7 7 min readShaokun ZhangPhD student at the Pennsylvania State UniversityJieyu ZhangPhD student at University of Washington",
        "segment_195": "TL;DR:",
        "segment_196": "Introducing AgentOptimizer, a new class for training LLM agents in the era of LLMs as a service.",
        "segment_197": "AgentOptimizer is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.",
        "segment_198": "Checkout one implementation for AgentOptimizer on MATH dataset",
        "segment_199": "here.",
        "segment_200": "Paper is coming soon!",
        "segment_201": "Introduction\u200b",
        "segment_202": "In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?",
        "segment_203": "Here, we take an initial step towards the agent training.",
        "segment_204": "Inspired by the function calling capabilities provided by OpenAI,",
        "segment_205": "we draw an analogy between model weights and agent functions/skills, and update an agent\u2019s functions/skills based on its historical performance on a training set.",
        "segment_206": "Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents\u2019 functions as a set of function calls,",
        "segment_207": "to support iteratively adding, revising, and removing existing functions.",
        "segment_208": "As an agentic way of training an agent, our approach helps enhance the agents\u2019 abilities without requiring access to the LLMs weights.",
        "segment_209": "AgentOptimizer\u200b",
        "segment_210": "AgentOptimizer is a class designed to optimize the agents by improving their function calls.",
        "segment_211": "It contains two core methods:",
        "segment_213": "step(): step() takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).",
        "segment_215": "actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)",
        "segment_216": "It has two outputs actions and updated_functions. actions is a series of actions to manipulate the current functions. And updated_functions is the updated functions after the actions are applied (including code implementation).",
        "segment_218": "update_function_call():",
        "segment_219": "This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from step().",
        "segment_220": "For AssistantAgent, it first uses update_function_signature to update the function signatures.",
        "segment_221": "Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from step().",
        "segment_223": "Sometimes, the function signatures (JSON schema) returned by the step() may not be valid, and the generated code may also face syntax errors.",
        "segment_224": "AgentOptimizer includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.",
        "segment_225": "Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.",
        "segment_226": "Pseudocode for the optimization process\u200b",
        "segment_227": "The optimization process is as follows:",
        "segment_228": "for - in range(EPOCH): history, statistic, current_functions = solve_problems(train_problems) actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions) AgentOptimizer.update_function_call(actions)",
        "segment_229": "Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.",
        "segment_230": "The functions are then improved using AgentOptimizer.",
        "segment_231": "Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.",
        "segment_232": "After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks",
        "segment_233": "The implementation technology behind the AgentOptimizer\u200b",
        "segment_234": "To obtain stable and structured function signatures and code implementations from AgentOptimizer,",
        "segment_235": "we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.",
        "segment_236": "Specifically, we introduce three function calls to manipulate the current functions at each step: add_function, remove_function, and revise_function.",
        "segment_237": "These calls add, remove, and revise functions in the existing function list, respectively.",
        "segment_238": "This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.",
        "segment_239": "Below is the JSON schema of these function calls:",
        "segment_241": "add_function: Add one new function that may be used in the future tasks.",
        "segment_243": "ADD_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"add_function\", \"description\": \"Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_245": "revise_function: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.",
        "segment_247": "REVISE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"revise_function\", \"description\": \"Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_249": "remove_function: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.",
        "segment_251": "REMOVE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"remove_function\", \"description\": \"Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" } }, \"required\": [\"name\"] } }}",
        "segment_252": "Limitation & Future work\u200b",
        "segment_254": "Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.",
        "segment_255": "When the training epoch is small, the agent\u2019s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.",
        "segment_256": "The Current implementation of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.",
        "segment_257": "It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.",
        "segment_258": "Currently, it only supports optimizing the multi-agent system in solving problems from MATH dataset. We will integrate it into autogen with more general interface in the future.",
        "segment_259": "Tags:LLMresearchAutoGen Studio: Interactively Explore Multi-Agent WorkflowsDecember 1, 2023 \u00b7 10 min readVictor DibiaPrincipal RSDE at Microsoft ResearchGagan BansalSenior Researcher at Microsoft ResearchSaleema AmershiSenior Principal Research Manager at Microsoft Research",
        "segment_260": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_261": "TLDR\u200b",
        "segment_262": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_264": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_265": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_266": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_267": "Publish your sessions to a local gallery.",
        "segment_269": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_270": "pip install autogenstudio",
        "segment_271": "Introduction\u200b",
        "segment_272": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_273": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_275": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_277": "Getting Started with AutoGen Studio\u200b",
        "segment_278": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_279": "Configuring an LLM Provider\u200b",
        "segment_280": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_281": "For example, in your terminal, you would set the API key like this:",
        "segment_282": "export OPENAI_API_KEY=",
        "segment_283": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_284": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_285": "Installation\u200b",
        "segment_286": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_289": "Install from PyPi",
        "segment_290": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_291": "pip install autogenstudio",
        "segment_294": "Install from Source",
        "segment_296": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_298": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_301": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_302": "pip install -e .",
        "segment_305": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_306": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_309": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_312": "Running the Application\u200b",
        "segment_313": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_314": "autogenstudio ui --port 8081",
        "segment_315": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_316": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_317": "What Can You Do with AutoGen Studio?\u200b",
        "segment_318": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_319": "Build\u200b",
        "segment_321": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_322": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_324": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_325": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_326": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_327": "Playground\u200b",
        "segment_329": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_330": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_331": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_332": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_333": "Gallery\u200b",
        "segment_334": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_335": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_336": "The AutoGen Studio API\u200b",
        "segment_337": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_338": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_339": "Road Map and Next Steps\u200b",
        "segment_340": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_342": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_343": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_344": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_345": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_347": "Contribution Guide\u200b",
        "segment_348": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_350": "Review the overall AutoGen project contribution guide.",
        "segment_351": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_352": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_353": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_354": "Submit a pull request with your contribution!",
        "segment_355": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_356": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_358": "FAQ\u200b",
        "segment_359": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_360": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_361": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_362": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_363": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_364": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_365": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_366": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_367": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_368": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_369": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_370": "Q: The Server Starts But I Can't Access the UI",
        "segment_371": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_372": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_373": "Tags:AutoGenUIwebUXAgent AutoBuild - Automatically Building Multi-agent SystemsNovember 26, 2023 \u00b7 7 min readLinxin SongMS student at Waseda UniversityJieyu ZhangPhD student at University of Washington",
        "segment_374": "TL;DR:",
        "segment_375": "Introducing AutoBuild, building multi-agent system automatically, fast, and easily for complex tasks with minimal",
        "segment_376": "user prompt required, powered by a new designed class AgentBuilder. AgentBuilder also supports open-source LLMs by",
        "segment_377": "leveraging vLLM and FastChat.",
        "segment_378": "Checkout example notebooks and source code for reference:",
        "segment_380": "AutoBuild Examples",
        "segment_381": "AgentBuilder",
        "segment_383": "Introduction\u200b",
        "segment_384": "In this blog, we introduce AutoBuild, a pipeline that can automatically build multi-agent systems for complex tasks.",
        "segment_385": "Specifically, we design a new class called AgentBuilder, which will complete the generation of participant expert agents",
        "segment_386": "and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.",
        "segment_387": "AgentBuilder supports open-source models on Hugging Face powered by vLLM",
        "segment_388": "and FastChat. Once the user chooses to use open-source LLM, AgentBuilder will set",
        "segment_389": "up an endpoint server automatically without any user participation.",
        "segment_390": "Installation\u200b",
        "segment_392": "AutoGen:",
        "segment_394": "pip install pyautogen[autobuild]",
        "segment_396": "(Optional: if you want to use open-source LLMs) vLLM and FastChat",
        "segment_398": "pip install vllm fastchat",
        "segment_399": "Basic Example\u200b",
        "segment_400": "In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.",
        "segment_401": "Step 1: prepare configurations\u200b",
        "segment_402": "First, we need to prepare the Agent configurations.",
        "segment_403": "Specifically, a config path containing the model name and API key, and a default config for each agent, are required.",
        "segment_404": "config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST' # modify pathdefault_llm_config = { 'temperature': 0}",
        "segment_405": "Step 2: create an AgentBuilder instance\u200b",
        "segment_406": "Then, we create an AgentBuilder instance with the config path and default config.",
        "segment_407": "You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.",
        "segment_408": "from autogen.agentchat.contrib.agent_builder import AgentBuilderbuilder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')",
        "segment_409": "Step 3: specify the building task\u200b",
        "segment_410": "Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.",
        "segment_411": "Note that your building task should have a general description of the task. Adding some specific examples is better.",
        "segment_412": "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"",
        "segment_413": "Step 4: build group chat agents\u200b",
        "segment_414": "Use build() to let the build manager (with a builder_model as backbone) complete the group chat agents generation.",
        "segment_415": "If you think coding is necessary for your task, you can use coding=True to add a user proxy (a local code interpreter) into the agent list as:",
        "segment_416": "agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)",
        "segment_417": "If coding is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.",
        "segment_418": "The generated agent_list is a list of AssistantAgent instances.",
        "segment_419": "If coding is true, a user proxy (a UserProxyAssistant instance) will be added as the first element to the agent_list.",
        "segment_420": "agent_configs is a list of agent configurations including agent name, backbone LLM model, and system message.",
        "segment_421": "For example",
        "segment_422": "// an example of agent_configs. AgentBuilder will generate agents with the following configurations.[ { \"name\": \"ArXiv_Data_Scraper_Developer\", \"model\": \"gpt-4-1106-preview\", \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\", \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\" }, ...]",
        "segment_423": "Step 5: execute the task\u200b",
        "segment_424": "Let agents generated in build() complete the task collaboratively in a group chat.",
        "segment_425": "import autogendef start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\"]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task)start_task( execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\", agent_list=agent_list, llm_config=default_llm_config)",
        "segment_426": "Step 6 (Optional): clear all agents and prepare for the next task\u200b",
        "segment_427": "You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.",
        "segment_428": "builder.clear_all_agents(recycle_endpoint=True)",
        "segment_429": "If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.",
        "segment_430": "If necessary, you can use recycle_endpoint=False to retain the previous open-source LLM's endpoint server.",
        "segment_431": "Save and Load\u200b",
        "segment_432": "You can save all necessary information of the built group chat agents by",
        "segment_433": "saved_path = builder.save()",
        "segment_434": "Configurations will be saved in JSON format with the following content:",
        "segment_435": "// FILENAME: save_config_TASK_MD5.json{ \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"agent_configs\": [ { \"name\": \"...\", \"model\": \"...\", \"system_message\": \"...\", \"description\": \"...\" }, ... ], \"manager_system_message\": \"...\", \"code_execution_config\": {...}, \"default_llm_config\": {...}}",
        "segment_436": "You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename save_config_TASK_MD5.json.",
        "segment_437": "You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.",
        "segment_438": "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)agent_list, agent_config = new_builder.load(saved_path)start_task(...) # skip build()",
        "segment_439": "Use OpenAI Assistant\u200b",
        "segment_440": "Assistants API allows you to build AI assistants within your own applications.",
        "segment_441": "An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.",
        "segment_442": "AutoBuild also supports the assistant API by adding use_oai_assistant=True to build().",
        "segment_443": "# Transfer to the OpenAI Assistant API.agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)...",
        "segment_444": "(Experimental) Use Open-source LLM\u200b",
        "segment_445": "AutoBuild supports open-source LLM by vLLM and FastChat.",
        "segment_446": "Check the supported model list here.",
        "segment_447": "After satisfying the requirements, you can add an open-source LLM's huggingface repository to the config file,",
        "segment_448": "// Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.[ ... { \"model\": \"meta-llama/Llama-2-13b-chat-hf\", \"api_key\": \"EMPTY\" }]",
        "segment_449": "and specify it when initializing AgentBuilder.",
        "segment_450": "AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.",
        "segment_451": "Future work/Roadmap\u200b",
        "segment_453": "Let the builder select the best agents from a given library/database to solve the task.",
        "segment_455": "Summary\u200b",
        "segment_456": "We propose AutoBuild with a new class AgentBuilder.",
        "segment_457": "AutoBuild can help user solve their complex task with an automatically built multi-agent system.",
        "segment_458": "AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.",
        "segment_459": "More advanced features are coming soon.Tags:LLMresearchHow to Assess Utility of LLM-powered Applications?November 20, 2023 \u00b7 10 min readJulia KiselevaSenior Researcher at Microsoft ResearchNegar ArabzadehPhD student at the University of Waterloo",
        "segment_460": "Fig.1 illustrates the general flow of AgentEval",
        "segment_461": "TL;DR:",
        "segment_463": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_464": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_465": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_467": "Introduction\u200b",
        "segment_468": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_469": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_471": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_472": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_474": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_475": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_477": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_478": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_482": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_483": "AgentEval Framework\u200b",
        "segment_484": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_486": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_488": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_489": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_491": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_493": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_494": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_495": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_496": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_497": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_499": "AgentChat",
        "segment_500": "ReAct",
        "segment_501": "GPT-4 Vanilla Solver",
        "segment_503": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_505": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_506": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_507": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_508": "Limitations and Future Work\u200b",
        "segment_509": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_511": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_512": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_514": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_515": "Summary\u200b",
        "segment_516": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_517": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_518": "Previous Research\u200b",
        "segment_519": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_520": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityAutoGen Meets GPTsNovember 13, 2023 \u00b7 3 min readGagan BansalSenior Researcher at Microsoft Research",
        "segment_521": "AutoGen enables collaboration among multiple ChatGPTs for complex tasks.",
        "segment_522": "TLDR\u200b",
        "segment_523": "OpenAI assistants are now integrated into AutoGen via GPTAssistantAgent.",
        "segment_524": "This enables multiple OpenAI assistants, which form the backend of the now popular GPTs, to collaborate and tackle complex tasks.",
        "segment_525": "Checkout example notebooks for reference:",
        "segment_527": "Basic example",
        "segment_528": "Code interpreter",
        "segment_529": "Function calls",
        "segment_531": "Introduction\u200b",
        "segment_532": "Earlier last week, OpenAI introduced GPTs, giving users ability to create custom ChatGPTs tailored for them.",
        "segment_533": "But what if these individual GPTs could collaborate to do even more?",
        "segment_534": "Fortunately, because of AutoGen, this is now a reality!",
        "segment_535": "AutoGen has been pioneering agents and supporting multi-agent workflows since earlier this year, and now (starting with version 0.2.0b5) we are introducing compatibility with the Assistant API, which is currently in beta preview.",
        "segment_536": "To accomplish this, we've added a new (experimental) agent called the GPTAssistantAgent that",
        "segment_537": "lets you seamlessly add these new OpenAI assistants into AutoGen-based multi-agent workflows.",
        "segment_538": "This integration shows great potential and synergy, and we plan to continue enhancing it.",
        "segment_539": "Installation\u200b",
        "segment_540": "pip install pyautogen==0.2.0b5",
        "segment_541": "Basic Example\u200b",
        "segment_542": "Here's a basic example that uses a UserProxyAgent to allow an interface",
        "segment_543": "with the GPTAssistantAgent.",
        "segment_544": "First, import the new agent and setup config_list:",
        "segment_545": "from autogen import config_list_from_jsonfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgentfrom autogen import UserProxyAgentconfig_list = config_list_from_json(\"OAI_CONFIG_LIST\")",
        "segment_546": "Then simply define the OpenAI assistant agent and give it the task!",
        "segment_547": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_548": "GPTAssistantAgent supports both creating new OpenAI assistants or reusing existing assistants",
        "segment_549": "(e.g, by providing an assistant_id).",
        "segment_550": "Code Interpreter Example\u200b",
        "segment_551": "GPTAssistantAgent allows you to specify an OpenAI tools",
        "segment_552": "(e.g., function calls, code interpreter, etc). The example below enables an assistant",
        "segment_553": "that can use OpenAI code interpreter to solve tasks.",
        "segment_554": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None, \"tools\": [ { \"type\": \"code_interpreter\" } ], })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_555": "Checkout more examples here.",
        "segment_556": "Limitations and Future Work\u200b",
        "segment_558": "Group chat managers using GPT assistant are pending.",
        "segment_559": "GPT assistants with multimodal capabilities haven't been released yet but we are committed to support them.",
        "segment_561": "Acknowledgements\u200b",
        "segment_562": "GPTAssistantAgent was made possible through collaboration with",
        "segment_563": "@IANTHEREAL,",
        "segment_564": "Jiale Liu,",
        "segment_565": "Yiran Wu,",
        "segment_566": "Qingyun Wu,",
        "segment_567": "Chi Wang, and many other AutoGen maintainers.Tags:openai-assistantEcoAssistant - Using LLM Assistants More Accurately and AffordablyNovember 9, 2023 \u00b7 5 min readJieyu ZhangPhD student at University of Washington",
        "segment_568": "TL;DR:",
        "segment_570": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_571": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_572": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_573": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_575": "EcoAssistant\u200b",
        "segment_576": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_577": "Problem setup\u200b",
        "segment_578": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_579": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_580": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_581": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_582": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_583": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_584": "Leveraging external APIs\u200b",
        "segment_585": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_586": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_587": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_588": "A visualization of the two-agent system is shown below.",
        "segment_590": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_591": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_593": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_594": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_595": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_596": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_597": "Solution Demonstration\u200b",
        "segment_598": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_599": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_600": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_601": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_602": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_604": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_605": "Assistant Hierarchy\u200b",
        "segment_606": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_607": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_608": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_609": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_610": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_611": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_612": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_613": "A Synergistic Effect\u200b",
        "segment_614": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_615": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_616": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_617": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_618": "Experimental Results\u200b",
        "segment_619": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_620": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_622": "Further reading\u200b",
        "segment_623": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_624": "If you find this blog useful, please consider citing:",
        "segment_625": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessOlder EntriesCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class ConversableAgent(Agent)",
        "segment_2": "(In preview) A class for generic conversable agents which can be configured as assistant or user proxy.",
        "segment_3": "After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.",
        "segment_4": "For example, AssistantAgent and UserProxyAgent are subclasses of this class,",
        "segment_5": "configured with different default settings.",
        "segment_6": "To modify auto reply, override generate_reply method.",
        "segment_7": "To disable/enable human response in every turn, set human_input_mode to \"NEVER\" or \"ALWAYS\".",
        "segment_8": "To modify the way to get human input, override get_human_input method.",
        "segment_9": "To modify the way to execute code blocks, single code block, or function call, override execute_code_blocks,",
        "segment_10": "run_code, and execute_function methods respectively.",
        "segment_11": "To customize the initial message when a conversation starts, override generate_init_message method.",
        "segment_12": "DEFAULT_CONFIG\u200b",
        "segment_13": "An empty configuration",
        "segment_14": "MAX_CONSECUTIVE_AUTO_REPLY\u200b",
        "segment_15": "maximum number of consecutive auto replies (subject to future change)",
        "segment_16": "__init__\u200b",
        "segment_17": "def __init__(name: str, system_message: Optional[Union[ str, List]] = \"You are a helpful AI Assistant.\", is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"TERMINATE\", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Union[Dict, Literal[False]] = False, llm_config: Optional[Union[Dict, Literal[False]]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = \"\", description: Optional[str] = None)",
        "segment_18": "Arguments:",
        "segment_20": "name str - name of the agent.",
        "segment_21": "system_message str or list - system message for the ChatCompletion inference.",
        "segment_22": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_23": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_24": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_25": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_26": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_27": "When set to 0, no auto reply will be generated.",
        "segment_28": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_29": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_30": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_31": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_32": "or when is_termination_msg is True and there is no human input.",
        "segment_33": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_34": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_35": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_36": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_37": "function_map dict[str, callable] - Mapping function names (passed to openai) to callable functions, also used for tool calls.",
        "segment_38": "code_execution_config dict or False - config for the code execution.",
        "segment_39": "To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:",
        "segment_41": "work_dir (Optional, str): The working directory for the code execution.",
        "segment_42": "If None, a default working directory will be used.",
        "segment_43": "The default working directory is the \"extensions\" directory under",
        "segment_44": "\"path_to_autogen\".",
        "segment_45": "use_docker (Optional, list, str or bool): The docker image to use for code execution.",
        "segment_46": "Default is True, which means the code will be executed in a docker container. A default list of images will be used.",
        "segment_47": "If a list or a str of image name(s) is provided, the code will be executed in a docker container",
        "segment_48": "with the first image successfully pulled.",
        "segment_49": "If False, the code will be executed in the current environment.",
        "segment_50": "We strongly recommend using docker for code execution.",
        "segment_51": "timeout (Optional, int): The maximum execution time in seconds.",
        "segment_52": "last_n_messages (Experimental, int or str): The number of messages to look back for code execution.",
        "segment_53": "If set to 'auto', it will scan backwards through all messages arriving since the agent last spoke, which is typically the last time execution was attempted. (Default: auto)",
        "segment_56": "llm_config dict or False - llm inference configuration.",
        "segment_57": "Please refer to OpenAIWrapper.create",
        "segment_58": "for available options.",
        "segment_59": "To disable llm-based auto reply, set to False.",
        "segment_60": "default_auto_reply str or dict or None - default auto reply when no code execution or llm-based reply is generated.",
        "segment_61": "description str - a short description of the agent. This description is used by other agents",
        "segment_62": "(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)",
        "segment_64": "register_reply\u200b",
        "segment_65": "def register_reply(trigger: Union[Type[Agent], str, Agent, Callable[[Agent], bool], List], reply_func: Callable, position: int = 0, config: Optional[Any] = None, reset_config: Optional[Callable] = None, *, ignore_async_in_sync_chat: bool = False)",
        "segment_66": "Register a reply function.",
        "segment_67": "The reply function will be called when the trigger matches the sender.",
        "segment_68": "The function registered later will be checked earlier by default.",
        "segment_69": "To change the order, set the position to a positive integer.",
        "segment_70": "Both sync and async reply functions can be registered. The sync reply function will be triggered",
        "segment_71": "from both sync and async chats. However, an async reply function will only be triggered from async",
        "segment_72": "chats (initiated with ConversableAgent.a_initiate_chat). If an async reply function is registered",
        "segment_73": "and a chat is initialized with a sync function, ignore_async_in_sync_chat determines the behaviour as follows:",
        "segment_75": "if ignore_async_in_sync_chat is set to False (default value), an exception will be raised, and",
        "segment_76": "if ignore_async_in_sync_chat is set to True, the reply function will be ignored.",
        "segment_78": "Arguments:",
        "segment_80": "trigger Agent class, str, Agent instance, callable, or list - the trigger.",
        "segment_82": "If a class is provided, the reply function will be called when the sender is an instance of the class.",
        "segment_83": "If a string is provided, the reply function will be called when the sender's name matches the string.",
        "segment_84": "If an agent instance is provided, the reply function will be called when the sender is the agent instance.",
        "segment_85": "If a callable is provided, the reply function will be called when the callable returns True.",
        "segment_86": "If a list is provided, the reply function will be called when any of the triggers in the list is activated.",
        "segment_87": "If None is provided, the reply function will be called only when the sender is None.",
        "segment_90": "Note - Be sure to register None as a trigger if you would like to trigger an auto-reply function with non-empty messages and async0.",
        "segment_91": "async1 Callable - the reply function.",
        "segment_92": "The function takes a recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.",
        "segment_93": "async2 - the position of the reply function in the reply function list.",
        "segment_94": "async3 - the config to be passed to the reply function, see below.",
        "segment_95": "async4 - the function to reset the config, see below.",
        "segment_96": "ignore_async_in_sync_chat - whether to ignore the async reply function in sync chats. If False, an exception",
        "segment_97": "will be raised if an async reply function is registered and a chat is initialized with a sync",
        "segment_98": "function.",
        "segment_99": "async7",
        "segment_100": "async2 int - the position of the reply function in the reply function list.",
        "segment_101": "The function registered later will be checked earlier by default.",
        "segment_102": "To change the order, set the position to a positive integer.",
        "segment_103": "async3 Any - the config to be passed to the reply function.",
        "segment_104": "When an agent is reset, the config will be reset to the original value.",
        "segment_105": "async4 Callable - the function to reset the config.",
        "segment_106": "The function returns None. Signature: ignore_async_in_sync_chat1",
        "segment_108": "system_message\u200b",
        "segment_109": "@propertydef system_message() -> Union[str, List]",
        "segment_110": "Return the system message.",
        "segment_111": "update_system_message\u200b",
        "segment_112": "def update_system_message(system_message: Union[str, List])",
        "segment_113": "Update the system message.",
        "segment_114": "Arguments:",
        "segment_116": "system_message str or List - system message for the ChatCompletion inference.",
        "segment_118": "update_max_consecutive_auto_reply\u200b",
        "segment_119": "def update_max_consecutive_auto_reply(value: int, sender: Optional[Agent] = None)",
        "segment_120": "Update the maximum number of consecutive auto replies.",
        "segment_121": "Arguments:",
        "segment_123": "value int - the maximum number of consecutive auto replies.",
        "segment_124": "sender Agent - when the sender is provided, only update the max_consecutive_auto_reply for that sender.",
        "segment_126": "max_consecutive_auto_reply\u200b",
        "segment_127": "def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int",
        "segment_128": "The maximum number of consecutive auto replies.",
        "segment_129": "chat_messages\u200b",
        "segment_130": "@propertydef chat_messages() -> Dict[Agent, List[Dict]]",
        "segment_131": "A dictionary of conversations from agent to list of messages.",
        "segment_132": "last_message\u200b",
        "segment_133": "def last_message(agent: Optional[Agent] = None) -> Optional[Dict]",
        "segment_134": "The last message exchanged with the agent.",
        "segment_135": "Arguments:",
        "segment_137": "agent Agent - The agent in the conversation.",
        "segment_138": "If None and more than one agent's conversations are found, an error will be raised.",
        "segment_139": "If None and only one conversation is found, the last message of the only conversation will be returned.",
        "segment_141": "Returns:",
        "segment_142": "The last message exchanged with the agent.",
        "segment_143": "use_docker\u200b",
        "segment_144": "@propertydef use_docker() -> Union[bool, str, None]",
        "segment_145": "Bool value of whether to use docker to execute the code,",
        "segment_146": "or str value of the docker image name to use, or None when code execution is disabled.",
        "segment_147": "send\u200b",
        "segment_148": "def send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_149": "Send a message to another agent.",
        "segment_150": "Arguments:",
        "segment_152": "message dict or str - message to be sent.",
        "segment_153": "The message could contain the following fields:",
        "segment_155": "content (str or List): Required, the content of the message. (Can be None)",
        "segment_156": "function_call (str): the name of the function to be called.",
        "segment_157": "name (str): the name of the function to be called.",
        "segment_158": "role (str): the role of the message, any role that is not \"function\"",
        "segment_159": "will be modified to \"assistant\".",
        "segment_160": "context (dict): the context of the message, which will be passed to",
        "segment_161": "OpenAIWrapper.create.",
        "segment_162": "For example, one agent can send a message A as:",
        "segment_166": "{ \"content\": lambda context: context[\"use_tool_msg\"], \"context\": { \"use_tool_msg\": \"Use tool X if they are relevant.\" }}",
        "segment_167": "Next time, one agent can send a message B with a different \"use_tool_msg\".",
        "segment_168": "Then the content of message A will be refreshed to the new \"use_tool_msg\".",
        "segment_169": "So effectively, this provides a way for an agent to send a \"link\" and modify",
        "segment_170": "the content of the \"link\" later.",
        "segment_172": "recipient Agent - the recipient of the message.",
        "segment_173": "request_reply bool or None - whether to request a reply from the recipient.",
        "segment_174": "silent bool or None - (Experimental) whether to print the message sent.",
        "segment_176": "Raises:",
        "segment_178": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_180": "a_send\u200b",
        "segment_181": "async def a_send(message: Union[Dict, str], recipient: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_182": "(async) Send a message to another agent.",
        "segment_183": "Arguments:",
        "segment_185": "message dict or str - message to be sent.",
        "segment_186": "The message could contain the following fields:",
        "segment_188": "content (str or List): Required, the content of the message. (Can be None)",
        "segment_189": "function_call (str): the name of the function to be called.",
        "segment_190": "name (str): the name of the function to be called.",
        "segment_191": "role (str): the role of the message, any role that is not \"function\"",
        "segment_192": "will be modified to \"assistant\".",
        "segment_193": "context (dict): the context of the message, which will be passed to",
        "segment_194": "OpenAIWrapper.create.",
        "segment_195": "For example, one agent can send a message A as:",
        "segment_199": "{ \"content\": lambda context: context[\"use_tool_msg\"], \"context\": { \"use_tool_msg\": \"Use tool X if they are relevant.\" }}",
        "segment_200": "Next time, one agent can send a message B with a different \"use_tool_msg\".",
        "segment_201": "Then the content of message A will be refreshed to the new \"use_tool_msg\".",
        "segment_202": "So effectively, this provides a way for an agent to send a \"link\" and modify",
        "segment_203": "the content of the \"link\" later.",
        "segment_205": "recipient Agent - the recipient of the message.",
        "segment_206": "request_reply bool or None - whether to request a reply from the recipient.",
        "segment_207": "silent bool or None - (Experimental) whether to print the message sent.",
        "segment_209": "Raises:",
        "segment_211": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_213": "receive\u200b",
        "segment_214": "def receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_215": "Receive a message from another agent.",
        "segment_216": "Once a message is received, this function sends a reply to the sender or stop.",
        "segment_217": "The reply can be generated automatically or entered manually by a human.",
        "segment_218": "Arguments:",
        "segment_220": "message dict or str - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).",
        "segment_222": "\"content\": content of the message, can be None.",
        "segment_223": "\"function_call\": a dictionary containing the function name and arguments. (deprecated in favor of \"tool_calls\")",
        "segment_224": "\"tool_calls\": a list of dictionaries containing the function name and arguments.",
        "segment_225": "\"role\": role of the message, can be \"assistant\", \"user\", \"function\", \"tool\".",
        "segment_226": "This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".",
        "segment_227": "\"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.",
        "segment_228": "\"context\" (dict): the context of the message, which will be passed to",
        "segment_229": "OpenAIWrapper.create.",
        "segment_232": "sender - sender of an Agent instance.",
        "segment_233": "request_reply bool or None - whether a reply is requested from the sender.",
        "segment_234": "If None, the value is determined by self.reply_at_receive[sender].",
        "segment_235": "silent bool or None - (Experimental) whether to print the message received.",
        "segment_237": "Raises:",
        "segment_239": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_241": "a_receive\u200b",
        "segment_242": "async def a_receive(message: Union[Dict, str], sender: Agent, request_reply: Optional[bool] = None, silent: Optional[bool] = False)",
        "segment_243": "(async) Receive a message from another agent.",
        "segment_244": "Once a message is received, this function sends a reply to the sender or stop.",
        "segment_245": "The reply can be generated automatically or entered manually by a human.",
        "segment_246": "Arguments:",
        "segment_248": "message dict or str - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).",
        "segment_250": "\"content\": content of the message, can be None.",
        "segment_251": "\"function_call\": a dictionary containing the function name and arguments. (deprecated in favor of \"tool_calls\")",
        "segment_252": "\"tool_calls\": a list of dictionaries containing the function name and arguments.",
        "segment_253": "\"role\": role of the message, can be \"assistant\", \"user\", \"function\".",
        "segment_254": "This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".",
        "segment_255": "\"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.",
        "segment_256": "\"context\" (dict): the context of the message, which will be passed to",
        "segment_257": "OpenAIWrapper.create.",
        "segment_260": "sender - sender of an Agent instance.",
        "segment_261": "request_reply bool or None - whether a reply is requested from the sender.",
        "segment_262": "If None, the value is determined by self.reply_at_receive[sender].",
        "segment_263": "silent bool or None - (Experimental) whether to print the message received.",
        "segment_265": "Raises:",
        "segment_267": "ValueError - if the message can't be converted into a valid ChatCompletion message.",
        "segment_269": "initiate_chat\u200b",
        "segment_270": "def initiate_chat(recipient: \"ConversableAgent\", clear_history: Optional[bool] = True, silent: Optional[bool] = False, cache: Optional[Cache] = None, **context)",
        "segment_271": "Initiate a chat with the recipient agent.",
        "segment_272": "Reset the consecutive auto reply counter.",
        "segment_273": "If clear_history is True, the chat history with the recipient agent will be cleared.",
        "segment_274": "generate_init_message is called to generate the initial message for the agent.",
        "segment_275": "Arguments:",
        "segment_277": "recipient - the recipient agent.",
        "segment_278": "clear_history bool - whether to clear the chat history with the agent.",
        "segment_279": "silent bool or None - (Experimental) whether to print the messages for this conversation.",
        "segment_280": "cache Cache or None - the cache client to be used for this conversation.",
        "segment_281": "**context - any context information.",
        "segment_282": "\"message\" needs to be provided if the generate_init_message method is not overridden.",
        "segment_283": "Otherwise, input() will be called to get the initial message.",
        "segment_285": "Raises:",
        "segment_287": "RuntimeError - if any async reply functions are registered and not ignored in sync chat.",
        "segment_289": "a_initiate_chat\u200b",
        "segment_290": "async def a_initiate_chat(recipient: \"ConversableAgent\", clear_history: Optional[bool] = True, silent: Optional[bool] = False, cache: Optional[Cache] = None, **context)",
        "segment_291": "(async) Initiate a chat with the recipient agent.",
        "segment_292": "Reset the consecutive auto reply counter.",
        "segment_293": "If clear_history is True, the chat history with the recipient agent will be cleared.",
        "segment_294": "a_generate_init_message is called to generate the initial message for the agent.",
        "segment_295": "Arguments:",
        "segment_297": "recipient - the recipient agent.",
        "segment_298": "clear_history bool - whether to clear the chat history with the agent.",
        "segment_299": "silent bool or None - (Experimental) whether to print the messages for this conversation.",
        "segment_300": "cache Cache or None - the cache client to be used for this conversation.",
        "segment_301": "**context - any context information.",
        "segment_302": "\"message\" needs to be provided if the a_generate_init_message method is not overridden.",
        "segment_303": "Otherwise, input() will be called to get the initial message.",
        "segment_305": "reset\u200b",
        "segment_306": "def reset()",
        "segment_307": "Reset the agent.",
        "segment_308": "stop_reply_at_receive\u200b",
        "segment_309": "def stop_reply_at_receive(sender: Optional[Agent] = None)",
        "segment_310": "Reset the reply_at_receive of the sender.",
        "segment_311": "reset_consecutive_auto_reply_counter\u200b",
        "segment_312": "def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None)",
        "segment_313": "Reset the consecutive_auto_reply_counter of the sender.",
        "segment_314": "clear_history\u200b",
        "segment_315": "def clear_history(recipient: Optional[Agent] = None, nr_messages_to_preserve: Optional[int] = None)",
        "segment_316": "Clear the chat history of the agent.",
        "segment_317": "Arguments:",
        "segment_319": "recipient - the agent with whom the chat history to clear. If None, clear the chat history with all agents.",
        "segment_320": "nr_messages_to_preserve - the number of newest messages to preserve in the chat history.",
        "segment_322": "generate_oai_reply\u200b",
        "segment_323": "def generate_oai_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_324": "Generate a reply using autogen.oai.",
        "segment_325": "a_generate_oai_reply\u200b",
        "segment_326": "async def a_generate_oai_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_327": "Generate a reply using autogen.oai asynchronously.",
        "segment_328": "generate_code_execution_reply\u200b",
        "segment_329": "def generate_code_execution_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Union[Dict, Literal[False]]] = None)",
        "segment_330": "Generate a reply using code execution.",
        "segment_331": "generate_function_call_reply\u200b",
        "segment_332": "def generate_function_call_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_333": "Generate a reply using function call.",
        "segment_334": "\"function_call\" replaced by \"tool_calls\" as of OpenAI API v1.1.0",
        "segment_335": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions",
        "segment_336": "a_generate_function_call_reply\u200b",
        "segment_337": "async def a_generate_function_call_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_338": "Generate a reply using async function call.",
        "segment_339": "\"function_call\" replaced by \"tool_calls\" as of OpenAI API v1.1.0",
        "segment_340": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-functions",
        "segment_341": "generate_tool_calls_reply\u200b",
        "segment_342": "def generate_tool_calls_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_343": "Generate a reply using tool call.",
        "segment_344": "a_generate_tool_calls_reply\u200b",
        "segment_345": "async def a_generate_tool_calls_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]]",
        "segment_346": "Generate a reply using async function call.",
        "segment_347": "check_termination_and_human_reply\u200b",
        "segment_348": "def check_termination_and_human_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, None]]",
        "segment_349": "Check if the conversation should be terminated, and if human reply is provided.",
        "segment_350": "This method checks for conditions that require the conversation to be terminated, such as reaching",
        "segment_351": "a maximum number of consecutive auto-replies or encountering a termination message. Additionally,",
        "segment_352": "it prompts for and processes human input based on the configured human input mode, which can be",
        "segment_353": "'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter",
        "segment_354": "for the conversation and prints relevant messages based on the human input received.",
        "segment_355": "Arguments:",
        "segment_357": "messages (Optional[List[Dict]]): A list of message dictionaries, representing the conversation history.",
        "segment_358": "sender (Optional[Agent]): The agent object representing the sender of the message.",
        "segment_359": "config (Optional[Any]): Configuration object, defaults to the current instance if not provided.",
        "segment_361": "Returns:",
        "segment_363": "Tuple[bool, Union[str, Dict, None]]: A tuple containing a boolean indicating if the conversation",
        "segment_364": "should be terminated, and a human reply which can be a string, a dictionary, or None.",
        "segment_366": "a_check_termination_and_human_reply\u200b",
        "segment_367": "async def a_check_termination_and_human_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, None]]",
        "segment_368": "(async) Check if the conversation should be terminated, and if human reply is provided.",
        "segment_369": "This method checks for conditions that require the conversation to be terminated, such as reaching",
        "segment_370": "a maximum number of consecutive auto-replies or encountering a termination message. Additionally,",
        "segment_371": "it prompts for and processes human input based on the configured human input mode, which can be",
        "segment_372": "'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter",
        "segment_373": "for the conversation and prints relevant messages based on the human input received.",
        "segment_374": "Arguments:",
        "segment_376": "messages (Optional[List[Dict]]): A list of message dictionaries, representing the conversation history.",
        "segment_377": "sender (Optional[Agent]): The agent object representing the sender of the message.",
        "segment_378": "config (Optional[Any]): Configuration object, defaults to the current instance if not provided.",
        "segment_380": "Returns:",
        "segment_382": "Tuple[bool, Union[str, Dict, None]]: A tuple containing a boolean indicating if the conversation",
        "segment_383": "should be terminated, and a human reply which can be a string, a dictionary, or None.",
        "segment_385": "generate_reply\u200b",
        "segment_386": "def generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_387": "Reply based on the conversation history and the sender.",
        "segment_388": "Either messages or sender must be provided.",
        "segment_389": "Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.",
        "segment_390": "Use registered auto reply functions to generate replies.",
        "segment_391": "By default, the following functions are checked in order:",
        "segment_393": "check_termination_and_human_reply",
        "segment_394": "generate_function_call_reply (deprecated in favor of tool_calls)",
        "segment_395": "generate_tool_calls_reply",
        "segment_396": "generate_code_execution_reply",
        "segment_397": "generate_oai_reply",
        "segment_398": "Every function returns a tuple (final, reply).",
        "segment_399": "When a function returns final=False, the next function will be checked.",
        "segment_400": "So by default, termination and human reply will be checked first.",
        "segment_401": "If not terminating and human reply is skipped, execute function or code and return the result.",
        "segment_402": "AI replies are generated only when no code execution is performed.",
        "segment_404": "Arguments:",
        "segment_406": "messages - a list of messages in the conversation history.",
        "segment_407": "default_reply str or dict - default reply.",
        "segment_408": "sender - sender of an Agent instance.",
        "segment_409": "exclude - a list of functions to exclude.",
        "segment_411": "Returns:",
        "segment_412": "str or dict or None: reply. None if no reply is generated.",
        "segment_413": "a_generate_reply\u200b",
        "segment_414": "async def a_generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_415": "(async) Reply based on the conversation history and the sender.",
        "segment_416": "Either messages or sender must be provided.",
        "segment_417": "Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.",
        "segment_418": "Use registered auto reply functions to generate replies.",
        "segment_419": "By default, the following functions are checked in order:",
        "segment_421": "check_termination_and_human_reply",
        "segment_422": "generate_function_call_reply",
        "segment_423": "generate_tool_calls_reply",
        "segment_424": "generate_code_execution_reply",
        "segment_425": "generate_oai_reply",
        "segment_426": "Every function returns a tuple (final, reply).",
        "segment_427": "When a function returns final=False, the next function will be checked.",
        "segment_428": "So by default, termination and human reply will be checked first.",
        "segment_429": "If not terminating and human reply is skipped, execute function or code and return the result.",
        "segment_430": "AI replies are generated only when no code execution is performed.",
        "segment_432": "Arguments:",
        "segment_434": "messages - a list of messages in the conversation history.",
        "segment_435": "default_reply str or dict - default reply.",
        "segment_436": "sender - sender of an Agent instance.",
        "segment_437": "exclude - a list of functions to exclude.",
        "segment_439": "Returns:",
        "segment_440": "str or dict or None: reply. None if no reply is generated.",
        "segment_441": "get_human_input\u200b",
        "segment_442": "def get_human_input(prompt: str) -> str",
        "segment_443": "Get human input.",
        "segment_444": "Override this method to customize the way to get human input.",
        "segment_445": "Arguments:",
        "segment_447": "prompt str - prompt for the human input.",
        "segment_449": "Returns:",
        "segment_451": "str - human input.",
        "segment_453": "a_get_human_input\u200b",
        "segment_454": "async def a_get_human_input(prompt: str) -> str",
        "segment_455": "(Async) Get human input.",
        "segment_456": "Override this method to customize the way to get human input.",
        "segment_457": "Arguments:",
        "segment_459": "prompt str - prompt for the human input.",
        "segment_461": "Returns:",
        "segment_463": "str - human input.",
        "segment_465": "run_code\u200b",
        "segment_466": "def run_code(code, **kwargs)",
        "segment_467": "Run the code and return the result.",
        "segment_468": "Override this function to modify the way to run the code.",
        "segment_469": "Arguments:",
        "segment_471": "code str - the code to be executed.",
        "segment_472": "**kwargs - other keyword arguments.",
        "segment_474": "Returns:",
        "segment_475": "A tuple of (exitcode, logs, image).",
        "segment_477": "exitcode int - the exit code of the code execution.",
        "segment_478": "logs str - the logs of the code execution.",
        "segment_479": "image str or None - the docker image used for the code execution.",
        "segment_481": "execute_code_blocks\u200b",
        "segment_482": "def execute_code_blocks(code_blocks)",
        "segment_483": "Execute the code blocks and return the result.",
        "segment_484": "execute_function\u200b",
        "segment_485": "def execute_function(func_call, verbose: bool = False) -> Tuple[bool, Dict[str, str]]",
        "segment_486": "Execute a function call and return the result.",
        "segment_487": "Override this function to modify the way to execute function and tool calls.",
        "segment_488": "Arguments:",
        "segment_490": "func_call - a dictionary extracted from openai message at \"function_call\" or \"tool_calls\" with keys \"name\" and \"arguments\".",
        "segment_492": "Returns:",
        "segment_493": "A tuple of (is_exec_success, result_dict).",
        "segment_496": "is_exec_success boolean - whether the execution is successful.",
        "segment_499": "result_dict - a dictionary with keys \"name\", \"role\", and \"content\". Value of \"role\" is \"function\".",
        "segment_500": "\"function_call\" deprecated as of OpenAI API v1.1.0",
        "segment_501": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_504": "a_execute_function\u200b",
        "segment_505": "async def a_execute_function(func_call)",
        "segment_506": "Execute an async function call and return the result.",
        "segment_507": "Override this function to modify the way async functions and tools are executed.",
        "segment_508": "Arguments:",
        "segment_510": "func_call - a dictionary extracted from openai message at key \"function_call\" or \"tool_calls\" with keys \"name\" and \"arguments\".",
        "segment_512": "Returns:",
        "segment_513": "A tuple of (is_exec_success, result_dict).",
        "segment_516": "is_exec_success boolean - whether the execution is successful.",
        "segment_519": "result_dict - a dictionary with keys \"name\", \"role\", and \"content\". Value of \"role\" is \"function\".",
        "segment_520": "\"function_call\" deprecated as of OpenAI API v1.1.0",
        "segment_521": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_524": "generate_init_message\u200b",
        "segment_525": "def generate_init_message(**context) -> Union[str, Dict]",
        "segment_526": "Generate the initial message for the agent.",
        "segment_527": "Override this function to customize the initial message based on user's request.",
        "segment_528": "If not overridden, \"message\" needs to be provided in the context.",
        "segment_529": "Arguments:",
        "segment_531": "**context - any context information, and \"message\" parameter needs to be provided.",
        "segment_532": "If message is not given, prompt for it via input()",
        "segment_534": "a_generate_init_message\u200b",
        "segment_535": "async def a_generate_init_message(**context) -> Union[str, Dict]",
        "segment_536": "Generate the initial message for the agent.",
        "segment_537": "Override this function to customize the initial message based on user's request.",
        "segment_538": "If not overridden, \"message\" needs to be provided in the context.",
        "segment_539": "Arguments:",
        "segment_541": "**context - any context information, and \"message\" parameter needs to be provided.",
        "segment_542": "If message is not given, prompt for it via input()",
        "segment_544": "register_function\u200b",
        "segment_545": "def register_function(function_map: Dict[str, Callable])",
        "segment_546": "Register functions to the agent.",
        "segment_547": "Arguments:",
        "segment_549": "function_map - a dictionary mapping function names to functions.",
        "segment_551": "update_function_signature\u200b",
        "segment_552": "def update_function_signature(func_sig: Union[str, Dict], is_remove: None)",
        "segment_553": "update a function_signature in the LLM configuration for function_call.",
        "segment_554": "Arguments:",
        "segment_557": "func_sig str or dict - description/name of the function to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat/create-functions",
        "segment_560": "is_remove - whether removing the function from llm_config with name 'func_sig'",
        "segment_561": "Deprecated as of OpenAI API v1.1.0",
        "segment_562": "See https://platform.openai.com/docs/api-reference/chat/create#chat-create-function_call",
        "segment_565": "update_tool_signature\u200b",
        "segment_566": "def update_tool_signature(tool_sig: Union[str, Dict], is_remove: None)",
        "segment_567": "update a tool_signature in the LLM configuration for tool_call.",
        "segment_568": "Arguments:",
        "segment_570": "tool_sig str or dict - description/name of the tool to update/remove to the model. See: https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools",
        "segment_571": "is_remove - whether removing the tool from llm_config with name 'tool_sig'",
        "segment_573": "can_execute_function\u200b",
        "segment_574": "def can_execute_function(name: Union[List[str], str]) -> bool",
        "segment_575": "Whether the agent can execute the function.",
        "segment_576": "function_map\u200b",
        "segment_577": "@propertydef function_map() -> Dict[str, Callable]",
        "segment_578": "Return the function map.",
        "segment_579": "register_for_llm\u200b",
        "segment_580": "def register_for_llm( *, name: Optional[str] = None, description: Optional[str] = None, api_style: Literal[\"function\", \"tool\"] = \"tool\") -> Callable[[F], F]",
        "segment_581": "Decorator factory for registering a function to be used by an agent.",
        "segment_582": "It's return value is used to decorate a function to be registered to the agent. The function uses type hints to",
        "segment_583": "specify the arguments and return type. The function name is used as the default name for the function,",
        "segment_584": "but a custom name can be provided. The function description is used to describe the function in the",
        "segment_585": "agent's configuration.",
        "segment_586": "Arguments:",
        "segment_587": "name (optional(str)): name of the function. If None, the function name will be used (default: None).",
        "segment_588": "description (optional(str)): description of the function (default: None). It is mandatory",
        "segment_589": "for the initial decorator, but the following ones can omit it.",
        "segment_591": "api_style - (literal): the API style for function call.",
        "segment_592": "For Azure OpenAI API, use version 2023-12-01-preview or later.",
        "segment_593": "\"function\" style will be deprecated. For earlier version use",
        "segment_594": "\"function\" if \"tool\" doesn't work.",
        "segment_595": "See Azure OpenAI documentation for details.",
        "segment_597": "Returns:",
        "segment_598": "The decorator for registering a function to be used by an agent.",
        "segment_599": "Examples:",
        "segment_600": "```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description=\"This is a very useful function\")def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14) -> str: return a + str(b * c)```",
        "segment_601": "For Azure OpenAI versions prior to 2023-12-01-preview, set api_style",
        "segment_602": "to \"function\" if \"tool\" doesn't work:",
        "segment_603": "@agent2.register_for_llm(api_style=\"function\") def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14) -> str: return a + str(b * c)",
        "segment_604": "register_for_execution\u200b",
        "segment_605": "def register_for_execution(name: Optional[str] = None) -> Callable[[F], F]",
        "segment_606": "Decorator factory for registering a function to be executed by an agent.",
        "segment_607": "It's return value is used to decorate a function to be registered to the agent.",
        "segment_608": "Arguments:",
        "segment_609": "name (optional(str)): name of the function. If None, the function name will be used (default: None).",
        "segment_610": "Returns:",
        "segment_611": "The decorator for registering a function to be used by an agent.",
        "segment_612": "Examples:",
        "segment_613": "```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description=\"This is a very useful function\")def my_function(a: Annotated[str, \"description of a parameter\"] = \"a\", b: int, c=3.14): return a + str(b * c)```",
        "segment_614": "register_model_client\u200b",
        "segment_615": "def register_model_client(model_client_cls: ModelClient, **kwargs)",
        "segment_616": "Register a model client.",
        "segment_617": "Arguments:",
        "segment_619": "model_client_cls - A custom client class that follows the Client interface",
        "segment_620": "**kwargs - The kwargs for the custom client class to be initialized with",
        "segment_622": "register_hook\u200b",
        "segment_623": "def register_hook(hookable_method: Callable, hook: Callable)",
        "segment_624": "Registers a hook to be called by a hookable method, in order to add a capability to the agent.",
        "segment_625": "Registered hooks are kept in lists (one per hookable method), and are called in their order of registration.",
        "segment_626": "Arguments:",
        "segment_628": "hookable_method - A hookable method implemented by ConversableAgent.",
        "segment_629": "hook - A method implemented by a subclass of AgentCapability.",
        "segment_631": "process_last_message\u200b",
        "segment_632": "def process_last_message(messages)",
        "segment_633": "Calls any registered capability hooks to use and potentially modify the text of the last message,",
        "segment_634": "as long as the last message is not a function call or exit command.",
        "segment_635": "print_usage_summary\u200b",
        "segment_636": "def print_usage_summary( mode: Union[str, List[str]] = [\"actual\", \"total\"]) -> None",
        "segment_637": "Print the usage summary.",
        "segment_638": "get_actual_usage\u200b",
        "segment_639": "def get_actual_usage() -> Union[None, Dict[str, int]]",
        "segment_640": "Get the actual usage summary.",
        "segment_641": "get_total_usage\u200b",
        "segment_642": "def get_total_usage() -> Union[None, Dict[str, int]]",
        "segment_643": "Get the total usage summary.",
        "segment_644": "register_function\u200b",
        "segment_645": "def register_function(f: Callable[..., Any], *, caller: ConversableAgent, executor: ConversableAgent, name: Optional[str] = None, description: str) -> None",
        "segment_646": "Register a function to be proposed by an agent and executed for an executor.",
        "segment_647": "This function can be used instead of function decorators @ConversationAgent.register_for_llm and",
        "segment_648": "@ConversationAgent.register_for_execution.",
        "segment_649": "Arguments:",
        "segment_651": "f - the function to be registered.",
        "segment_652": "caller - the agent calling the function, typically an instance of ConversableAgent.",
        "segment_653": "executor - the agent executing the function, typically an instance of UserProxy.",
        "segment_654": "name - name of the function. If None, the function name will be used (default: None).",
        "segment_655": "description - description of the function. The description is used by LLM to decode whether the function",
        "segment_656": "is called. Make sure the description is properly describing what the function does or it might not be",
        "segment_657": "called by LLM when needed.",
        "segment_658": "Edit this pagePreviousassistant_agentNextgroupchatConversableAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "autogen.OpenAIWrapper provides enhanced LLM inference for openai>=1.",
        "segment_2": "autogen.Completion is a drop-in replacement of openai.Completion and openai.ChatCompletion for enhanced LLM inference using openai<1.",
        "segment_3": "There are a number of benefits of using autogen to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on.",
        "segment_4": "Tune Inference Parameters (for openai<1)\u200b",
        "segment_5": "Find a list of examples in this page: Tune Inference Parameters Examples",
        "segment_6": "Choices to optimize\u200b",
        "segment_7": "The cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined. From the perspective of an application builder using foundation models, the use case is to maximize the utility of the generated text under an inference budget constraint (e.g., measured by the average dollar cost needed to solve a coding problem). This can be achieved by optimizing the hyperparameters of the inference,",
        "segment_8": "which can significantly affect both the utility and the cost of the generated text.",
        "segment_9": "The tunable hyperparameters include:",
        "segment_11": "model - this is a required input, specifying the model ID to use.",
        "segment_12": "prompt/messages - the input prompt/messages to the model, which provides the context for the text generation task.",
        "segment_13": "max_tokens - the maximum number of tokens (words or word pieces) to generate in the output.",
        "segment_14": "temperature - a value between 0 and 1 that controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable text.",
        "segment_15": "top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation. A lower top_p value will make it more likely to generate text based on the most likely tokens, while a higher value will allow the model to explore a wider range of possible tokens.",
        "segment_16": "n - the number of responses to generate for a given prompt. Generating multiple responses can provide more diverse and potentially more useful output, but it also increases the cost of the request.",
        "segment_17": "stop - a list of strings that, when encountered in the generated text, will cause the generation to stop. This can be used to control the length or the validity of the output.",
        "segment_18": "presence_penalty, frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrases in the generated text.",
        "segment_19": "best_of - the number of responses to generate server-side when selecting the \"best\" (the one with the highest log probability per token) response for a given prompt.",
        "segment_21": "The cost and utility of text generation are intertwined with the joint effect of these hyperparameters.",
        "segment_22": "There are also complex interactions among subsets of the hyperparameters. For example,",
        "segment_23": "the temperature and top_p are not recommended to be altered from their default values together because they both control the randomness of the generated text, and changing both at the same time can result in conflicting effects; n and best_of are rarely tuned together because if the application can process multiple outputs, filtering on the server side causes unnecessary information loss; both n and max_tokens will affect the total number of tokens generated, which in turn will affect the cost of the request.",
        "segment_24": "These interactions and trade-offs make it difficult to manually determine the optimal hyperparameter settings for a given text generation task.",
        "segment_25": "Do the choices matter? Check this blogpost to find example tuning results about gpt-3.5-turbo and gpt-4.",
        "segment_26": "With AutoGen, the tuning can be performed with the following information:",
        "segment_28": "Validation data.",
        "segment_29": "Evaluation function.",
        "segment_30": "Metric to optimize.",
        "segment_31": "Search space.",
        "segment_32": "Budgets: inference and optimization respectively.",
        "segment_34": "Validation data\u200b",
        "segment_35": "Collect a diverse set of instances. They can be stored in an iterable of dicts. For example, each instance dict can contain \"problem\" as a key and the description str of a math problem as the value; and \"solution\" as a key and the solution str as the value.",
        "segment_36": "Evaluation function\u200b",
        "segment_37": "The evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. For example,",
        "segment_38": "def eval_math_responses(responses: List[str], solution: str, **args) -> Dict: # select a response from the list of responses answer = voted_answer(responses) # check whether the answer is correct return {\"success\": is_equivalent(answer, solution)}",
        "segment_39": "autogen.code_utils and autogen.math_utils offer some example evaluation functions for code generation and math problem solving.",
        "segment_40": "Metric to optimize\u200b",
        "segment_41": "The metric to optimize is usually an aggregated metric over all the tuning data instances. For example, users can specify \"success\" as the metric and \"max\" as the optimization mode. By default, the aggregation function is taking the average. Users can provide a customized aggregation function if needed.",
        "segment_42": "Search space\u200b",
        "segment_43": "Users can specify the (optional) search range for each hyperparameter.",
        "segment_45": "model. Either a constant str, or multiple choices specified by flaml.tune.choice.",
        "segment_46": "prompt/messages. Prompt is either a str or a list of strs, of the prompt templates. messages is a list of dicts or a list of lists, of the message templates.",
        "segment_47": "Each prompt/message template will be formatted with each data instance. For example, the prompt template can be:",
        "segment_48": "\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\"",
        "segment_49": "And {problem} will be replaced by the \"problem\" field of each data instance.",
        "segment_50": "max_tokens, n, best_of. They can be constants, or specified by flaml.tune.randint, flaml.tune.qrandint, flaml.tune.lograndint or flaml.qlograndint. By default, max_tokens is searched in [50, 1000); n is searched in [1, 100); and best_of is fixed to 1.",
        "segment_51": "stop. It can be a str or a list of strs, or a list of lists of strs or None. Default is None.",
        "segment_52": "temperature or top_p. One of them can be specified as a constant or by flaml.tune.uniform or flaml.tune.loguniform etc.",
        "segment_53": "Please don't provide both. By default, each configuration will choose either a temperature or a top_p in [0, 1] uniformly.",
        "segment_54": "presence_penalty, frequency_penalty. They can be constants or specified by flaml.tune.uniform etc. Not tuned by default.",
        "segment_56": "Budgets\u200b",
        "segment_57": "One can specify an inference budget and an optimization budget.",
        "segment_58": "The inference budget refers to the average inference cost per data instance.",
        "segment_59": "The optimization budget refers to the total budget allowed in the tuning process. Both are measured by dollars and follow the price per 1000 tokens.",
        "segment_60": "Perform tuning\u200b",
        "segment_61": "Now, you can use autogen.Completion.tune for tuning. For example,",
        "segment_62": "import autogenconfig, analysis = autogen.Completion.tune( data=tune_data, metric=\"success\", mode=\"max\", eval_func=eval_func, inference_budget=0.05, optimization_budget=3, num_samples=-1,)",
        "segment_63": "num_samples is the number of configurations to sample. -1 means unlimited (until optimization budget is exhausted).",
        "segment_64": "The returned config contains the optimized configuration and analysis contains an ExperimentAnalysis object for all the tried configurations and results.",
        "segment_65": "The tuned config can be used to perform inference.",
        "segment_66": "API unification\u200b",
        "segment_67": "autogen.OpenAIWrapper.create() can be used to create completions for both chat and non-chat models, and both OpenAI API and Azure OpenAI API.",
        "segment_68": "from autogen import OpenAIWrapper# OpenAI endpointclient = OpenAIWrapper()# ChatCompletionresponse = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}], model=\"gpt-3.5-turbo\")# extract the response textprint(client.extract_text_or_completion_object(response))# get cost of this completionprint(response.cost)# Azure OpenAI endpointclient = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type=\"azure\")# Completionresponse = client.create(prompt=\"2+2=\", model=\"gpt-3.5-turbo-instruct\")# extract the response textprint(client.extract_text_or_completion_object(response))",
        "segment_69": "For local LLMs, one can spin up an endpoint using a package like FastChat, and then use the same API to send a request. See here for examples on how to make inference with local LLMs.",
        "segment_70": "For custom model clients, one can register the client with autogen.OpenAIWrapper.register_model_client and then use the same API to send a request. See here for examples on how to make inference with custom model clients.",
        "segment_71": "Usage Summary\u200b",
        "segment_72": "The OpenAIWrapper from autogen tracks token counts and costs of your API calls. Use the create() method to initiate requests and print_usage_summary() to retrieve a detailed usage report, including total cost and token usage for both cached and actual requests.",
        "segment_74": "mode=[\"actual\", \"total\"] (default): print usage summary for all completions and non-caching completions.",
        "segment_75": "mode='actual': only print non-cached usage.",
        "segment_76": "mode='total': only print all usage (including cache).",
        "segment_78": "Reset your session's usage data with clear_usage_summary() when needed. View Notebook",
        "segment_79": "Example usage:",
        "segment_80": "from autogen import OpenAIWrapperclient = OpenAIWrapper()client.create(messages=[{\"role\": \"user\", \"content\": \"Python learning tips.\"}], model=\"gpt-3.5-turbo\")client.print_usage_summary() # Display usageclient.clear_usage_summary() # Reset usage data",
        "segment_81": "Sample output:",
        "segment_82": "Usage summary excluding cached usage:Total cost: 0.00015* Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83Usage summary including cached usage:Total cost: 0.00027* Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150",
        "segment_83": "Note: if using a custom model client (see here for details) and if usage summary is not implemented, then the usage summary will not be available.",
        "segment_84": "Caching\u200b",
        "segment_85": "API call results are cached locally and reused when the same request is issued.",
        "segment_86": "This is useful when repeating or continuing experiments for reproducibility and cost saving.",
        "segment_87": "Starting version 0.2.8, a configurable context manager allows you to easily configure",
        "segment_88": "the cache, using either DiskCache or Redis.",
        "segment_89": "All OpenAIWrapper created inside the context manager can use the same cache",
        "segment_90": "through the constructor.",
        "segment_91": "from autogen import Cachewith Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache: client = OpenAIWrapper(..., cache=cache) client.create(...)with Cache.disk() as cache: client = OpenAIWrapper(..., cache=cache) client.create(...)",
        "segment_92": "You can also set a cache directly in the create() method.",
        "segment_93": "client = OpenAIWrapper(...)with Cache.disk() as cache: client.create(..., cache=cache)",
        "segment_94": "You can vary the cache_seed parameter to get different LLM output while",
        "segment_95": "still using cache.",
        "segment_96": "# Setting the cache_seed to 1 will use a different cache from the default one# and you will see different output.with Cache.disk(cache_seed=1) as cache: client.create(..., cache=cache)",
        "segment_97": "By default DiskCache uses .cache for storage. To change the cache directory,",
        "segment_98": "set cache_path_root:",
        "segment_99": "with Cache.disk(cache_path_root=\"/tmp/autogen_cache\") as cache: client.create(..., cache=cache)",
        "segment_100": "Turnning off cache\u200b",
        "segment_101": "For backward compatibility, DiskCache is always enabled by default",
        "segment_102": "with cache_seed set to 41. To fully disable it, set cache_seed to None.",
        "segment_103": "# Turn off cache in constructor,client = OpenAIWrapper(..., cache_seed=None)# or directly in create().client.create(..., cache_seed=None)",
        "segment_104": "Difference between cache_seed and openai's seed parameter\u200b",
        "segment_105": "openai v1.1 introduces a new param seed.",
        "segment_106": "The differences between autogen's cache_seed and openai's seed:",
        "segment_107": "- autogen uses local disk cache to guarantee the exactly same output is produced",
        "segment_108": "for the same input and when cache is hit, no openai api call will be made.",
        "segment_109": "- openai's seed is a best-effort deterministic sampling with no guarantee",
        "segment_110": "of determinism. When using openai's seed with cache_seed set to None,",
        "segment_111": "even for the same input, an openai api call will be made and there is",
        "segment_112": "no guarantee for getting exactly the same output.",
        "segment_113": "Error handling\u200b",
        "segment_114": "Runtime error\u200b",
        "segment_115": "One can pass a list of configurations of different models/endpoints to mitigate the rate limits and other runtime error. For example,",
        "segment_116": "client = OpenAIWrapper( config_list=[ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-08-01-preview\", }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"base_url\": \"https://api.openai.com/v1\", }, { \"model\": \"llama2-chat-7B\", \"base_url\": \"http://127.0.0.1:8080\", }, { \"model\": \"microsoft/phi-2\", \"model_client_cls\": \"CustomModelClient\" } ],)",
        "segment_117": "client.create() will try querying Azure OpenAI gpt-4, OpenAI gpt-3.5-turbo, a locally hosted llama2-chat-7B, and phi-2 using a custom model client class named CustomModelClient, one by one,",
        "segment_118": "until a valid result is returned. This can speed up the development process where the rate limit is a bottleneck. An error will be raised if the last choice fails. So make sure the last choice in the list has the best availability.",
        "segment_119": "For convenience, we provide a number of utility functions to load config lists.",
        "segment_121": "get_config_list: Generates configurations for API calls, primarily from provided API keys.",
        "segment_122": "config_list_openai_aoai: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files.",
        "segment_123": "config_list_from_json: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria.",
        "segment_124": "config_list_from_models: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration.",
        "segment_125": "config_list_from_dotenv: Constructs a configuration list from a .env file, offering a consolidated way to manage multiple API configurations and keys from a single file.",
        "segment_127": "We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints.",
        "segment_128": "Logic error\u200b",
        "segment_129": "Another type of error is that the returned response does not satisfy a requirement. For example, if the response is required to be a valid json string, one would like to filter the responses that are not. This can be achieved by providing a list of configurations and a filter function. For example,",
        "segment_130": "def valid_json_filter(response, **_): for text in OpenAIWrapper.extract_text_or_completion_object(response): try: json.loads(text) return True except ValueError: pass return Falseclient = OpenAIWrapper( config_list=[{\"model\": \"text-ada-001\"}, {\"model\": \"gpt-3.5-turbo-instruct\"}, {\"model\": \"text-davinci-003\"}],)response = client.create( prompt=\"How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.\", filter_func=valid_json_filter,)",
        "segment_131": "The example above will try to use text-ada-001, gpt-3.5-turbo-instruct, and text-davinci-003 iteratively, until a valid json string is returned or the last config is used. One can also repeat the same model in the list for multiple times (with different seeds) to try one model multiple times for increasing the robustness of the final response.",
        "segment_132": "Advanced use case: Check this blogpost to find how to improve GPT-4's coding performance from 68% to 90% while reducing the inference cost.",
        "segment_133": "Templating\u200b",
        "segment_134": "If the provided prompt or message is a template, it will be automatically materialized with a given context. For example,",
        "segment_135": "response = client.create( context={\"problem\": \"How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?\"}, prompt=\"{problem} Solve the problem carefully.\", allow_format_str_template=True, **config)",
        "segment_136": "A template is either a format str, like the example above, or a function which produces a str from several input fields, like the example below.",
        "segment_137": "def content(turn, context): return \"\\n\".join( [ context[f\"user_message_{turn}\"], context[f\"external_info_{turn}\"] ] )messages = [ { \"role\": \"system\", \"content\": \"You are a teaching assistant of math.\", }, { \"role\": \"user\", \"content\": partial(content, turn=0), },]context = { \"user_message_0\": \"Could you explain the solution to Problem 1?\", \"external_info_0\": \"Problem 1: ...\",}response = client.create(context=context, messages=messages, **config)messages.append( { \"role\": \"assistant\", \"content\": client.extract_text(response)[0] })messages.append( { \"role\": \"user\", \"content\": partial(content, turn=1), },)context.append( { \"user_message_1\": \"Why can't we apply Theorem 1 to Equation (2)?\", \"external_info_1\": \"Theorem 1: ...\", })response = client.create(context=context, messages=messages, **config)",
        "segment_138": "Logging (for openai<1)\u200b",
        "segment_139": "When debugging or diagnosing an LLM-based system, it is often convenient to log the API calls and analyze them. autogen.Completion and autogen.ChatCompletion offer an easy way to collect the API call histories. For example, to log the chat histories, simply run:",
        "segment_140": "autogen.ChatCompletion.start_logging()",
        "segment_141": "The API calls made after this will be automatically logged. They can be retrieved at any time by:",
        "segment_142": "autogen.ChatCompletion.logged_history",
        "segment_143": "There is a function that can be used to print usage summary (total cost, and token count usage from each model):",
        "segment_144": "autogen.ChatCompletion.print_usage_summary()",
        "segment_145": "To stop logging, use",
        "segment_146": "autogen.ChatCompletion.stop_logging()",
        "segment_147": "If one would like to append the history to an existing dict, pass the dict like:",
        "segment_148": "autogen.ChatCompletion.start_logging(history_dict=existing_history_dict)",
        "segment_149": "By default, the counter of API calls will be reset at start_logging(). If no reset is desired, set reset_counter=False.",
        "segment_150": "There are two types of logging formats: compact logging and individual API call logging. The default format is compact.",
        "segment_151": "Set compact=False in start_logging() to switch.",
        "segment_153": "Example of a history dict with compact logging.",
        "segment_155": "{ \"\"\" [ { 'role': 'system', 'content': system_message, }, { 'role': 'user', 'content': user_message_1, }, { 'role': 'assistant', 'content': assistant_message_1, }, { 'role': 'user', 'content': user_message_2, }, { 'role': 'assistant', 'content': assistant_message_2, }, ]\"\"\": { \"created_at\": [0, 1], \"cost\": [0.1, 0.2], }}",
        "segment_157": "Example of a history dict with individual API call logging.",
        "segment_159": "{ 0: { \"request\": { \"messages\": [ { \"role\": \"system\", \"content\": system_message, }, { \"role\": \"user\", \"content\": user_message_1, } ], ... # other parameters in the request }, \"response\": { \"choices\": [ \"messages\": { \"role\": \"assistant\", \"content\": assistant_message_1, }, ], ... # other fields in the response } }, 1: { \"request\": { \"messages\": [ { \"role\": \"system\", \"content\": system_message, }, { \"role\": \"user\", \"content\": user_message_1, }, { \"role\": \"assistant\", \"content\": assistant_message_1, }, { \"role\": \"user\", \"content\": user_message_2, }, ], ... # other parameters in the request }, \"response\": { \"choices\": [ \"messages\": { \"role\": \"assistant\", \"content\": assistant_message_2, }, ], ... # other fields in the response } },}",
        "segment_161": "Example of printing for usage summary",
        "segment_163": "Total cost: Token count summary for model : prompt_tokens: , completion_tokens: , total_tokens:",
        "segment_164": "It can be seen that the individual API call history contains redundant information of the conversation. For a long conversation the degree of redundancy is high.",
        "segment_165": "The compact history is more efficient and the individual API call history contains more details.Edit this pagePreviousMulti-agent Conversation FrameworkNextContributingTune Inference Parameters (for openai<1)Choices to optimizeValidation dataEvaluation functionMetric to optimizeSearch spaceBudgetsPerform tuningAPI unificationUsage SummaryCachingTurnning off cacheDifference between cache_seed and openai's seed parameterError handlingRuntime errorLogic errorTemplatingLogging (for openai<1)CommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_2": "Set your API endpoints",
        "segment_4": "Use the constructed configuration list in agents",
        "segment_5": "Unexpected keyword argument 'base_url'",
        "segment_6": "How does an agent decide which model to pick out of the list?",
        "segment_7": "Can I use non-OpenAI models?",
        "segment_10": "Handle Rate Limit Error and Timeout Error",
        "segment_11": "How to continue a finished conversation",
        "segment_12": "How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?",
        "segment_13": "Why is code not saved as file?",
        "segment_14": "Code execution",
        "segment_16": "Enable Python 3 docker image",
        "segment_17": "Agents keep thanking each other when using gpt-3.5-turbo",
        "segment_20": "ChromaDB fails in codespaces because of old version of sqlite3",
        "segment_21": "How to register a reply function",
        "segment_22": "How to get last message?",
        "segment_23": "How to get each agent message?",
        "segment_24": "When using autogen docker, is it always necessary to reinstall modules?",
        "segment_25": "Agents are throwing due to docker not running, how can I resolve this?",
        "segment_27": "Set your API endpoints\u200b",
        "segment_28": "Autogen relies on 3rd party API endpoints for LLM inference. It works great with OpenAI and Azure OpenAI models, and can work with any model (self-hosted or local) that is accessible through an inference server compatible with OpenAI Chat Completions API.",
        "segment_29": "An agent requires a list of configuration dictionaries for setting up model endpoints. Each configuration dictionary can contain the following keys:",
        "segment_31": "model (str): Required. The identifier of the model to be used, such as 'gpt-4', 'gpt-3.5-turbo', or 'llama-7B'.",
        "segment_32": "api_key (str): Optional. The API key required for authenticating requests to the model's API endpoint.",
        "segment_33": "api_type (str): Optional. The type of API service being used. This could be 'azure' for Azure Cognitive Services, 'openai' for OpenAI, or other custom types.",
        "segment_34": "base_url (str): Optional. The base URL of the API endpoint. This is the root address where API calls are directed.",
        "segment_35": "api_version (str): Optional. The version of the Azure API you wish to use",
        "segment_37": "For example:",
        "segment_38": "config_list = [ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-12-01-preview\", }, { \"model\": \"llama-7B\", \"base_url\": \"http://127.0.0.1:8080\", \"api_type\": \"openai\", }]",
        "segment_39": "In autogen module there are multiple helper functions allowing to construct configurations using different sources:",
        "segment_41": "get_config_list: Generates configurations for API calls, primarily from provided API keys.",
        "segment_42": "config_list_openai_aoai: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files.",
        "segment_43": "config_list_from_json: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria.",
        "segment_44": "config_list_from_models: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration.",
        "segment_45": "config_list_from_dotenv: Constructs a configuration list from a .env file, offering a consolidated way to manage multiple API configurations and keys from a single file.",
        "segment_47": "We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints.",
        "segment_48": "Use the constructed configuration list in agents\u200b",
        "segment_49": "Make sure the \"config_list\" is included in the llm_config in the constructor of the LLM-based agent. For example,",
        "segment_50": "assistant = autogen.AssistantAgent( name=\"assistant\", llm_config={\"config_list\": config_list})",
        "segment_51": "The llm_config is used in the create function for LLM inference.",
        "segment_52": "When llm_config is not provided, the agent will rely on other openai settings such as openai.api_key or the environment variable OPENAI_API_KEY, which can also work when you'd like to use a single endpoint.",
        "segment_53": "You can also explicitly specify that by:",
        "segment_54": "assistant = autogen.AssistantAgent(name=\"assistant\", llm_config={\"api_key\": ...})",
        "segment_55": "How does an agent decide which model to pick out of the list?\u200b",
        "segment_56": "An agent uses the very first model available in the \"config_list\" and makes LLM calls against this model. If the model fail (e.g. API throttling) the agent will retry the request against the 2nd model and so on until prompt completion is received (or throws an error if none of the models successfully completes the request). There's no implicit/hidden logic inside agents that is used to pick \"the best model for the task\". It is developers responsibility to pick the right models and use them with agents.",
        "segment_57": "Besides throttling/rotating models the 'config_list' can be useful for:",
        "segment_59": "Having a single global list of models and filtering it based on certain keys (e.g. name, tag) in order to pass select models into a certain agent (e.g. use cheaper GPT 3.5 for agents solving easier tasks)",
        "segment_60": "Using more advanced features for special purposes related to inference, such as filter_func with OpenAIWrapper or inference optimization",
        "segment_62": "Unexpected keyword argument 'base_url'\u200b",
        "segment_63": "In version >=1, OpenAI renamed their api_base parameter to base_url. So for older versions, use api_base but for newer versions use base_url.",
        "segment_64": "Can I use non-OpenAI models?\u200b",
        "segment_65": "Yes. You currently have two options:",
        "segment_67": "Autogen can work with any API endpoint which complies with OpenAI-compatible RESTful APIs - e.g. serving local LLM via FastChat or LM Studio. Please check https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs for an example.",
        "segment_68": "You can supply your own custom model implementation and use it with Autogen. Please check https://microsoft.github.io/autogen/blog/2024/01/26/Custom-Models for more information.",
        "segment_70": "Handle Rate Limit Error and Timeout Error\u200b",
        "segment_71": "You can set max_retries to handle rate limit error. And you can set timeout to handle timeout error. They can all be specified in llm_config for an agent, which will be used in the OpenAI client for LLM inference. They can be set differently for different clients if they are set in the config_list.",
        "segment_73": "max_retries (int): the total number of times allowed for retrying failed requests for a single client.",
        "segment_74": "timeout (int): the timeout (in seconds) for a single client.",
        "segment_76": "Please refer to the documentation for more info.",
        "segment_77": "How to continue a finished conversation\u200b",
        "segment_78": "When you call initiate_chat the conversation restarts by default. You can use send or initiate_chat(clear_history=False) to continue the conversation.",
        "segment_79": "How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?\u200b",
        "segment_80": "Each agent can be customized. You can use LLMs, tools, or humans behind each agent. If you use an LLM for an agent, use the one best suited for its role. There is no limit of the number of agents, but start from a small number like 2, 3. The more capable is the LLM and the fewer roles you need, the fewer agents you need.",
        "segment_81": "The default user proxy agent doesn't use LLM. If you'd like to use an LLM in UserProxyAgent, the use case could be to simulate user's behavior.",
        "segment_82": "The default assistant agent is instructed to use both coding and language skills. It doesn't have to do coding, depending on the tasks. And you can customize the system message. So if you want to use it for coding, use a model that's good at coding.",
        "segment_83": "Why is code not saved as file?\u200b",
        "segment_84": "If you are using a custom system message for the coding agent, please include something like:",
        "segment_85": "If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line.",
        "segment_86": "in the system message. This line is in the default system message of the AssistantAgent.",
        "segment_87": "If the # filename doesn't appear in the suggested code still, consider adding explicit instructions such as \"save the code to disk\" in the initial user message in initiate_chat.",
        "segment_88": "The AssistantAgent doesn't save all the code by default, because there are cases in which one would just like to finish a task without saving the code.",
        "segment_89": "Code execution\u200b",
        "segment_90": "We strongly recommend using docker to execute code. There are two ways to use docker:",
        "segment_92": "Run AutoGen in a docker container. For example, when developing in GitHub codespace, AutoGen runs in a docker container. If you are not developing in Github codespace, follow instructions here to install and run AutoGen in docker.",
        "segment_93": "Run AutoGen outside of a docker, while performing code execution with a docker container. For this option, make sure docker is up and running. If you want to run the code locally (not recommended) then use_docker can be set to False in code_execution_config for each code-execution agent, or set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_95": "Enable Python 3 docker image\u200b",
        "segment_96": "You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.:",
        "segment_97": "user_proxy = autogen.UserProxyAgent( name=\"agent\", human_input_mode=\"TERMINATE\", max_consecutive_auto_reply=10, code_execution_config={\"work_dir\":\"_output\", \"use_docker\":\"python:3\"}, llm_config=llm_config, system_message=\"\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")",
        "segment_98": "If you have problems with agents running pip install or get errors similar to Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'), you can choose 'python:3' as image as shown in the code example above and that should solve the problem.",
        "segment_99": "Agents keep thanking each other when using gpt-3.5-turbo\u200b",
        "segment_100": "When using gpt-3.5-turbo you may often encounter agents going into a \"gratitude loop\", meaning when they complete a task they will begin congratulating and thanking each other in a continuous loop. This is a limitation in the performance of gpt-3.5-turbo, in contrast to gpt-4 which has no problem remembering instructions. This can hinder the experimentation experience when trying to test out your own use case with cheaper models.",
        "segment_101": "A workaround is to add an additional termination notice to the prompt. This acts a \"little nudge\" for the LLM to remember that they need to terminate the conversation when their task is complete. You can do this by appending a string such as the following to your user input string:",
        "segment_102": "prompt = \"Some user query\"termination_notice = ( '\\n\\nDo not show appreciation in your responses, say only what is necessary. ' 'if \"Thank you\" or \"You\\'re welcome\" are said in the conversation, then say TERMINATE ' 'to indicate the conversation is finished and this is your last message.')prompt += termination_notice",
        "segment_103": "Note: This workaround gets the job done around 90% of the time, but there are occurrences where the LLM still forgets to terminate the conversation.",
        "segment_104": "ChromaDB fails in codespaces because of old version of sqlite3\u200b",
        "segment_105": "(from issue #251)",
        "segment_106": "Code examples that use chromadb (like retrieval) fail in codespaces due to a sqlite3 requirement.",
        "segment_107": ">>> import chromadbTraceback (most recent call last): File \"\", line 1, in File \"/home/vscode/.local/lib/python3.10/site-packages/chromadb/__init__.py\", line 69, in raise RuntimeError(RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade.",
        "segment_108": "Workaround:",
        "segment_110": "pip install pysqlite3-binary",
        "segment_111": "mkdir /home/vscode/.local/lib/python3.10/site-packages/google/colab",
        "segment_113": "Explanation: Per this gist, linked from the official chromadb docs, adding this folder triggers chromadb to use pysqlite3 instead of the default.",
        "segment_114": "How to register a reply function\u200b",
        "segment_115": "(from issue #478)",
        "segment_116": "See here https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#register_reply",
        "segment_117": "For example, you can register a reply function that gets called when generate_reply is called for an agent.",
        "segment_118": "def print_messages(recipient, messages, sender, config): if \"callback\" in config and config[\"callback\"] is not None: callback = config[\"callback\"] callback(sender, recipient, messages[-1]) print(f\"Messages sent to: {recipient.name} | num messages: {len(messages)}\") return False, None # required to ensure the agent communication flow continuesuser_proxy.register_reply( [autogen.Agent, None], reply_func=print_messages, config={\"callback\": None},)assistant.register_reply( [autogen.Agent, None], reply_func=print_messages, config={\"callback\": None},)",
        "segment_119": "In the above, we register a print_messages function that is called each time the agent's generate_reply is triggered after receiving a message.",
        "segment_120": "How to get last message ?\u200b",
        "segment_121": "Refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#last_message",
        "segment_122": "How to get each agent message ?\u200b",
        "segment_123": "Please refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#chat_messages",
        "segment_124": "When using autogen docker, is it always necessary to reinstall modules?\u200b",
        "segment_125": "The \"use_docker\" arg in an agent's code_execution_config will be set to the name of the image containing the change after execution, when the conversation finishes.",
        "segment_126": "You can save that image name. For a new conversation, you can set \"use_docker\" to the saved name of the image to start execution there.",
        "segment_127": "Database locked error\u200b",
        "segment_128": "When using VMs such as Azure Machine Learning compute instances,",
        "segment_129": "you may encounter a \"database locked error\". This is because the",
        "segment_130": "LLM cache",
        "segment_131": "is trying to write to a location that the application does not have access to.",
        "segment_132": "You can set the cache_path_root to a location where the application has access.",
        "segment_133": "For example,",
        "segment_134": "from autogen import Cachewith Cache.disk(cache_path_root=\"/tmp/.cache\") as cache: agent_a.initate_chat(agent_b, ..., cache=cache)",
        "segment_135": "You can also use Redis cache instead of disk cache. For example,",
        "segment_136": "from autogen import Cachewith Cache.redis(redis_url=...) as cache: agent_a.initate_chat(agent_b, ..., cache=cache)",
        "segment_137": "You can also disable the cache. See here for details.",
        "segment_138": "Agents are throwing due to docker not running, how can I resolve this?\u200b",
        "segment_139": "If running AutoGen locally the default for agents who execute code is for them to try and perform code execution within a docker container. If docker is not running, this will cause the agent to throw an error. To resolve this you have some options.",
        "segment_140": "If you want to disable code execution entirely\u200b",
        "segment_142": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_144": "user_proxy = autogen.UserProxyAgent( name=\"agent\", llm_config=llm_config, code_execution_config=False)",
        "segment_145": "If you want to run code execution in docker\u200b",
        "segment_147": "Recommended: Make sure docker is up and running.",
        "segment_149": "If you want to run code execution locally\u200b",
        "segment_151": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_152": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_154": "E.g.:",
        "segment_155": "user_proxy = autogen.UserProxyAgent( name=\"agent\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})Edit this pageSet your API endpointsUse the constructed configuration list in agentsHow does an agent decide which model to pick out of the list?Unexpected keyword argument 'base_url'Can I use non-OpenAI models?Handle Rate Limit Error and Timeout ErrorHow to continue a finished conversationHow do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?Why is code not saved as file?Code executionEnable Python 3 docker imageAgents keep thanking each other when using gpt-3.5-turboChromaDB fails in codespaces because of old version of sqlite3How to register a reply functionHow to get last message ?How to get each agent message ?When using autogen docker, is it always necessary to reinstall modules?Database locked errorAgents are throwing due to docker not running, how can I resolve this?If you want to disable code execution entirelyIf you want to run code execution in dockerIf you want to run code execution locallyCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGenBench is a standalone tool for evaluating AutoGen agents and workflows on common benchmarks.",
        "segment_2": "TLDR\u200b",
        "segment_3": "Today we are releasing AutoGenBench \u2013 a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.",
        "segment_4": "AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.",
        "segment_6": "See the AutoGenBench README for information on installation and running benchmarks.",
        "segment_7": "See the AutoGenBench CONTRIBUTING guide for information on developing or contributing benchmark datasets.",
        "segment_9": "Quick Start\u200b",
        "segment_10": "Get started quickly by running the following commands in a bash terminal.",
        "segment_11": "Note: You may need to adjust the path to the OAI_CONFIG_LIST, as appropriate.",
        "segment_12": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)pip install autogenbenchautogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate Results/human_eval_two_agents",
        "segment_13": "Introduction\u200b",
        "segment_14": "Measurement and evaluation are core components of every major AI or ML research project. The same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to AgentEval. In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluation; and conclude with an open call for contributions.",
        "segment_15": "Design Principles\u200b",
        "segment_16": "AutoGenBench is designed around three core design principles. Knowing these principles will help you understand the tool, its operation and its output. These three principles are:",
        "segment_19": "Repetition: LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.",
        "segment_22": "Isolation: Agents interact with their worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to ordering effects that can impact future measurements. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in its own Docker container. This ensures that all runs start with the same initial conditions. (Docker is also a much safer way to run agent-produced code, in general.)",
        "segment_25": "Instrumentation: While top-line metrics are great for comparing agents or models, we often want much more information about how the agents are performing, where they are getting stuck, and how they can be improved. We may also later think of new research questions that require computing a different set of metrics. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like AgentEval.",
        "segment_28": "Installing and Running AutoGenBench\u200b",
        "segment_29": "As noted above, isolation is a key design principle, and so AutoGenBench must be run in an environment where Docker is available (desktop or Engine). It will not run in GitHub codespaces, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see https://www.docker.com/products/docker-desktop/.",
        "segment_30": "Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With pip, installation can be achieved as follows:",
        "segment_31": "pip install autogenbench",
        "segment_32": "After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter.",
        "segment_33": "If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:",
        "segment_34": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)",
        "segment_35": "A Typical Session\u200b",
        "segment_36": "Once AutoGenBench and necessary keys are installed, a typical session will look as follows:",
        "segment_37": "autogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate results/human_eval_two_agents",
        "segment_38": "Where:",
        "segment_40": "autogenbench clone HumanEval downloads and expands the HumanEval benchmark scenario.",
        "segment_41": "cd HumanEval; cat README.md navigates to the benchmark directory, and prints the README (which you should always read!)",
        "segment_42": "autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl",
        "segment_43": "runs a 10% subsample of the tasks defined in Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.",
        "segment_44": "autogenbench tabulate results/human_eval_two_agents tabulates the results of the run.",
        "segment_46": "After running the above tabulate command, you should see output similar to the following:",
        "segment_47": "Trial 0 Trial 1 Trial 2Task Id Success Success Success------------- --------- --------- ---------HumanEval_107 False True TrueHumanEval_22 True True TrueHumanEval_43 True True TrueHumanEval_88 True True TrueHumanEval_14 True True TrueHumanEval_157 True True TrueHumanEval_141 True True TrueHumanEval_57 True True TrueHumanEval_154 True True TrueHumanEval_153 True True TrueHumanEval_93 False True FalseHumanEval_137 True True TrueHumanEval_143 True True TrueHumanEval_13 True True TrueHumanEval_49 True True TrueHumanEval_95 True True True------------- --------- --------- ---------Successes 14 16 15Failures 2 0 1Missing 0 0 0Total 16 16 16CAUTION: 'autogenbench tabulate' is in early preview.Please do not cite these values in academic work without first inspecting and verifying the results in the logs yourself.",
        "segment_48": "From this output we can see the results of the three separate repetitions of each task, and final summary statistics of each run. In this case, the results were generated via GPT-4 (as defined in the OAI_CONFIG_LIST that was provided), and used the TwoAgents template. It is important to remember that AutoGenBench evaluates specific end-to-end configurations of agents (as opposed to evaluating a model or cognitive framework more generally).",
        "segment_49": "Finally, complete execution traces and logs can be found in the Results folder. See the AutoGenBench README for more details about command-line options and output formats. Each of these commands also offers extensive in-line help via:",
        "segment_51": "autogenbench --help",
        "segment_52": "autogenbench clone --help",
        "segment_53": "autogenbench run --help",
        "segment_54": "autogenbench tabulate --help",
        "segment_56": "Roadmap\u200b",
        "segment_57": "While we are announcing AutoGenBench, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:",
        "segment_59": "Onboard many additional benchmarks beyond those shipping today",
        "segment_60": "Greatly improve logging and telemetry",
        "segment_61": "Introduce new core metrics including total costs, task completion time, conversation turns, etc.",
        "segment_62": "Provide tighter integration with AgentEval and AutoGen Studio",
        "segment_64": "For an up to date tracking of our work items on this project, please see AutoGenBench Work Items",
        "segment_65": "Call for Participation\u200b",
        "segment_66": "Finally, we want to end this blog post with an open call for contributions. AutoGenBench is still nascent, and has much opportunity for improvement. New benchmarks are constantly being published, and will need to be added. Everyone may have their own distinct set of metrics that they care most about optimizing, and these metrics should be onboarded. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the contributor\u2019s guide and join our Discord discussion in the #autogenbench channel!Tags:AutoGenNewer PostAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismOlder PostCode execution is now by default inside docker containerTLDRQuick StartIntroductionDesign PrinciplesInstalling and Running AutoGenBenchA Typical SessionRoadmapCall for ParticipationCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_2": "Preparations\u200b",
        "segment_3": "Clone FastChat\u200b",
        "segment_4": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_5": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_6": "Download checkpoint\u200b",
        "segment_7": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_8": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_9": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_10": "Initiate server\u200b",
        "segment_11": "First, launch the controller",
        "segment_12": "python -m fastchat.serve.controller",
        "segment_13": "Then, launch the model worker(s)",
        "segment_14": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_15": "Finally, launch the RESTful API server",
        "segment_16": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_17": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_18": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_19": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_20": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_21": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_22": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_23": "interacting with multiple local LLMs\u200b",
        "segment_24": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_25": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_26": "The inference code would be:",
        "segment_27": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_28": "For Further Reading\u200b",
        "segment_30": "Documentation about autogen.",
        "segment_31": "Documentation about FastChat.",
        "segment_32": "Tags:LLMNewer PostRetrieval-Augmented Generation (RAG) Applications with AutoGenOlder PostMathChat - An Conversational Framework to Solve Math ProblemsPreparationsClone FastChatDownload checkpointInitiate serverInteract with model using oai.Completion (requires openai<1)interacting with multiple local LLMsFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class SocietyOfMindAgent(ConversableAgent)",
        "segment_2": "(In preview) A single agent that runs a Group Chat as an inner monologue.",
        "segment_3": "At the end of the conversation (termination for any reason), the SocietyOfMindAgent",
        "segment_4": "applies the response_preparer method on the entire inner monologue message history to",
        "segment_5": "extract a final answer for the reply.",
        "segment_6": "Most arguments are inherited from ConversableAgent. New arguments are:",
        "segment_7": "chat_manager (GroupChatManager): the group chat manager that will be running the inner monologue",
        "segment_8": "response_preparer (Optional, Callable or String): If response_preparer is a callable function, then",
        "segment_9": "it should have the signature:",
        "segment_10": "f( self: SocietyOfMindAgent, messages: List[Dict])",
        "segment_11": "where self is this SocietyOfMindAgent, and messages is a list of inner-monologue messages.",
        "segment_12": "The function should return a string representing the final response (extracted or prepared)",
        "segment_13": "from that history.",
        "segment_14": "If response_preparer is a string, then it should be the LLM prompt used to extract the final",
        "segment_15": "message from the inner chat transcript.",
        "segment_16": "The default response_preparer depends on if an llm_config is provided. If llm_config is False,",
        "segment_17": "then the response_preparer deterministically returns the last message in the inner-monolgue. If",
        "segment_18": "llm_config is set to anything else, then a default LLM prompt is used.",
        "segment_19": "chat_manager\u200b",
        "segment_20": "@propertydef chat_manager() -> Union[GroupChatManager, None]",
        "segment_21": "Return the group chat manager.",
        "segment_22": "update_chat_manager\u200b",
        "segment_23": "def update_chat_manager(chat_manager: Union[GroupChatManager, None])",
        "segment_24": "Update the chat manager.",
        "segment_25": "Arguments:",
        "segment_27": "chat_manager GroupChatManager - the group chat manager",
        "segment_29": "generate_inner_monologue_reply\u200b",
        "segment_30": "def generate_inner_monologue_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_31": "Generate a reply by running the group chatEdit this pagePreviousretrieve_user_proxy_agentNexttext_analyzer_agentSocietyOfMindAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "This project welcomes and encourages all forms of contributions, including but not limited to:",
        "segment_3": "Pushing patches.",
        "segment_4": "Code review of pull requests.",
        "segment_5": "Documentation, examples and test cases.",
        "segment_6": "Readability improvement, e.g., improvement on docstr and comments.",
        "segment_7": "Community participation in issues, discussions, discord, and twitter.",
        "segment_8": "Tutorials, blog posts, talks that promote the project.",
        "segment_9": "Sharing application scenarios and/or related research.",
        "segment_11": "Most contributions require you to agree to a",
        "segment_12": "Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us",
        "segment_13": "the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.",
        "segment_14": "If you are new to GitHub here is a detailed help source on getting involved with development on GitHub.",
        "segment_15": "When you submit a pull request, a CLA bot will automatically determine whether you need to provide",
        "segment_16": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions",
        "segment_17": "provided by the bot. You will only need to do this once across all repos using our CLA.",
        "segment_18": "This project has adopted the Microsoft Open Source Code of Conduct.",
        "segment_19": "For more information see the Code of Conduct FAQ or",
        "segment_20": "contact opencode@microsoft.com with any additional questions or comments.",
        "segment_21": "How to make a good bug report\u200b",
        "segment_22": "When you submit an issue to GitHub, please do your best to",
        "segment_23": "follow these guidelines! This will make it a lot easier to provide you with good",
        "segment_24": "feedback:",
        "segment_27": "The ideal bug report contains a short reproducible code snippet. This way",
        "segment_28": "anyone can try to reproduce the bug easily (see this for more details). If your snippet is",
        "segment_29": "longer than around 50 lines, please link to a gist or a GitHub repo.",
        "segment_32": "If an exception is raised, please provide the full traceback.",
        "segment_35": "Please include your operating system type and version number, as well as",
        "segment_36": "your Python, autogen, scikit-learn versions. The version of autogen",
        "segment_37": "can be found by running the following code snippet:",
        "segment_40": "import autogenprint(autogen.__version__)",
        "segment_42": "Please ensure all code snippets and error messages are formatted in",
        "segment_43": "appropriate code blocks. See Creating and highlighting code blocks",
        "segment_44": "for more details.",
        "segment_46": "Becoming a Reviewer\u200b",
        "segment_47": "There is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord.",
        "segment_48": "Guidance for Maintainers\u200b",
        "segment_49": "General\u200b",
        "segment_51": "Be a member of the community and treat everyone as a member. Be inclusive.",
        "segment_52": "Help each other and encourage mutual help.",
        "segment_53": "Actively post and respond.",
        "segment_54": "Keep open communication.",
        "segment_56": "Pull Requests\u200b",
        "segment_59": "For new PR, decide whether to close without review. If not, find the right reviewers. The default reviewer is microsoft/autogen. Ask users who can benefit from the PR to review it.",
        "segment_62": "For old PR, check the blocker: reviewer or PR creator. Try to unblock. Get additional help when needed.",
        "segment_65": "When requesting changes, make sure you can check back in time because it blocks merging.",
        "segment_68": "Make sure all the checks are passed.",
        "segment_71": "For changes that require running OpenAI tests, make sure the OpenAI tests pass too. Running these tests requires approval.",
        "segment_74": "In general, suggest small PRs instead of a giant PR.",
        "segment_77": "For documentation change, request snapshot of the compiled website, or compile by yourself to verify the format.",
        "segment_80": "For new contributors who have not signed the contributing agreement, remind them to sign before reviewing.",
        "segment_83": "For multiple PRs which may have conflict, coordinate them to figure out the right order.",
        "segment_86": "Pay special attention to:",
        "segment_88": "Breaking changes. Don\u2019t make breaking changes unless necessary. Don\u2019t merge to main until enough headsup is provided and a new release is ready.",
        "segment_89": "Test coverage decrease.",
        "segment_90": "Changes that may cause performance degradation. Do regression test when test suites are available.",
        "segment_91": "Discourage change to the core library when there is an alternative.",
        "segment_95": "Issues and Discussions\u200b",
        "segment_98": "For new issues, write a reply, apply a label if relevant. Ask on discord when necessary. For roadmap issues, add to the roadmap project and encourage community discussion. Mention relevant experts when necessary.",
        "segment_101": "For old issues, provide an update or close. Ask on discord when necessary. Encourage PR creation when relevant.",
        "segment_104": "Use \u201cgood first issue\u201d for easy fix suitable for first-time contributors.",
        "segment_107": "Use \u201ctask list\u201d for issues that require multiple PRs.",
        "segment_110": "For discussions, create an issue when relevant. Discuss on discord when appropriate.",
        "segment_113": "Docker for Development\u200b",
        "segment_114": "For developers contributing to the AutoGen project, we offer a specialized Docker environment. This setup is designed to streamline the development process, ensuring that all contributors work within a consistent and well-equipped environment.",
        "segment_115": "Autogen Developer Image (autogen_dev_img)\u200b",
        "segment_117": "Purpose: The autogen_dev_img is tailored for contributors to the AutoGen project. It includes a suite of tools and configurations that aid in the development and testing of new features or fixes.",
        "segment_118": "Usage: This image is recommended for developers who intend to contribute code or documentation to AutoGen.",
        "segment_119": "Forking the Project: It's advisable to fork the AutoGen GitHub project to your own repository. This allows you to make changes in a separate environment without affecting the main project.",
        "segment_120": "Updating Dockerfile: Modify your copy of Dockerfile in the dev folder as needed for your development work.",
        "segment_121": "Submitting Pull Requests: Once your changes are ready, submit a pull request from your branch to the upstream AutoGen GitHub project for review and integration. For more details on contributing, see the AutoGen Contributing page.",
        "segment_123": "Building the Developer Docker Image\u200b",
        "segment_126": "To build the developer Docker image (autogen_dev_img), use the following commands:",
        "segment_127": "docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git",
        "segment_130": "For building the developer image built from a specific Dockerfile in a branch other than main/master",
        "segment_131": "# clone the branch you want to work out ofgit clone --branch {branch-name} https://github.com/microsoft/autogen.git# cd to your new directorycd autogen# build your Docker imagedocker build -f .devcontainer/dev/Dockerfile -t autogen_dev-srv_img .",
        "segment_134": "Using the Developer Docker Image\u200b",
        "segment_135": "Once you have built the autogen_dev_img, you can run it using the standard Docker commands. This will place you inside the containerized development environment where you can run tests, develop code, and ensure everything is functioning as expected before submitting your contributions.",
        "segment_136": "docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_138": "Note that the pwd is shorthand for present working directory. Thus, any path after the pwd is relative to that. If you want a more verbose method you could remove the \"pwd/autogen-newcode\" and replace it with the full path to your directory",
        "segment_140": "docker run -it -p 8081:3000 -v /home/AutoGenDeveloper/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_141": "Develop in Remote Container\u200b",
        "segment_142": "If you use vscode, you can open the autogen folder in a Container.",
        "segment_143": "We have provided the configuration in devcontainer. They can be used in GitHub codespace too. Developing AutoGen in dev containers is recommended.",
        "segment_144": "Pre-commit\u200b",
        "segment_145": "Run pre-commit install to install pre-commit into your git hooks. Before you commit, run",
        "segment_146": "pre-commit run to check if you meet the pre-commit requirements. If you use Windows (without WSL) and can't commit after installing pre-commit, you can run pre-commit uninstall to uninstall the hook. In WSL or Linux this is supposed to work.",
        "segment_147": "Write tests\u200b",
        "segment_148": "Tests are automatically run via GitHub actions. There are two workflows:",
        "segment_150": "build.yml",
        "segment_151": "openai.yml",
        "segment_153": "The first workflow is required to pass for all PRs (and it doesn't do any OpenAI calls). The second workflow is required for changes that affect the OpenAI tests (and does actually call LLM). The second workflow requires approval to run. When writing tests that require OpenAI calls, please use pytest.mark.skipif to make them run in only when openai package is installed. If additional dependency for this test is required, install the dependency in the corresponding python version in openai.yml.",
        "segment_154": "Make sure all tests pass, this is required for build.yml checks to pass",
        "segment_155": "Running tests locally\u200b",
        "segment_156": "To run tests, install the [test] option:",
        "segment_157": "pip install -e.\"[test]\"",
        "segment_158": "Then you can run the tests from the test folder using the following command:",
        "segment_159": "pytest test",
        "segment_160": "Tests for the autogen.agentchat.contrib module may be skipped automatically if the",
        "segment_161": "required dependencies are not installed. Please consult the documentation for",
        "segment_162": "each contrib module to see what dependencies are required.",
        "segment_163": "Skip flags for tests\u200b",
        "segment_165": "--skip-openai for skipping tests that require access to OpenAI services.",
        "segment_166": "--skip-docker for skipping tests that explicitly use docker",
        "segment_167": "--skip-redis for skipping tests that require a Redis server",
        "segment_169": "For example, the following command will skip tests that require access to",
        "segment_170": "OpenAI and docker services:",
        "segment_171": "pytest test --skip-openai --skip-docker",
        "segment_172": "Coverage\u200b",
        "segment_173": "Any code you commit should not decrease coverage. To run all unit tests, install the [test] option:",
        "segment_174": "pip install -e.\"[test]\"coverage run -m pytest test",
        "segment_175": "Then you can see the coverage report by",
        "segment_176": "coverage report -m or coverage html.",
        "segment_177": "Documentation\u200b",
        "segment_178": "To build and test documentation locally, install Node.js. For example,",
        "segment_179": "nvm install --lts",
        "segment_180": "Then:",
        "segment_181": "npm install --global yarn # skip if you use the dev container we providedpip install pydoc-markdown # skip if you use the dev container we providedcd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownquarto render ./docsyarn start",
        "segment_182": "The last command starts a local development server and opens up a browser window.",
        "segment_183": "Most changes are reflected live without having to restart the server.",
        "segment_184": "To build and test documentation within a docker container. Use the Dockerfile in the dev folder as described above to build your image",
        "segment_185": "docker build -f .devcontainer/dev/Dockerfile -t autogen_dev_img https://github.com/microsoft/autogen.git",
        "segment_186": "Then start the container like so, this will log you in and ensure that Docker port 3000 is mapped to port 8081 on your local machine",
        "segment_187": "docker run -it -p 8081:3000 -v `pwd`/autogen-newcode:newstuff/ autogen_dev_img bash",
        "segment_188": "Once at the CLI in Docker run the following commands:",
        "segment_189": "cd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownquarto render ./docsyarn start --host 0.0.0.0 --port 3000",
        "segment_190": "Once done you should be able to access the documentation at http://127.0.0.1:8081/autogen",
        "segment_191": "Note:",
        "segment_192": "some tips in this guide are based off the contributor guide from flaml.Edit this pagePreviousEnhanced InferenceNextResearchHow to make a good bug reportBecoming a ReviewerGuidance for MaintainersGeneralPull RequestsIssues and DiscussionsDocker for DevelopmentAutogen Developer Image (autogen_dev_img)Building the Developer Docker ImageUsing the Developer Docker ImageDevelop in Remote ContainerPre-commitWrite testsCoverageDocumentationCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {},
    {
        "segment_1": "This page contains a list of demos that use AutoGen in various applications from the community.",
        "segment_2": "Contribution guide:",
        "segment_3": "Built something interesting with AutoGen? Submit a PR to add it to the list! See the Contribution Guide below for more details.",
        "segment_4": "Filter by tagsAutoGen Group Chat Playgroundlg...A huggingface space to explore AutoGen group chat, build agents with zero-code, and access source code for reuse.lg...uiA Stateful Dev Environment Powered by Jupyter Notebooklg...An AutoGen Teams Powered by Jupyter notebook.lg...extensiontoolsSolving Security Vulnerabilities with AutoGenlg...An article discussing the use of AutoGen to tackle security vulnerabilities.lg...appResearch Agents via AutoGenlg...A guide to building a team of AI research agents using AutoGen.lg...groupchattoolsAutoGen with Ollama and LiteLLMlg...A demonstration of integrating Ollama, LiteLLM, and AutoGen.lg...extensionAutoGen Engineerlg...Join the AutoGen Engineer group chat to collaborate and build with others.lg...groupchatappAutoGen with Obsidianlg...Learn how to integrate AutoGen with Obsidian for note-taking and management.lg...toolsappAutoGen Builder GPTlg...A platform for building conversational AI agents with AutoGen Builder GPT.lg...groupchatuiAutoGen Multi-Round Human Interaction Chatbot with Gradio 4.0lg...Experience a multi-round human interaction chatbot built with AutoGen and Gradio 4.0.lg...uiappAgentcloud.dev (UI for AutoGen)lg...Agentcloud.dev provides a user interface for managing and collaborating with AutoGen agents.lg...uiNext.js + FASTAPI Based UI for AutoGenlg...A project featuring a UI for AutoGen built with Next.js and FastAPI.lg...uiFull Function UI for AutoGen Powered by Panellg...A UI allows users to directly interact with AI agents in real-time during a group chat scenariolg...uiAutoGen Monitoring and Observabilitylg...Documentation on monitoring and observability features for AutoGen.lg...extensionPostgres Data Analytics AI Agent with AutoGenlg...Utilizing AutoGen to speak directly to Postgres Database.lg...toolsappAutoGen with Local LLMslg...An article on deploying multiple AI agents using local LLMs with AutoGen.lg...extensionAutoGen with FastApi backend and React Frontendlg...A demonstration of using AutoGen with a FastAPI backend and React frontend.lg...uiTalk to AutoGen Agents Using Whisper and Gradiolg...Interact with AutoGen agents using Whisper and Gradio interfaces.lg...uiAutoGen + LangChain + ChromaDB = You Super AI Assistantlg...Create a super AI assistant combining AutoGen, LangChain, and ChromaDB.lg...appAutoGen + Flowise = Super AI Agents on No-Code Platformlg...Build super AI agents on a no-code platform using AutoGen and Flowise.lg...appAutoGen with RunPod and TextGen WebUIlg...Learn how to use AutoGen with RunPod and TextGen WebUI for enhanced AI agent integration.lg...uiextensionJarvis Collaborates with AutoGen for Tweet Analysislg...Explore how Jarvis collaborates with AutoGen for tweet analysis.lg...toolsappAutoGen + LangChain + PlayHT = Super AI Agent that Speakslg...Combine AutoGen, LangChain, and PlayHT to create a speaking super AI agent.lg...toolsappAutoGen Flow with FastAPI and Nextjslg...A development flow using AutoGen with FastAPI and Next.js.lg...uiBuild Vision-Enabled AI Agents with AutoGen + Llavalg...Tutorial on building vision-enabled AI agents using AutoGen and llava.lg...toolsappAutoGen + Chainlit chat interface with multi-agent conversationlg...Chainlit chat interface with multi-agent conversation between agents to complete a taskslg...uiXForce IDE: Build AutoGen based workforces from a drag and drop UIlg...X-Force IDE is a low-code, agent-as-a-service UI framework that lets you create AutoGen-based workforces from a drag-and-drop-like user interface.lg...ui",
        "segment_5": "Contributing\u200b",
        "segment_6": "To contribute, please open a PR that adds an entry to the data/gallery.json file in the src directory. The entry should be an object with the following properties:",
        "segment_7": "{ \"title\": \"AutoGen Playground\", \"link\": \"https://huggingface.co/spaces/thinkall/AutoGen_Playground\", \"description\": \"A space to explore the capabilities of AutoGen.\", \"image\": \"default.png\", \"tags\": [\"ui\"] }",
        "segment_8": "The image property should be the name of a file in the static/img/gallery directory.",
        "segment_9": "The tags property should be an array of strings that describe the demo. We recommend using no more than two tags for clarity.",
        "segment_10": "Here are the meanings of several tags for reference:",
        "segment_12": "app: Using Autogen for specific applications.",
        "segment_13": "extension: Enhancing AutoGen beyond the features in current version.",
        "segment_14": "ui: Building user interface for AutoGen.",
        "segment_15": "tool: Strengthening AutoGen Agents with external tools.",
        "segment_16": "groupchat: Solving complex tasks with a group of Agents.",
        "segment_18": "if the existing ones do not precisely portray your own demos, new tags are also encouraged to add.Edit this pageContributingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Introducing the EcoAssistant, which is designed to solve user queries more accurately and affordably.",
        "segment_4": "We show how to let the LLM assistant agent leverage external API to solve user query.",
        "segment_5": "We show how to reduce the cost of using GPT models via Assistant Hierarchy.",
        "segment_6": "We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via Solution Demonstration.",
        "segment_8": "EcoAssistant\u200b",
        "segment_9": "In this blog, we introduce the EcoAssistant, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.",
        "segment_10": "Problem setup\u200b",
        "segment_11": "Recently, users have been using conversational LLMs such as ChatGPT for various queries.",
        "segment_12": "Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.",
        "segment_13": "Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).",
        "segment_14": "These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.",
        "segment_15": "In the table below, we show three types of user queries that we aim to address in this work.",
        "segment_16": "DatasetAPIExample queryPlacesGoogle PlacesI\u2019m looking for a 24-hour pharmacy in Montreal, can you find one for me?WeatherWeather APIWhat is the current cloud coverage in Mumbai, India?StockAlpha Vantage Stock APICan you give me the opening price of Microsoft for the month of January 2023?",
        "segment_17": "Leveraging external APIs\u200b",
        "segment_18": "To address these queries, we first build a two-agent system based on AutoGen,",
        "segment_19": "where the first agent is a LLM assistant agent (AssistantAgent in AutoGen) that is responsible for proposing and refining the code and",
        "segment_20": "the second agent is a code executor agent (UserProxyAgent in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.",
        "segment_21": "A visualization of the two-agent system is shown below.",
        "segment_23": "To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.",
        "segment_24": "The template is shown below, where the red part is the information of APIs and black part is user query.",
        "segment_26": "Importantly, we don't want to reveal our real API key to the assistant agent for safety concerns.",
        "segment_27": "Therefore, we use a fake API key to replace the real API key in the initial message.",
        "segment_28": "In particular, we generate a random token (e.g., 181dbb37) for each API key and replace the real API key with the token in the initial message.",
        "segment_29": "Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.",
        "segment_30": "Solution Demonstration\u200b",
        "segment_31": "In most practical scenarios, queries from users would appear sequentially over time.",
        "segment_32": "Our EcoAssistant leverages past success to help the LLM assistants address future queries via Solution Demonstration.",
        "segment_33": "Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.",
        "segment_34": "These query-code pairs are saved in a specialized vector database. When new queries appear, EcoAssistant retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.",
        "segment_35": "The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.",
        "segment_37": "We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system's performance.",
        "segment_38": "Assistant Hierarchy\u200b",
        "segment_39": "LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.",
        "segment_40": "Thus, we propose the Assistant Hierarchy to reduce the cost of using LLMs.",
        "segment_41": "The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.",
        "segment_42": "By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.",
        "segment_43": "In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.",
        "segment_44": "If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, EcoAssistant would then restart the conversation with the next more expensive LLM assistant in the hierarchy.",
        "segment_45": "We found that this strategy significantly reduces costs while still effectively addressing queries.",
        "segment_46": "A Synergistic Effect\u200b",
        "segment_47": "We found that the Assistant Hierarchy and Solution Demonstration of EcoAssistant have a synergistic effect.",
        "segment_48": "Because the query-code database is shared by all LLM assistants, even without specialized design,",
        "segment_49": "the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).",
        "segment_50": "Such a synergistic effect further improves the performance and reduces the cost of EcoAssistant.",
        "segment_51": "Experimental Results\u200b",
        "segment_52": "We evaluate EcoAssistant on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that EcoAssistant achieves a higher success rate with a lower cost as shown in the figure below.",
        "segment_53": "For more details about the experimental results and other experiments, please refer to our paper.",
        "segment_55": "Further reading\u200b",
        "segment_56": "Please refer to our paper and codebase for more details about EcoAssistant.",
        "segment_57": "If you find this blog useful, please consider citing:",
        "segment_58": "@article{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, journal={arXiv preprint arXiv:2310.03046}, year={2023}}Tags:LLMRAGcost-effectivenessNewer PostAutoGen Meets GPTsOlder PostMultimodal with GPT-4V and LLaVAEcoAssistantProblem setupLeveraging external APIsSolution DemonstrationAssistant HierarchyA Synergistic EffectExperimental ResultsFurther readingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen now supports custom models! This feature empowers users to define and load their own models, allowing for a more flexible and personalized inference mechanism. By adhering to a specific protocol, you can integrate your custom model for use with AutoGen and respond to prompts any way needed by using any model/API call/hardcoded response you want.",
        "segment_2": "NOTE: Depending on what model you use, you may need to play with the default prompts of the Agent's",
        "segment_3": "Quickstart\u200b",
        "segment_4": "An interactive and easy way to get started is by following the notebook here which loads a local model from HuggingFace into AutoGen and uses it for inference, and making changes to the class provided.",
        "segment_5": "Step 1: Create the custom model client class\u200b",
        "segment_6": "To get started with using custom models in AutoGen, you need to create a model client class that adheres to the ModelClient protocol defined in client.py. The new model client class should implement these methods:",
        "segment_8": "create(): Returns a response object that implements the ModelClientResponseProtocol (more details in the Protocol section).",
        "segment_9": "message_retrieval(): Processes the response object and returns a list of strings or a list of message objects (more details in the Protocol section).",
        "segment_10": "cost(): Returns the cost of the response.",
        "segment_11": "get_usage(): Returns a dictionary with keys from RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"].",
        "segment_13": "E.g. of a bare bones dummy custom class:",
        "segment_14": "class CustomModelClient: def __init__(self, config, **kwargs): print(f\"CustomModelClient config: {config}\") def create(self, params): num_of_responses = params.get(\"n\", 1) # can create my own data response class # here using SimpleNamespace for simplicity # as long as it adheres to the ModelClientResponseProtocol response = SimpleNamespace() response.choices = [] response.model = \"model_name\" # should match the OAI_CONFIG_LIST registration for _ in range(num_of_responses): text = \"this is a dummy text response\" choice = SimpleNamespace() choice.message = SimpleNamespace() choice.message.content = text choice.message.function_call = None response.choices.append(choice) return response def message_retrieval(self, response): choices = response.choices return [choice.message.content for choice in choices] def cost(self, response) -> float: response.cost = 0 return 0 @staticmethod def get_usage(response): return {}",
        "segment_15": "Step 2: Add the configuration to the OAI_CONFIG_LIST\u200b",
        "segment_16": "The field that is necessary is setting model_client_cls to the name of the new class (as a string) \"model_client_cls\":\"CustomModelClient\". Any other fields will be forwarded to the class constructor, so you have full control over what parameters to specify and how to use them. E.g.:",
        "segment_17": "{ \"model\": \"Open-Orca/Mistral-7B-OpenOrca\", \"model_client_cls\": \"CustomModelClient\", \"device\": \"cuda\", \"n\": 1, \"params\": { \"max_length\": 1000, }}",
        "segment_18": "Step 3: Register the new custom model to the agent that will use it\u200b",
        "segment_19": "If a configuration with the field \"model_client_cls\":\"\" has been added to an Agent's config list, then the corresponding model with the desired class must be registered after the agent is created and before the conversation is initialized:",
        "segment_20": "my_agent.register_model_client(model_client_cls=CustomModelClient, [other args that will be forwarded to CustomModelClient constructor])",
        "segment_21": "model_client_cls=CustomModelClient arg matches the one specified in the OAI_CONFIG_LIST and CustomModelClient is the class that adheres to the ModelClient protocol (more details on the protocol below).",
        "segment_22": "If the new model client is in the config list but not registered by the time the chat is initialized, then an error will be raised.",
        "segment_23": "Protocol details\u200b",
        "segment_24": "A custom model class can be created in many ways, but needs to adhere to the ModelClient protocol and response structure which is defined in client.py and shown below.",
        "segment_25": "The response protocol is currently using the minimum required fields from the autogen codebase that match the OpenAI response structure. Any response protocol that matches the OpenAI response structure will probably be more resilient to future changes, but we are starting off with minimum requirements to make adpotion of this feature easier.",
        "segment_26": "class ModelClient(Protocol): \"\"\" A client class must implement the following methods: - create must return a response object that implements the ModelClientResponseProtocol - cost must return the cost of the response - get_usage must return a dict with the following keys: - prompt_tokens - completion_tokens - total_tokens - cost - model This class is used to create a client that can be used by OpenAIWrapper. The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed. The message_retrieval method must be implemented to return a list of str or a list of messages from the response. \"\"\" RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"] class ModelClientResponseProtocol(Protocol): class Choice(Protocol): class Message(Protocol): content: Optional[str] message: Message choices: List[Choice] model: str def create(self, params) -> ModelClientResponseProtocol: ... def message_retrieval( self, response: ModelClientResponseProtocol ) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]: \"\"\" Retrieve and return a list of strings or a list of Choice.Message from the response. NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used. \"\"\" ... def cost(self, response: ModelClientResponseProtocol) -> float: ... @staticmethod def get_usage(response: ModelClientResponseProtocol) -> Dict: \"\"\"Return usage summary of the response using RESPONSE_USAGE_KEYS.\"\"\" ...",
        "segment_27": "Troubleshooting steps\u200b",
        "segment_28": "If something doesn't work then run through the checklist:",
        "segment_30": "Make sure you have followed the client protocol and client response protocol when creating the custom model class",
        "segment_32": "create() method: ModelClientResponseProtocol must be followed when returning an inference response during create call.",
        "segment_33": "message_retrieval() method: returns a list of strings or a list of message objects. If a list of message objects is returned, they currently must contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.",
        "segment_34": "cost()method: returns an integer, and if you don't care about cost tracking you can just return 0.",
        "segment_35": "get_usage(): returns a dictionary, and if you don't care about usage tracking you can just return an empty dictionary {}.",
        "segment_38": "Make sure you have a corresponding entry in the OAI_CONFIG_LIST and that that entry has the \"model_client_cls\":\"\" field.",
        "segment_39": "Make sure you have registered the client using the corresponding config entry and your new class agent.register_model_client(model_client_cls=, [other optional args])",
        "segment_40": "Make sure that all of the custom models defined in the OAI_CONFIG_LIST have been registered.",
        "segment_41": "Any other troubleshooting might need to be done in the custom code itself.",
        "segment_43": "Conclusion\u200b",
        "segment_44": "With the ability to use custom models, AutoGen now offers even more flexibility and power for your AI applications. Whether you've trained your own model or want to use a specific pre-trained model, AutoGen can accommodate your needs. Happy coding!Tags:AutoGenAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsJanuary 25, 2024 \u00b7 7 min readAdam FourneyPrincipal Researcher Microsoft ResearchQingyun WuAssistant Professor at the Pennsylvania State University",
        "segment_45": "AutoGenBench is a standalone tool for evaluating AutoGen agents and workflows on common benchmarks.",
        "segment_46": "TLDR\u200b",
        "segment_47": "Today we are releasing AutoGenBench \u2013 a tool for evaluating AutoGen agents and workflows on established LLM and agentic benchmarks.",
        "segment_48": "AutoGenBench is a standalone command line tool, installable from PyPI, which handles downloading, configuring, running, and reporting supported benchmarks. AutoGenBench works best when run alongside Docker, since it uses Docker to isolate tests from one another.",
        "segment_50": "See the AutoGenBench README for information on installation and running benchmarks.",
        "segment_51": "See the AutoGenBench CONTRIBUTING guide for information on developing or contributing benchmark datasets.",
        "segment_53": "Quick Start\u200b",
        "segment_54": "Get started quickly by running the following commands in a bash terminal.",
        "segment_55": "Note: You may need to adjust the path to the OAI_CONFIG_LIST, as appropriate.",
        "segment_56": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)pip install autogenbenchautogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate Results/human_eval_two_agents",
        "segment_57": "Introduction\u200b",
        "segment_58": "Measurement and evaluation are core components of every major AI or ML research project. The same is true for AutoGen. To this end, today we are releasing AutoGenBench, a standalone command line tool that we have been using to guide development of AutoGen. Conveniently, AutoGenBench handles: downloading, configuring, running, and reporting results of agents on various public benchmark datasets. In addition to reporting top-line numbers, each AutoGenBench run produces a comprehensive set of logs and telemetry that can be used for debugging, profiling, computing custom metrics, and as input to AgentEval. In the remainder of this blog post, we outline core design principles for AutoGenBench (key to understanding its operation); present a guide to installing and running AutoGenBench; outline a roadmap for evaluation; and conclude with an open call for contributions.",
        "segment_59": "Design Principles\u200b",
        "segment_60": "AutoGenBench is designed around three core design principles. Knowing these principles will help you understand the tool, its operation and its output. These three principles are:",
        "segment_63": "Repetition: LLMs are stochastic, and in many cases, so too is the code they write to solve problems. For example, a Python script might call an external search engine, and the results may vary run-to-run. This can lead to variance in agent performance. Repetition is key to measuring and understanding this variance. To this end, AutoGenBench is built from the ground up with an understanding that tasks may be run multiple times, and that variance is a metric we often want to measure.",
        "segment_66": "Isolation: Agents interact with their worlds in both subtle and overt ways. For example an agent may install a python library or write a file to disk. This can lead to ordering effects that can impact future measurements. Consider, for example, comparing two agents on a common benchmark. One agent may appear more efficient than the other simply because it ran second, and benefitted from the hard work the first agent did in installing and debugging necessary Python libraries. To address this, AutoGenBench isolates each task in its own Docker container. This ensures that all runs start with the same initial conditions. (Docker is also a much safer way to run agent-produced code, in general.)",
        "segment_69": "Instrumentation: While top-line metrics are great for comparing agents or models, we often want much more information about how the agents are performing, where they are getting stuck, and how they can be improved. We may also later think of new research questions that require computing a different set of metrics. To this end, AutoGenBench is designed to log everything, and to compute metrics from those logs. This ensures that one can always go back to the logs to answer questions about what happened, run profiling software, or feed the logs into tools like AgentEval.",
        "segment_72": "Installing and Running AutoGenBench\u200b",
        "segment_73": "As noted above, isolation is a key design principle, and so AutoGenBench must be run in an environment where Docker is available (desktop or Engine). It will not run in GitHub codespaces, unless you opt for native execution (which is strongly discouraged). To install Docker Desktop see https://www.docker.com/products/docker-desktop/.",
        "segment_74": "Once Docker is installed, AutoGenBench can then be installed as a standalone tool from PyPI. With pip, installation can be achieved as follows:",
        "segment_75": "pip install autogenbench",
        "segment_76": "After installation, you must configure your API keys. As with other AutoGen applications, AutoGenBench will look for the OpenAI keys in the OAI_CONFIG_LIST file in the current working directory, or the OAI_CONFIG_LIST environment variable. This behavior can be overridden using a command-line parameter.",
        "segment_77": "If you will be running multiple benchmarks, it is often most convenient to leverage the environment variable option. You can load your keys into the environment variable by executing:",
        "segment_78": "export OAI_CONFIG_LIST=$(cat ./OAI_CONFIG_LIST)",
        "segment_79": "A Typical Session\u200b",
        "segment_80": "Once AutoGenBench and necessary keys are installed, a typical session will look as follows:",
        "segment_81": "autogenbench clone HumanEvalcd HumanEvalcat README.mdautogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonlautogenbench tabulate results/human_eval_two_agents",
        "segment_82": "Where:",
        "segment_84": "autogenbench clone HumanEval downloads and expands the HumanEval benchmark scenario.",
        "segment_85": "cd HumanEval; cat README.md navigates to the benchmark directory, and prints the README (which you should always read!)",
        "segment_86": "autogenbench run --subsample 0.1 --repeat 3 Tasks/human_eval_two_agents.jsonl",
        "segment_87": "runs a 10% subsample of the tasks defined in Tasks/human_eval_two_agents.jsonl. Each task is run 3 times.",
        "segment_88": "autogenbench tabulate results/human_eval_two_agents tabulates the results of the run.",
        "segment_90": "After running the above tabulate command, you should see output similar to the following:",
        "segment_91": "Trial 0 Trial 1 Trial 2Task Id Success Success Success------------- --------- --------- ---------HumanEval_107 False True TrueHumanEval_22 True True TrueHumanEval_43 True True TrueHumanEval_88 True True TrueHumanEval_14 True True TrueHumanEval_157 True True TrueHumanEval_141 True True TrueHumanEval_57 True True TrueHumanEval_154 True True TrueHumanEval_153 True True TrueHumanEval_93 False True FalseHumanEval_137 True True TrueHumanEval_143 True True TrueHumanEval_13 True True TrueHumanEval_49 True True TrueHumanEval_95 True True True------------- --------- --------- ---------Successes 14 16 15Failures 2 0 1Missing 0 0 0Total 16 16 16CAUTION: 'autogenbench tabulate' is in early preview.Please do not cite these values in academic work without first inspecting and verifying the results in the logs yourself.",
        "segment_92": "From this output we can see the results of the three separate repetitions of each task, and final summary statistics of each run. In this case, the results were generated via GPT-4 (as defined in the OAI_CONFIG_LIST that was provided), and used the TwoAgents template. It is important to remember that AutoGenBench evaluates specific end-to-end configurations of agents (as opposed to evaluating a model or cognitive framework more generally).",
        "segment_93": "Finally, complete execution traces and logs can be found in the Results folder. See the AutoGenBench README for more details about command-line options and output formats. Each of these commands also offers extensive in-line help via:",
        "segment_95": "autogenbench --help",
        "segment_96": "autogenbench clone --help",
        "segment_97": "autogenbench run --help",
        "segment_98": "autogenbench tabulate --help",
        "segment_100": "Roadmap\u200b",
        "segment_101": "While we are announcing AutoGenBench, we note that it is very much an evolving project in its own right. Over the next few weeks and months we hope to:",
        "segment_103": "Onboard many additional benchmarks beyond those shipping today",
        "segment_104": "Greatly improve logging and telemetry",
        "segment_105": "Introduce new core metrics including total costs, task completion time, conversation turns, etc.",
        "segment_106": "Provide tighter integration with AgentEval and AutoGen Studio",
        "segment_108": "For an up to date tracking of our work items on this project, please see AutoGenBench Work Items",
        "segment_109": "Call for Participation\u200b",
        "segment_110": "Finally, we want to end this blog post with an open call for contributions. AutoGenBench is still nascent, and has much opportunity for improvement. New benchmarks are constantly being published, and will need to be added. Everyone may have their own distinct set of metrics that they care most about optimizing, and these metrics should be onboarded. To this end, we welcome any and all contributions to this corner of the AutoGen project. If contributing is something that interests you, please see the contributor\u2019s guide and join our Discord discussion in the #autogenbench channel!Tags:AutoGenCode execution is now by default inside docker containerJanuary 23, 2024 \u00b7 3 min readOlga VrousgouSenior Software Engineer at Microsoft ResearchTLDR\u200b",
        "segment_111": "AutoGen 0.2.8 enhances operational safety by making 'code execution inside a Docker container' the default setting, focusing on informing users about its operations and empowering them to make informed decisions regarding code execution.",
        "segment_112": "The new release introduces a breaking change where the use_docker argument is set to True by default in code executing agents. This change underscores our commitment to prioritizing security and safety in AutoGen.",
        "segment_113": "Introduction\u200b",
        "segment_114": "AutoGen has code-executing agents, usually defined as a UserProxyAgent, where code execution is by default ON. Until now, unless explicitly specified by the user, any code generated by other agents would be executed by code-execution agents locally, i.e. wherever AutoGen was being executed. If AutoGen happened to be run in a docker container then the risks of running code were minimized. However, if AutoGen runs outside of Docker, it's easy particularly for new users to overlook code-execution risks.",
        "segment_115": "AutoGen has now changed to by default execute any code inside a docker container (unless execution is already happening inside a docker container). It will launch a Docker image (either user-provided or default), execute the new code, and then terminate the image, preparing for the next code execution cycle.",
        "segment_116": "We understand that not everyone is concerned about this especially when playing around with AutoGen for the first time. We have provided easy ways to turn this requirement off. But we believe that making sure that the user is aware of the fact that code will be executed locally, and prompting them to think about the security implications of running code locally is the right step for AutoGen.",
        "segment_117": "Example\u200b",
        "segment_118": "The example shows the default behaviour which is that any code generated by assistant agent and executed by user_proxy agent, will attempt to use a docker container to execute the code. If docker is not running, it will throw an error. User can decide to activate docker or opt in for local code execution.",
        "segment_119": "from autogen import AssistantAgent, UserProxyAgent, config_list_from_jsonassistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")",
        "segment_120": "To opt out of from this default behaviour there are some options.",
        "segment_121": "Diasable code execution entirely\u200b",
        "segment_123": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_125": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config=False)",
        "segment_126": "Run code execution locally\u200b",
        "segment_128": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_129": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_131": "E.g.:",
        "segment_132": "user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})",
        "segment_133": "Related documentation\u200b",
        "segment_135": "Code execution with docker",
        "segment_136": "How to disable code execution in docker",
        "segment_138": "Conclusion\u200b",
        "segment_139": "AutoGen 0.2.8 now improves the code execution safety and is ensuring that the user is properly informed of what autogen is doing and can make decisions around code-execution.Tags:AutoGenAll About Agent DescriptionsDecember 29, 2023 \u00b7 9 min readAdam FourneyPrincipal Researcher Microsoft ResearchTLDR\u200b",
        "segment_140": "AutoGen 0.2.2 introduces a description field to ConversableAgent (and all subclasses), and changes GroupChat so that it uses agent descriptions rather than system_messages when choosing which agents should speak next.",
        "segment_141": "This is expected to simplify GroupChat\u2019s job, improve orchestration, and make it easier to implement new GroupChat or GroupChat-like alternatives.",
        "segment_142": "If you are a developer, and things were already working well for you, no action is needed -- backward compatibility is ensured because the description field defaults to the system_message when no description is provided.",
        "segment_143": "However, if you were struggling with getting GroupChat to work, you can now try updating the description field.",
        "segment_144": "Introduction\u200b",
        "segment_145": "As AutoGen matures and developers build increasingly complex combinations of agents, orchestration is becoming an important capability. At present, GroupChat and the GroupChatManager are the main built-in tools for orchestrating conversations between 3 or more agents. For orchestrators like GroupChat to work well, they need to know something about each agent so that they can decide who should speak and when. Prior to AutoGen 0.2.2, GroupChat relied on each agent's system_message and name to learn about each participating agent. This is likely fine when the system prompt is short and sweet, but can lead to problems when the instructions are very long (e.g., with the AssistantAgent), or non-existent (e.g., with the UserProxyAgent).",
        "segment_146": "AutoGen 0.2.2 introduces a description field to all agents, and replaces the use of the system_message for orchestration in GroupChat and all future orchestrators. The description field defaults to the system_message to ensure backwards compatibility, so you may not need to change anything with your code if things are working well for you. However, if you were struggling with GroupChat, give setting the description field a try.",
        "segment_147": "The remainder of this post provides an example of how using the description field simplifies GroupChat's job, provides some evidence of its effectiveness, and provides tips for writing good descriptions.",
        "segment_148": "Example\u200b",
        "segment_149": "The current GroupChat orchestration system prompt has the following template:",
        "segment_150": "You are in a role play game. The following roles are available:{self._participant_roles(agents)}.Read the following conversation.Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.",
        "segment_151": "Suppose that you wanted to include 3 agents: A UserProxyAgent, an AssistantAgent, and perhaps a GuardrailsAgent.",
        "segment_152": "Prior to 0.2.2, this template would expand to:",
        "segment_153": "You are in a role play game. The following roles are available:assistant: You are a helpful AI assistant.Solve tasks using your coding and language skills.In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.Reply \"TERMINATE\" in the end when everything is done.user_proxy:guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:- You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.- You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.- You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.- You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.- You MUST TERMINATE the conversation if it involves seeking or giving advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice- You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.- You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.- You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.- You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.- You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word \"TERMINATE\". If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_154": "As you can see, this description is super confusing:",
        "segment_156": "It is hard to make out where each agent's role-description ends",
        "segment_157": "You appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)",
        "segment_158": "It takes a lot of tokens!",
        "segment_160": "Consequently, it's not hard to see why the GroupChat manager sometimes struggles with this orchestration task.",
        "segment_161": "With AutoGen 0.2.2 onward, GroupChat instead relies on the description field. With a description field the orchestration prompt becomes:",
        "segment_162": "You are in a role play game. The following roles are available:assistant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.Read the following conversation.Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.",
        "segment_163": "This is much easier to parse and understand, and it doesn't use nearly as many tokens. Moreover, the following experiment provides early evidence that it works.",
        "segment_164": "An Experiment with Distraction\u200b",
        "segment_165": "To illustrate the impact of the description field, we set up a three-agent experiment with a reduced 26-problem subset of the HumanEval benchmark. Here, three agents were added to a GroupChat to solve programming problems. The three agents were:",
        "segment_167": "Coder (default Assistant prompt)",
        "segment_168": "UserProxy (configured to execute code)",
        "segment_169": "ExecutiveChef (added as a distraction)",
        "segment_171": "The Coder and UserProxy used the AssistantAgent and UserProxy defaults (provided above), while the ExecutiveChef was given the system prompt:",
        "segment_172": "You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.",
        "segment_173": "The ExecutiveChef is clearly the distractor here -- given that no HumanEval problems are food-related, the GroupChat should rarely consult with the chef. However, when configured with GPT-3.5-turbo-16k, we can clearly see the GroupChat struggling with orchestration:",
        "segment_174": "With versions prior to 0.2.2, using system_message:\u200b",
        "segment_176": "The Agents solve 3 out of 26 problems on their first turn",
        "segment_177": "The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)",
        "segment_179": "With version 0.2.2, using description:\u200b",
        "segment_181": "The Agents solve 7 out of 26 problems on the first turn",
        "segment_182": "The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)",
        "segment_184": "Using the description field doubles performance on this task and halves the incidence of calling upon the distractor agent.",
        "segment_185": "Tips for Writing Good Descriptions\u200b",
        "segment_186": "Since descriptions serve a different purpose than system_messages, it is worth reviewing what makes a good agent description. While descriptions are new, the following tips appear to lead to good results:",
        "segment_188": "Avoid using the 1st or 2nd person perspective. Descriptions should not contain \"I\" or \"You\", unless perhaps \"You\" is in reference to the GroupChat / orchestrator",
        "segment_189": "Include any details that might help the orchestrator know when to call upon the agent",
        "segment_190": "Keep descriptions short (e.g., \"A helpful AI assistant with strong natural language and Python coding skills.\").",
        "segment_192": "The main thing to remember is that the description is for the benefit of the GroupChatManager, not for the Agent's own use or instruction.",
        "segment_193": "Conclusion\u200b",
        "segment_194": "AutoGen 0.2.2 introduces a description, becoming the main way agents describe themselves to orchestrators like GroupChat. Since the description defaults to the system_message, there's nothing you need to change if you were already satisfied with how your group chats were working. However, we expect this feature to generally improve orchestration, so please consider experimenting with the description field if you are struggling with GroupChat or want to boost performance.Tags:AutoGenAutoGen Studio: Interactively Explore Multi-Agent WorkflowsDecember 1, 2023 \u00b7 10 min readVictor DibiaPrincipal RSDE at Microsoft ResearchGagan BansalSenior Researcher at Microsoft ResearchSaleema AmershiSenior Principal Research Manager at Microsoft Research",
        "segment_195": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_196": "TLDR\u200b",
        "segment_197": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_199": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_200": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_201": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_202": "Publish your sessions to a local gallery.",
        "segment_204": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_205": "pip install autogenstudio",
        "segment_206": "Introduction\u200b",
        "segment_207": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_208": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_210": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_212": "Getting Started with AutoGen Studio\u200b",
        "segment_213": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_214": "Configuring an LLM Provider\u200b",
        "segment_215": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_216": "For example, in your terminal, you would set the API key like this:",
        "segment_217": "export OPENAI_API_KEY=",
        "segment_218": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_219": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_220": "Installation\u200b",
        "segment_221": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_224": "Install from PyPi",
        "segment_225": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_226": "pip install autogenstudio",
        "segment_229": "Install from Source",
        "segment_231": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_233": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_236": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_237": "pip install -e .",
        "segment_240": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_241": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_244": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_247": "Running the Application\u200b",
        "segment_248": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_249": "autogenstudio ui --port 8081",
        "segment_250": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_251": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_252": "What Can You Do with AutoGen Studio?\u200b",
        "segment_253": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_254": "Build\u200b",
        "segment_256": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_257": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_259": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_260": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_261": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_262": "Playground\u200b",
        "segment_264": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_265": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_266": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_267": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_268": "Gallery\u200b",
        "segment_269": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_270": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_271": "The AutoGen Studio API\u200b",
        "segment_272": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_273": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_274": "Road Map and Next Steps\u200b",
        "segment_275": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_277": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_278": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_279": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_280": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_282": "Contribution Guide\u200b",
        "segment_283": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_285": "Review the overall AutoGen project contribution guide.",
        "segment_286": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_287": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_288": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_289": "Submit a pull request with your contribution!",
        "segment_290": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_291": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_293": "FAQ\u200b",
        "segment_294": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_295": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_296": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_297": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_298": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_299": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_300": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_301": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_302": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_303": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_304": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_305": "Q: The Server Starts But I Can't Access the UI",
        "segment_306": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_307": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_308": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class TextAnalyzerAgent(ConversableAgent)",
        "segment_2": "(Experimental) Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed.",
        "segment_3": "__init__\u200b",
        "segment_4": "def __init__(name=\"analyzer\", system_message: Optional[str] = system_message, human_input_mode: Optional[str] = \"NEVER\", llm_config: Optional[Union[Dict, bool]] = None, **kwargs)",
        "segment_5": "Arguments:",
        "segment_7": "name str - name of the agent.",
        "segment_8": "system_message str - system message for the ChatCompletion inference.",
        "segment_9": "human_input_mode str - This agent should NEVER prompt the human for input.",
        "segment_10": "llm_config dict or False - llm inference configuration.",
        "segment_11": "Please refer to OpenAIWrapper.create",
        "segment_12": "for available options.",
        "segment_13": "To disable llm-based auto reply, set to False.",
        "segment_14": "**kwargs dict - other kwargs in ConversableAgent.",
        "segment_16": "analyze_text\u200b",
        "segment_17": "def analyze_text(text_to_analyze, analysis_instructions)",
        "segment_18": "Analyzes the given text as instructed, and returns the analysis.Edit this pagePrevioussociety_of_mind_agentNextweb_surferTextAnalyzerAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def get_key(config: Dict[str, Any]) -> str",
        "segment_2": "Get a unique identifier of a configuration.",
        "segment_3": "Arguments:",
        "segment_5": "config dict or list - A configuration.",
        "segment_7": "Returns:",
        "segment_9": "tuple - A unique identifier which can be used as a key for a dict.",
        "segment_11": "get_config_list\u200b",
        "segment_12": "def get_config_list(api_keys: List, base_urls: Optional[List] = None, api_type: Optional[str] = None, api_version: Optional[str] = None) -> List[Dict]",
        "segment_13": "Get a list of configs for OpenAI API client.",
        "segment_14": "Arguments:",
        "segment_16": "api_keys list - The api keys for openai api calls.",
        "segment_17": "base_urls list, optional - The api bases for openai api calls. If provided, should match the length of api_keys.",
        "segment_18": "api_type str, optional - The api type for openai api calls.",
        "segment_19": "api_version str, optional - The api version for openai api calls.",
        "segment_21": "Returns:",
        "segment_23": "list - A list of configs for OepnAI API calls.",
        "segment_25": "Example:",
        "segment_26": "# Define a list of API keysapi_keys = ['key1', 'key2', 'key3']# Optionally, define a list of base URLs corresponding to each API keybase_urls = ['https://api.service1.com', 'https://api.service2.com', 'https://api.service3.com']# Optionally, define the API type and version if they are common for all keysapi_type = 'azure'api_version = '2023-12-01-preview'# Call the get_config_list function to get a list of configuration dictionariesconfig_list = get_config_list(api_keys, base_urls, api_type, api_version)",
        "segment_27": "config_list_openai_aoai\u200b",
        "segment_28": "def config_list_openai_aoai( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", openai_api_base_file: Optional[str] = \"base_openai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None) -> List[Dict]",
        "segment_29": "Get a list of configs for OpenAI API client (including Azure or local model deployments that support OpenAI's chat completion API).",
        "segment_30": "This function constructs configurations by reading API keys and base URLs from environment variables or text files.",
        "segment_31": "It supports configurations for both OpenAI and Azure OpenAI services, allowing for the exclusion of one or the other.",
        "segment_32": "When text files are used, the environment variables will be overwritten.",
        "segment_33": "To prevent text files from being used, set the corresponding file name to None.",
        "segment_34": "Or set key_file_path to None to disallow reading from text files.",
        "segment_35": "Arguments:",
        "segment_37": "key_file_path str, optional - The directory path where the API key files are located. Defaults to the current directory.",
        "segment_38": "openai_api_key_file str, optional - The filename containing the OpenAI API key. Defaults to 'key_openai.txt'.",
        "segment_39": "aoai_api_key_file str, optional - The filename containing the Azure OpenAI API key. Defaults to 'key_aoai.txt'.",
        "segment_40": "openai_api_base_file str, optional - The filename containing the OpenAI API base URL. Defaults to 'base_openai.txt'.",
        "segment_41": "aoai_api_base_file str, optional - The filename containing the Azure OpenAI API base URL. Defaults to 'base_aoai.txt'.",
        "segment_42": "exclude str, optional - The API type to exclude from the configuration list. Can be 'openai' or 'aoai'. Defaults to None.",
        "segment_44": "Returns:",
        "segment_46": "List[Dict] - A list of configuration dictionaries. Each dictionary contains keys for 'api_key',",
        "segment_47": "and optionally 'base_url', 'api_type', and 'api_version'.",
        "segment_49": "Raises:",
        "segment_51": "FileNotFoundError - If the specified key files are not found and the corresponding API key is not set in the environment variables.",
        "segment_53": "Example:",
        "segment_54": "To generate configurations excluding Azure OpenAI:",
        "segment_55": "configs = config_list_openai_aoai(exclude='aoai')",
        "segment_56": "File samples:",
        "segment_59": "key_aoai.txt",
        "segment_60": "aoai-12345abcdef67890ghijklmnopqraoai-09876zyxwvuts54321fedcba",
        "segment_63": "base_aoai.txt",
        "segment_64": "https://api.azure.com/v1https://api.azure2.com/v1",
        "segment_67": "Notes:",
        "segment_69": "The function checks for API keys and base URLs in the following environment variables: 'OPENAI_API_KEY', 'AZURE_OPENAI_API_KEY',",
        "segment_70": "'OPENAI_API_BASE' and 'AZURE_OPENAI_API_BASE'. If these are not found, it attempts to read from the specified files in the",
        "segment_71": "'key_file_path' directory.",
        "segment_72": "The API version for Azure configurations is set to DEFAULT_AZURE_API_VERSION by default.",
        "segment_73": "If 'exclude' is set to 'openai', only Azure OpenAI configurations are returned, and vice versa.",
        "segment_74": "The function assumes that the API keys and base URLs in the environment variables are separated by new lines if there are",
        "segment_75": "multiple entries.",
        "segment_77": "config_list_from_models\u200b",
        "segment_78": "def config_list_from_models( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None, model_list: Optional[list] = None) -> List[Dict]",
        "segment_79": "Get a list of configs for API calls with models specified in the model list.",
        "segment_80": "This function extends config_list_openai_aoai by allowing to clone its' out for each of the models provided.",
        "segment_81": "Each configuration will have a 'model' key with the model name as its value. This is particularly useful when",
        "segment_82": "all endpoints have same set of models.",
        "segment_83": "Arguments:",
        "segment_85": "key_file_path str, optional - The path to the key files.",
        "segment_86": "openai_api_key_file str, optional - The file name of the OpenAI API key.",
        "segment_87": "aoai_api_key_file str, optional - The file name of the Azure OpenAI API key.",
        "segment_88": "aoai_api_base_file str, optional - The file name of the Azure OpenAI API base.",
        "segment_89": "exclude str, optional - The API type to exclude, \"openai\" or \"aoai\".",
        "segment_90": "model_list list, optional - The list of model names to include in the configs.",
        "segment_92": "Returns:",
        "segment_94": "list - A list of configs for OpenAI API calls, each including model information.",
        "segment_96": "Example:",
        "segment_97": "# Define the path where the API key files are located key_file_path = '/path/to/key/files' # Define the file names for the OpenAI and Azure OpenAI API keys and bases openai_api_key_file = 'key_openai.txt' aoai_api_key_file = 'key_aoai.txt' aoai_api_base_file = 'base_aoai.txt' # Define the list of models for which to create configurations model_list = ['gpt-4', 'gpt-3.5-turbo'] # Call the function to get a list of configuration dictionaries config_list = config_list_from_models( key_file_path=key_file_path, openai_api_key_file=openai_api_key_file, aoai_api_key_file=aoai_api_key_file, aoai_api_base_file=aoai_api_base_file, model_list=model_list ) # The `config_list` will contain configurations for the specified models, for example: # [ # {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-4'}, # {'api_key': '...', 'base_url': 'https://api.openai.com', 'model': 'gpt-3.5-turbo'} # ]",
        "segment_98": "config_list_gpt4_gpt35\u200b",
        "segment_99": "def config_list_gpt4_gpt35( key_file_path: Optional[str] = \".\", openai_api_key_file: Optional[str] = \"key_openai.txt\", aoai_api_key_file: Optional[str] = \"key_aoai.txt\", aoai_api_base_file: Optional[str] = \"base_aoai.txt\", exclude: Optional[str] = None) -> List[Dict]",
        "segment_100": "Get a list of configs for 'gpt-4' followed by 'gpt-3.5-turbo' API calls.",
        "segment_101": "Arguments:",
        "segment_103": "key_file_path str, optional - The path to the key files.",
        "segment_104": "openai_api_key_file str, optional - The file name of the openai api key.",
        "segment_105": "aoai_api_key_file str, optional - The file name of the azure openai api key.",
        "segment_106": "aoai_api_base_file str, optional - The file name of the azure openai api base.",
        "segment_107": "exclude str, optional - The api type to exclude, \"openai\" or \"aoai\".",
        "segment_109": "Returns:",
        "segment_111": "list - A list of configs for openai api calls.",
        "segment_113": "filter_config\u200b",
        "segment_114": "def filter_config(config_list, filter_dict)",
        "segment_115": "This function filters config_list by checking each configuration dictionary against the",
        "segment_116": "criteria specified in filter_dict. A configuration dictionary is retained if for every",
        "segment_117": "key in filter_dict, see example below.",
        "segment_118": "Arguments:",
        "segment_121": "config_list list of dict - A list of configuration dictionaries to be filtered.",
        "segment_124": "filter_dict dict - A dictionary representing the filter criteria, where each key is a",
        "segment_125": "field name to check within the configuration dictionaries, and the",
        "segment_126": "corresponding value is a list of acceptable values for that field.",
        "segment_127": "If the configuration's field's value is not a list, then a match occurs",
        "segment_128": "when it is found in the list of acceptable values. If the configuration's",
        "segment_129": "field's value is a list, then a match occurs if there is a non-empty",
        "segment_130": "intersection with the acceptable values.",
        "segment_133": "Returns:",
        "segment_134": "list of dict: A list of configuration dictionaries that meet all the criteria specified",
        "segment_135": "in filter_dict.",
        "segment_136": "Example:",
        "segment_137": "# Example configuration list with various models and API types configs = [ {'model': 'gpt-3.5-turbo'}, {'model': 'gpt-4'}, {'model': 'gpt-3.5-turbo', 'api_type': 'azure'}, {'model': 'gpt-3.5-turbo', 'tags': ['gpt35_turbo', 'gpt-35-turbo']}, ] # Define filter criteria to select configurations for the 'gpt-3.5-turbo' model # that are also using the 'azure' API type filter_criteria = { 'model': ['gpt-3.5-turbo'], # Only accept configurations for 'gpt-3.5-turbo' 'api_type': ['azure'] # Only accept configurations for 'azure' API type } # Apply the filter to the configuration list filtered_configs = filter_config(configs, filter_criteria) # The resulting `filtered_configs` will be: # [{'model': 'gpt-3.5-turbo', 'api_type': 'azure', ...}] # Define a filter to select a given tag filter_criteria = { 'tags': ['gpt35_turbo'], } # Apply the filter to the configuration list filtered_configs = filter_config(configs, filter_criteria) # The resulting `filtered_configs` will be: # [{'model': 'gpt-3.5-turbo', 'tags': ['gpt35_turbo', 'gpt-35-turbo']}]",
        "segment_138": "Notes:",
        "segment_140": "If filter_dict is empty or None, no filtering is applied and config_list is returned as is.",
        "segment_141": "If a configuration dictionary in config_list does not contain a key specified in filter_dict,",
        "segment_142": "it is considered a non-match and is excluded from the result.",
        "segment_143": "If the list of acceptable values for a key in filter_dict includes None, then configuration",
        "segment_144": "dictionaries that do not have that key will also be considered a match.",
        "segment_146": "config_list_from_json\u200b",
        "segment_147": "def config_list_from_json( env_or_file: str, file_location: Optional[str] = \"\", filter_dict: Optional[Dict[str, Union[List[Union[str, None]], Set[Union[str, None]]]]] = None) -> List[Dict]",
        "segment_148": "Retrieves a list of API configurations from a JSON stored in an environment variable or a file.",
        "segment_149": "This function attempts to parse JSON data from the given env_or_file parameter. If env_or_file is an",
        "segment_150": "environment variable containing JSON data, it will be used directly. Otherwise, it is assumed to be a filename,",
        "segment_151": "and the function will attempt to read the file from the specified file_location.",
        "segment_152": "The filter_dict parameter allows for filtering the configurations based on specified criteria. Each key in the",
        "segment_153": "filter_dict corresponds to a field in the configuration dictionaries, and the associated value is a list or set",
        "segment_154": "of acceptable values for that field. If a field is missing in a configuration and None is included in the list",
        "segment_155": "of acceptable values for that field, the configuration will still be considered a match.",
        "segment_156": "Arguments:",
        "segment_158": "env_or_file str - The name of the environment variable, the filename, or the environment variable of the filename",
        "segment_159": "that containing the JSON data.",
        "segment_160": "file_location str, optional - The directory path where the file is located, if env_or_file is a filename.",
        "segment_161": "filter_dict dict, optional - A dictionary specifying the filtering criteria for the configurations, with",
        "segment_162": "keys representing field names and values being lists or sets of acceptable values for those fields.",
        "segment_164": "Example:",
        "segment_165": "env_or_file0",
        "segment_166": "Returns:",
        "segment_168": "env_or_file1 - A list of configuration dictionaries that match the filtering criteria specified in filter_dict.",
        "segment_170": "Raises:",
        "segment_172": "env_or_file3 - if env_or_file is neither found as an environment variable nor a file",
        "segment_174": "get_config\u200b",
        "segment_175": "def get_config(api_key: str, base_url: Optional[str] = None, api_type: Optional[str] = None, api_version: Optional[str] = None) -> Dict",
        "segment_176": "Constructs a configuration dictionary for a single model with the provided API configurations.",
        "segment_177": "Example:",
        "segment_178": "config = get_config( api_key=\"sk-abcdef1234567890\", base_url=\"https://api.openai.com\", api_version=\"v1\")# The 'config' variable will now contain:# {# \"api_key\": \"sk-abcdef1234567890\",# \"base_url\": \"https://api.openai.com\",# \"api_version\": \"v1\"# }",
        "segment_179": "Arguments:",
        "segment_181": "api_key str - The API key for authenticating API requests.",
        "segment_182": "base_url Optional[str] - The base URL of the API. If not provided, defaults to None.",
        "segment_183": "api_type Optional[str] - The type of API. If not provided, defaults to None.",
        "segment_184": "api_version Optional[str] - The version of the API. If not provided, defaults to None.",
        "segment_186": "Returns:",
        "segment_188": "Dict - A dictionary containing the provided API configurations.",
        "segment_190": "config_list_from_dotenv\u200b",
        "segment_191": "def config_list_from_dotenv( dotenv_file_path: Optional[str] = None, model_api_key_map: Optional[dict] = None, filter_dict: Optional[dict] = None) -> List[Dict[str, Union[str, Set[str]]]]",
        "segment_192": "Load API configurations from a specified .env file or environment variables and construct a list of configurations.",
        "segment_193": "This function will:",
        "segment_195": "Load API keys from a provided .env file or from existing environment variables.",
        "segment_196": "Create a configuration dictionary for each model using the API keys and additional configurations.",
        "segment_197": "Filter and return the configurations based on provided filters.",
        "segment_199": "model_api_key_map will default to {\"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": \"OPENAI_API_KEY\"} if none",
        "segment_200": "Arguments:",
        "segment_202": "dotenv_file_path str, optional - The path to the .env file. Defaults to None.",
        "segment_203": "model_api_key_map str/dict, optional - A dictionary mapping models to their API key configurations.",
        "segment_204": "If a string is provided as configuration, it is considered as an environment",
        "segment_205": "variable name storing the API key.",
        "segment_206": "If a dict is provided, it should contain at least 'api_key_env_var' key,",
        "segment_207": "and optionally other API configurations like 'base_url', 'api_type', and 'api_version'.",
        "segment_208": "Defaults to a basic map with 'gpt-4' and 'gpt-3.5-turbo' mapped to 'OPENAI_API_KEY'.",
        "segment_209": "filter_dict dict, optional - A dictionary containing the models to be loaded.",
        "segment_210": "Containing a 'model' key mapped to a set of model names to be loaded.",
        "segment_211": "Defaults to None, which loads all found configurations.",
        "segment_213": "Returns:",
        "segment_214": "List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model.",
        "segment_215": "Raises:",
        "segment_217": "FileNotFoundError - If the specified .env file does not exist.",
        "segment_218": "TypeError - If an unsupported type of configuration is provided in model_api_key_map.",
        "segment_220": "retrieve_assistants_by_name\u200b",
        "segment_221": "def retrieve_assistants_by_name(client: OpenAI, name: str) -> List[Assistant]",
        "segment_222": "Return the assistants with the given name from OAI assistant APIEdit this pagePreviouscompletionNextagent_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen now supports custom models! This feature empowers users to define and load their own models, allowing for a more flexible and personalized inference mechanism. By adhering to a specific protocol, you can integrate your custom model for use with AutoGen and respond to prompts any way needed by using any model/API call/hardcoded response you want.",
        "segment_2": "NOTE: Depending on what model you use, you may need to play with the default prompts of the Agent's",
        "segment_3": "Quickstart\u200b",
        "segment_4": "An interactive and easy way to get started is by following the notebook here which loads a local model from HuggingFace into AutoGen and uses it for inference, and making changes to the class provided.",
        "segment_5": "Step 1: Create the custom model client class\u200b",
        "segment_6": "To get started with using custom models in AutoGen, you need to create a model client class that adheres to the ModelClient protocol defined in client.py. The new model client class should implement these methods:",
        "segment_8": "create(): Returns a response object that implements the ModelClientResponseProtocol (more details in the Protocol section).",
        "segment_9": "message_retrieval(): Processes the response object and returns a list of strings or a list of message objects (more details in the Protocol section).",
        "segment_10": "cost(): Returns the cost of the response.",
        "segment_11": "get_usage(): Returns a dictionary with keys from RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"].",
        "segment_13": "E.g. of a bare bones dummy custom class:",
        "segment_14": "class CustomModelClient: def __init__(self, config, **kwargs): print(f\"CustomModelClient config: {config}\") def create(self, params): num_of_responses = params.get(\"n\", 1) # can create my own data response class # here using SimpleNamespace for simplicity # as long as it adheres to the ModelClientResponseProtocol response = SimpleNamespace() response.choices = [] response.model = \"model_name\" # should match the OAI_CONFIG_LIST registration for _ in range(num_of_responses): text = \"this is a dummy text response\" choice = SimpleNamespace() choice.message = SimpleNamespace() choice.message.content = text choice.message.function_call = None response.choices.append(choice) return response def message_retrieval(self, response): choices = response.choices return [choice.message.content for choice in choices] def cost(self, response) -> float: response.cost = 0 return 0 @staticmethod def get_usage(response): return {}",
        "segment_15": "Step 2: Add the configuration to the OAI_CONFIG_LIST\u200b",
        "segment_16": "The field that is necessary is setting model_client_cls to the name of the new class (as a string) \"model_client_cls\":\"CustomModelClient\". Any other fields will be forwarded to the class constructor, so you have full control over what parameters to specify and how to use them. E.g.:",
        "segment_17": "{ \"model\": \"Open-Orca/Mistral-7B-OpenOrca\", \"model_client_cls\": \"CustomModelClient\", \"device\": \"cuda\", \"n\": 1, \"params\": { \"max_length\": 1000, }}",
        "segment_18": "Step 3: Register the new custom model to the agent that will use it\u200b",
        "segment_19": "If a configuration with the field \"model_client_cls\":\"\" has been added to an Agent's config list, then the corresponding model with the desired class must be registered after the agent is created and before the conversation is initialized:",
        "segment_20": "my_agent.register_model_client(model_client_cls=CustomModelClient, [other args that will be forwarded to CustomModelClient constructor])",
        "segment_21": "model_client_cls=CustomModelClient arg matches the one specified in the OAI_CONFIG_LIST and CustomModelClient is the class that adheres to the ModelClient protocol (more details on the protocol below).",
        "segment_22": "If the new model client is in the config list but not registered by the time the chat is initialized, then an error will be raised.",
        "segment_23": "Protocol details\u200b",
        "segment_24": "A custom model class can be created in many ways, but needs to adhere to the ModelClient protocol and response structure which is defined in client.py and shown below.",
        "segment_25": "The response protocol is currently using the minimum required fields from the autogen codebase that match the OpenAI response structure. Any response protocol that matches the OpenAI response structure will probably be more resilient to future changes, but we are starting off with minimum requirements to make adpotion of this feature easier.",
        "segment_26": "class ModelClient(Protocol): \"\"\" A client class must implement the following methods: - create must return a response object that implements the ModelClientResponseProtocol - cost must return the cost of the response - get_usage must return a dict with the following keys: - prompt_tokens - completion_tokens - total_tokens - cost - model This class is used to create a client that can be used by OpenAIWrapper. The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed. The message_retrieval method must be implemented to return a list of str or a list of messages from the response. \"\"\" RESPONSE_USAGE_KEYS = [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\", \"cost\", \"model\"] class ModelClientResponseProtocol(Protocol): class Choice(Protocol): class Message(Protocol): content: Optional[str] message: Message choices: List[Choice] model: str def create(self, params) -> ModelClientResponseProtocol: ... def message_retrieval( self, response: ModelClientResponseProtocol ) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]: \"\"\" Retrieve and return a list of strings or a list of Choice.Message from the response. NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used. \"\"\" ... def cost(self, response: ModelClientResponseProtocol) -> float: ... @staticmethod def get_usage(response: ModelClientResponseProtocol) -> Dict: \"\"\"Return usage summary of the response using RESPONSE_USAGE_KEYS.\"\"\" ...",
        "segment_27": "Troubleshooting steps\u200b",
        "segment_28": "If something doesn't work then run through the checklist:",
        "segment_30": "Make sure you have followed the client protocol and client response protocol when creating the custom model class",
        "segment_32": "create() method: ModelClientResponseProtocol must be followed when returning an inference response during create call.",
        "segment_33": "message_retrieval() method: returns a list of strings or a list of message objects. If a list of message objects is returned, they currently must contain the fields of OpenAI's ChatCompletion Message object, since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.",
        "segment_34": "cost()method: returns an integer, and if you don't care about cost tracking you can just return 0.",
        "segment_35": "get_usage(): returns a dictionary, and if you don't care about usage tracking you can just return an empty dictionary {}.",
        "segment_38": "Make sure you have a corresponding entry in the OAI_CONFIG_LIST and that that entry has the \"model_client_cls\":\"\" field.",
        "segment_39": "Make sure you have registered the client using the corresponding config entry and your new class agent.register_model_client(model_client_cls=, [other optional args])",
        "segment_40": "Make sure that all of the custom models defined in the OAI_CONFIG_LIST have been registered.",
        "segment_41": "Any other troubleshooting might need to be done in the custom code itself.",
        "segment_43": "Conclusion\u200b",
        "segment_44": "With the ability to use custom models, AutoGen now offers even more flexibility and power for your AI applications. Whether you've trained your own model or want to use a specific pre-trained model, AutoGen can accommodate your needs. Happy coding!Tags:AutoGenOlder PostAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsTL;DRQuickstartStep 1: Create the custom model client classStep 2: Add the configuration to the OAI_CONFIG_LISTStep 3: Register the new custom model to the agent that will use itProtocol detailsTroubleshooting stepsConclusionCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class GPTAssistantAgent(ConversableAgent)",
        "segment_2": "An experimental AutoGen agent class that leverages the OpenAI Assistant API for conversational capabilities.",
        "segment_3": "This agent is unique in its reliance on the OpenAI Assistant for state management, differing from other agents like ConversableAgent.",
        "segment_4": "__init__\u200b",
        "segment_5": "def __init__(name=\"GPT Assistant\", instructions: Optional[str] = None, llm_config: Optional[Union[Dict, bool]] = None, overwrite_instructions: bool = False, overwrite_tools: bool = False, **kwargs)",
        "segment_6": "Arguments:",
        "segment_8": "name str - name of the agent. It will be used to find the existing assistant by name. Please remember to delete an old assistant with the same name if you intend to create a new assistant with the same name.",
        "segment_9": "instructions str - instructions for the OpenAI assistant configuration.",
        "segment_10": "When instructions is not None, the system message of the agent will be",
        "segment_11": "set to the provided instructions and used in the assistant run, irrespective",
        "segment_12": "of the overwrite_instructions flag. But when instructions is None,",
        "segment_13": "and the assistant does not exist, the system message will be set to",
        "segment_14": "AssistantAgent.DEFAULT_SYSTEM_MESSAGE. If the assistant exists, the",
        "segment_15": "system message will be set to the existing assistant instructions.",
        "segment_16": "llm_config dict or False - llm inference configuration.",
        "segment_18": "assistant_id: ID of the assistant to use. If None, a new assistant will be created.",
        "segment_19": "model: Model to use for the assistant (gpt-4-1106-preview, gpt-3.5-turbo-1106).",
        "segment_20": "check_every_ms: check thread run status interval",
        "segment_21": "tools: Give Assistants access to OpenAI-hosted tools like Code Interpreter and Knowledge Retrieval,",
        "segment_22": "or build your own tools using Function calling. ref https://platform.openai.com/docs/assistants/tools",
        "segment_23": "file_ids: files used by retrieval in run",
        "segment_26": "overwrite_instructions bool - whether to overwrite the instructions of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.",
        "segment_27": "overwrite_tools bool - whether to overwrite the tools of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.",
        "segment_28": "kwargs dict - Additional configuration options for the agent.",
        "segment_30": "verbose (bool): If set to True, enables more detailed output from the assistant thread.",
        "segment_31": "Other kwargs: Except verbose, others are passed directly to ConversableAgent.",
        "segment_35": "can_execute_function\u200b",
        "segment_36": "def can_execute_function(name: str) -> bool",
        "segment_37": "Whether the agent can execute the function.",
        "segment_38": "reset\u200b",
        "segment_39": "def reset()",
        "segment_40": "Resets the agent, clearing any existing conversation thread and unread message indices.",
        "segment_41": "clear_history\u200b",
        "segment_42": "def clear_history(agent: Optional[Agent] = None)",
        "segment_43": "Clear the chat history of the agent.",
        "segment_44": "Arguments:",
        "segment_46": "agent - the agent with whom the chat history to clear. If None, clear the chat history with all agents.",
        "segment_48": "pretty_print_thread\u200b",
        "segment_49": "def pretty_print_thread(thread)",
        "segment_50": "Pretty print the thread.",
        "segment_51": "oai_threads\u200b",
        "segment_52": "@propertydef oai_threads() -> Dict[Agent, Any]",
        "segment_53": "Return the threads of the agent.",
        "segment_54": "assistant_id\u200b",
        "segment_55": "@propertydef assistant_id()",
        "segment_56": "Return the assistant id",
        "segment_57": "get_assistant_instructions\u200b",
        "segment_58": "def get_assistant_instructions()",
        "segment_59": "Return the assistant instructions from OAI assistant API",
        "segment_60": "delete_assistant\u200b",
        "segment_61": "def delete_assistant()",
        "segment_62": "Delete the assistant from OAI assistant API",
        "segment_63": "find_matching_assistant\u200b",
        "segment_64": "def find_matching_assistant(candidate_assistants, instructions, tools, file_ids)",
        "segment_65": "Find the matching assistant from a list of candidate assistants.",
        "segment_66": "Filter out candidates with the same name but different instructions, file IDs, and function names.",
        "segment_67": "TODO: implement accurate match based on assistant metadata fields.Edit this pagePreviouscompressible_agentNextimg_utilsGPTAssistantAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesExamplesContributingResearchOn this pageContributingThis project welcomes and encourages all forms of contributions, including but not limited to:Pushing patches.Code review of pull requests.Documentation, examples and test cases.Readability improvement, e.g., improvement on docstr and comments.Community participation in issues, discussions, and discord.Tutorials, blog posts, talks that promote the project.Sharing application scenarios and/or related research.You can take a look at the Roadmap for Upcoming Features to identify potential things to work on.Most contributions require you to agree to a",
        "segment_2": "Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us",
        "segment_3": "the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.If you are new to GitHub here is a detailed help source on getting involved with development on GitHub.When you submit a pull request, a CLA bot will automatically determine whether you need to provide",
        "segment_4": "a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions",
        "segment_5": "provided by the bot. You will only need to do this once across all repos using our CLA.This project has adopted the Microsoft Open Source Code of Conduct.",
        "segment_6": "For more information see the Code of Conduct FAQ or",
        "segment_7": "contact opencode@microsoft.com with any additional questions or comments.How to make a good bug report\u200bWhen you submit an issue to GitHub, please do your best to",
        "segment_8": "follow these guidelines! This will make it a lot easier to provide you with good",
        "segment_9": "feedback:The ideal bug report contains a short reproducible code snippet. This way",
        "segment_10": "anyone can try to reproduce the bug easily (see this for more details). If your snippet is",
        "segment_11": "longer than around 50 lines, please link to a gist or a GitHub repo.If an exception is raised, please provide the full traceback.Please include your operating system type and version number, as well as",
        "segment_12": "your Python, flaml, scikit-learn versions. The version of flaml",
        "segment_13": "can be found by running the following code snippet:import flamlprint(flaml.__version__)CopyPlease ensure all code snippets and error messages are formatted in",
        "segment_14": "appropriate code blocks. See Creating and highlighting code blocks",
        "segment_15": "for more details.Becoming a Reviewer\u200bThere is currently no formal reviewer solicitation process. Current reviewers identify reviewers from active contributors. If you are willing to become a reviewer, you are welcome to let us know on discord.Developing\u200bSetup\u200bgit clone https://github.com/microsoft/FLAML.gitpip install -e FLAML[notebook,autogen]CopyIn case the pip install command fails, try escaping the brackets such as pip install -e FLAML\\[notebook,autogen\\].Docker\u200bWe provide a simple Dockerfile.docker build https://github.com/microsoft/FLAML.git#main -t flaml-devdocker run -it flaml-devCopyDevelop in Remote Container\u200bIf you use vscode, you can open the FLAML folder in a Container.",
        "segment_16": "We have provided the configuration in devcontainer.Pre-commit\u200bRun pre-commit install to install pre-commit into your git hooks. Before you commit, run",
        "segment_17": "pre-commit run to check if you meet the pre-commit requirements. If you use Windows (without WSL) and can't commit after installing pre-commit, you can run pre-commit uninstall to uninstall the hook. In WSL or Linux this is supposed to work.Coverage\u200bAny code you commit should not decrease coverage. To run all unit tests, install the [test] option under FLAML/:pip install -e.\"[test]\"coverage run -m pytest testCopyThen you can see the coverage report by",
        "segment_18": "coverage report -m or coverage html.Documentation\u200bTo build and test documentation locally, install Node.js. For example,nvm install --ltsCopyThen:npm install --global yarn # skip if you use the dev container we providedpip install pydoc-markdown==4.5.0 # skip if you use the dev container we providedcd websiteyarn install --frozen-lockfile --ignore-enginespydoc-markdownyarn startCopyThe last command starts a local development server and opens up a browser window.",
        "segment_19": "Most changes are reflected live without having to restart the server.Note:",
        "segment_20": "some tips in this guide are based off the contributor guide from ray, scikit-learn, or hummingbird.Edit this pagePrevious\u00ab Tune - PyTorchNextResearch \u00bbHow to make a good bug reportBecoming a ReviewerDevelopingSetupDockerDevelop in Remote ContainerPre-commitCoverageDocumentationCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "class SimpleTextBrowser()",
        "segment_2": "(In preview) An extremely simple text-based web browser comparable to Lynx. Suitable for Agentic use.",
        "segment_3": "address\u200b",
        "segment_4": "@propertydef address() -> str",
        "segment_5": "Return the address of the current page.",
        "segment_6": "viewport\u200b",
        "segment_7": "@propertydef viewport() -> str",
        "segment_8": "Return the content of the current viewport.",
        "segment_9": "page_content\u200b",
        "segment_10": "@propertydef page_content() -> str",
        "segment_11": "Return the full contents of the current page.",
        "segment_12": "visit_page\u200b",
        "segment_13": "def visit_page(path_or_uri)",
        "segment_14": "Update the address, visit the page, and return the content of the viewport.Edit this pagePreviousagent_utilsNextcode_utilsSimpleTextBrowser ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class TextAnalyzerAgent(ConversableAgent)",
        "segment_2": "(Experimental) Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed.",
        "segment_3": "__init__\u200b",
        "segment_4": "def __init__(name=\"analyzer\", system_message: Optional[str] = system_message, human_input_mode: Optional[str] = \"NEVER\", llm_config: Optional[Union[Dict, bool]] = None, **kwargs)",
        "segment_5": "Arguments:",
        "segment_7": "name str - name of the agent.",
        "segment_8": "system_message str - system message for the ChatCompletion inference.",
        "segment_9": "human_input_mode str - This agent should NEVER prompt the human for input.",
        "segment_10": "llm_config dict or False - llm inference configuration.",
        "segment_11": "Please refer to OpenAIWrapper.create",
        "segment_12": "for available options.",
        "segment_13": "To disable llm-based auto reply, set to False.",
        "segment_14": "**kwargs dict - other kwargs in ConversableAgent.",
        "segment_16": "analyze_text\u200b",
        "segment_17": "def analyze_text(text_to_analyze, analysis_instructions)",
        "segment_18": "Analyzes the given text as instructed, and returns the analysis.Edit this pagePrevioussociety_of_mind_agentNextweb_surferTextAnalyzerAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Completion(openai_Completion)",
        "segment_2": "(openai<1) A class for OpenAI completion API.",
        "segment_3": "It also supports: ChatCompletion, Azure OpenAI API.",
        "segment_4": "set_cache\u200b",
        "segment_5": "@classmethoddef set_cache(cls, seed: Optional[int] = 41, cache_path_root: Optional[str] = \".cache\")",
        "segment_6": "Set cache path.",
        "segment_7": "Arguments:",
        "segment_9": "seed int, Optional - The integer identifier for the pseudo seed.",
        "segment_10": "Results corresponding to different seeds will be cached in different places.",
        "segment_11": "cache_path str, Optional - The root path for the cache.",
        "segment_12": "The complete cache path will be {cache_path_root}/{seed}.",
        "segment_14": "clear_cache\u200b",
        "segment_15": "@classmethoddef clear_cache(cls, seed: Optional[int] = None, cache_path_root: Optional[str] = \".cache\")",
        "segment_16": "Clear cache.",
        "segment_17": "Arguments:",
        "segment_19": "seed int, Optional - The integer identifier for the pseudo seed.",
        "segment_20": "If omitted, all caches under cache_path_root will be cleared.",
        "segment_21": "cache_path str, Optional - The root path for the cache.",
        "segment_22": "The complete cache path will be {cache_path_root}/{seed}.",
        "segment_24": "tune\u200b",
        "segment_25": "@classmethoddef tune(cls, data: List[Dict], metric: str, mode: str, eval_func: Callable, log_file_name: Optional[str] = None, inference_budget: Optional[float] = None, optimization_budget: Optional[float] = None, num_samples: Optional[int] = 1, logging_level: Optional[int] = logging.WARNING, **config)",
        "segment_26": "Tune the parameters for the OpenAI API call.",
        "segment_27": "TODO: support parallel tuning with ray or spark.",
        "segment_28": "TODO: support agg_method as in test",
        "segment_29": "Arguments:",
        "segment_31": "data list - The list of data points.",
        "segment_32": "metric str - The metric to optimize.",
        "segment_33": "mode str - The optimization mode, \"min\" or \"max.",
        "segment_34": "eval_func Callable - The evaluation function for responses.",
        "segment_35": "The function should take a list of responses and a data point as input,",
        "segment_36": "and return a dict of metrics. For example,",
        "segment_38": "def eval_func(responses, **data): solution = data[\"solution\"] success_list = [] n = len(responses) for i in range(n): response = responses[i] succeed = is_equiv_chain_of_thought(response, solution) success_list.append(succeed) return { \"expected_success\": 1 - pow(1 - sum(success_list) / n, n), \"success\": any(s for s in success_list), }",
        "segment_40": "log_file_name str, optional - The log file.",
        "segment_41": "inference_budget float, optional - The inference budget, dollar per instance.",
        "segment_42": "optimization_budget float, optional - The optimization budget, dollar in total.",
        "segment_43": "num_samples int, optional - The number of samples to evaluate.",
        "segment_44": "-1 means no hard restriction in the number of trials",
        "segment_45": "and the actual number is decided by optimization_budget. Defaults to 1.",
        "segment_46": "logging_level optional - logging level. Defaults to logging.WARNING.",
        "segment_47": "metric0 dict - The search space to update over the default search.",
        "segment_48": "For prompt, please provide a string/Callable or a list of strings/Callables.",
        "segment_50": "If prompt is provided for chat models, it will be converted to messages under role \"user\".",
        "segment_51": "Do not provide both prompt and messages for chat models, but provide either of them.",
        "segment_52": "A string template will be used to generate a prompt for each data instance",
        "segment_53": "using metric1.",
        "segment_54": "A callable template will be used to generate a prompt for each data instance",
        "segment_55": "using metric2.",
        "segment_56": "For stop, please provide a string, a list of strings, or a list of lists of strings.",
        "segment_57": "For messages (chat models only), please provide a list of messages (for a single chat prefix)",
        "segment_58": "or a list of lists of messages (for multiple choices of chat prefix to choose from).",
        "segment_59": "Each message should be a dict with keys \"role\" and \"content\". The value of \"content\" can be a string/Callable template.",
        "segment_63": "Returns:",
        "segment_65": "metric3 - The optimized hyperparameter setting.",
        "segment_66": "metric4 - The tuning results.",
        "segment_68": "create\u200b",
        "segment_69": "@classmethoddef create(cls, context: Optional[Dict] = None, use_cache: Optional[bool] = True, config_list: Optional[List[Dict]] = None, filter_func: Optional[Callable[[Dict, Dict], bool]] = None, raise_on_ratelimit_or_timeout: Optional[bool] = True, allow_format_str_template: Optional[bool] = False, **config)",
        "segment_70": "Make a completion for a given context.",
        "segment_71": "Arguments:",
        "segment_73": "context Dict, Optional - The context to instantiate the prompt.",
        "segment_74": "It needs to contain keys that are used by the prompt template or the filter function.",
        "segment_75": "E.g., prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}.",
        "segment_76": "The actual prompt will be:",
        "segment_77": "\"Complete the following sentence: Today I feel\".",
        "segment_78": "More examples can be found at templating.",
        "segment_79": "use_cache bool, Optional - Whether to use cached responses.",
        "segment_80": "config_list List, Optional - List of configurations for the completion to try.",
        "segment_81": "The first one that does not raise an error will be used.",
        "segment_82": "Only the differences from the default config need to be provided.",
        "segment_83": "E.g.,",
        "segment_85": "response = oai.Completion.create( config_list=[ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-03-15-preview\", }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"api_type\": \"openai\", \"base_url\": \"https://api.openai.com/v1\", }, { \"model\": \"llama-7B\", \"base_url\": \"http://127.0.0.1:8080\", \"api_type\": \"openai\", } ], prompt=\"Hi\",)",
        "segment_87": "filter_func Callable, Optional - A function that takes in the context and the response and returns a boolean to indicate whether the response is valid. E.g.,",
        "segment_89": "def yes_or_no_filter(context, config, response): return context.get(\"yes_or_no_choice\", False) is False or any( text in [\"Yes.\", \"No.\"] for text in oai.Completion.extract_text(response) )",
        "segment_91": "raise_on_ratelimit_or_timeout bool, Optional - Whether to raise RateLimitError or Timeout when all configs fail.",
        "segment_92": "When set to False, -1 will be returned when all configs fail.",
        "segment_93": "allow_format_str_template bool, Optional - Whether to allow format string template in the config.",
        "segment_94": "**config - Configuration for the openai API call. This is used as parameters for calling openai API.",
        "segment_95": "The \"prompt\" or \"messages\" parameter can contain a template (str or Callable) which will be instantiated with the context.",
        "segment_96": "Besides the parameters for the openai API call, it can also contain:",
        "segment_98": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}0 (int): the total time (in seconds) allowed for retrying failed requests.",
        "segment_99": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}1 (int): the time interval to wait (in seconds) before retrying a failed request.",
        "segment_100": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}2 (int) for the cache. This is useful when implementing \"controlled randomness\" for the completion.",
        "segment_104": "Returns:",
        "segment_105": "Responses from OpenAI API, with additional fields.",
        "segment_107": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}3: the total cost.",
        "segment_108": "When config_list is provided, the response will contain a few more fields:",
        "segment_109": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}5: the index of the config in the config_list that is used to generate the response.",
        "segment_110": "prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}6: whether the response passes the filter function. None if no filter is provided.",
        "segment_112": "test\u200b",
        "segment_113": "@classmethoddef test(cls, data, eval_func=None, use_cache=True, agg_method=\"avg\", return_responses_and_per_instance_result=False, logging_level=logging.WARNING, **config)",
        "segment_114": "Evaluate the responses created with the config for the OpenAI API call.",
        "segment_115": "Arguments:",
        "segment_117": "data list - The list of test data points.",
        "segment_118": "eval_func Callable - The evaluation function for responses per data instance.",
        "segment_119": "The function should take a list of responses and a data point as input,",
        "segment_120": "and return a dict of metrics. You need to either provide a valid callable",
        "segment_121": "eval_func; or do not provide one (set None) but call the test function after",
        "segment_122": "calling the tune function in which a eval_func is provided.",
        "segment_123": "In the latter case we will use the eval_func provided via tune function.",
        "segment_124": "Defaults to None.",
        "segment_126": "def eval_func(responses, **data): solution = data[\"solution\"] success_list = [] n = len(responses) for i in range(n): response = responses[i] succeed = is_equiv_chain_of_thought(response, solution) success_list.append(succeed) return { \"expected_success\": 1 - pow(1 - sum(success_list) / n, n), \"success\": any(s for s in success_list), }",
        "segment_128": "use_cache bool, Optional - Whether to use cached responses. Defaults to True.",
        "segment_129": "agg_method str, Callable or a dict of Callable - Result aggregation method (across",
        "segment_130": "multiple instances) for each of the metrics. Defaults to 'avg'.",
        "segment_131": "An example agg_method in str:",
        "segment_133": "agg_method = 'median'",
        "segment_134": "An example agg_method in a Callable:",
        "segment_135": "agg_method = np.median",
        "segment_136": "An example agg_method in a dict of Callable:",
        "segment_137": "agg_method={'median_success': np.median, 'avg_success': np.mean}",
        "segment_139": "return_responses_and_per_instance_result bool - Whether to also return responses",
        "segment_140": "and per instance results in addition to the aggregated results.",
        "segment_141": "logging_level optional - logging level. Defaults to logging.WARNING.",
        "segment_142": "eval_func0 dict - parameters passed to the openai api call eval_func1.",
        "segment_144": "Returns:",
        "segment_145": "None when no valid eval_func is provided in either test or tune;",
        "segment_146": "Otherwise, a dict of aggregated results, responses and per instance results if return_responses_and_per_instance_result is True;",
        "segment_147": "Otherwise, a dict of aggregated results (responses and per instance results are not returned).",
        "segment_148": "cost\u200b",
        "segment_149": "@classmethoddef cost(cls, response: dict)",
        "segment_150": "Compute the cost of an API call.",
        "segment_151": "Arguments:",
        "segment_153": "response dict - The response from OpenAI API.",
        "segment_155": "Returns:",
        "segment_156": "The cost in USD. 0 if the model is not supported.",
        "segment_157": "extract_text\u200b",
        "segment_158": "@classmethoddef extract_text(cls, response: dict) -> List[str]",
        "segment_159": "Extract the text from a completion or chat response.",
        "segment_160": "Arguments:",
        "segment_162": "response dict - The response from OpenAI API.",
        "segment_164": "Returns:",
        "segment_165": "A list of text in the responses.",
        "segment_166": "extract_text_or_function_call\u200b",
        "segment_167": "@classmethoddef extract_text_or_function_call(cls, response: dict) -> List[str]",
        "segment_168": "Extract the text or function calls from a completion or chat response.",
        "segment_169": "Arguments:",
        "segment_171": "response dict - The response from OpenAI API.",
        "segment_173": "Returns:",
        "segment_174": "A list of text or function calls in the responses.",
        "segment_175": "logged_history\u200b",
        "segment_176": "@classmethod@propertydef logged_history(cls) -> Dict",
        "segment_177": "Return the book keeping dictionary.",
        "segment_178": "print_usage_summary\u200b",
        "segment_179": "@classmethoddef print_usage_summary(cls) -> Dict",
        "segment_180": "Return the usage summary.",
        "segment_181": "start_logging\u200b",
        "segment_182": "@classmethoddef start_logging(cls, history_dict: Optional[Dict] = None, compact: Optional[bool] = True, reset_counter: Optional[bool] = True)",
        "segment_183": "Start book keeping.",
        "segment_184": "Arguments:",
        "segment_186": "history_dict Dict - A dictionary for book keeping.",
        "segment_187": "If no provided, a new one will be created.",
        "segment_188": "compact bool - Whether to keep the history dictionary compact.",
        "segment_189": "Compact history contains one key per conversation, and the value is a dictionary",
        "segment_190": "like:",
        "segment_192": "{ \"create_at\": [0, 1], \"cost\": [0.1, 0.2],}",
        "segment_193": "where \"created_at\" is the index of API calls indicating the order of all the calls,",
        "segment_194": "and \"cost\" is the cost of each call. This example shows that the conversation is based",
        "segment_195": "on two API calls. The compact format is useful for condensing the history of a conversation.",
        "segment_196": "If compact is False, the history dictionary will contain all the API calls: the key",
        "segment_197": "is the index of the API call, and the value is a dictionary like:",
        "segment_198": "{ \"request\": request_dict, \"response\": response_dict,}",
        "segment_199": "where request_dict is the request sent to OpenAI API, and response_dict is the response.",
        "segment_200": "For a conversation containing two API calls, the non-compact history dictionary will be like:",
        "segment_201": "{ 0: { \"request\": request_dict_0, \"response\": response_dict_0, }, 1: { \"request\": request_dict_1, \"response\": response_dict_1, },",
        "segment_202": "The first request's messages plus the response is equal to the second request's messages.",
        "segment_203": "For a conversation with many turns, the non-compact history dictionary has a quadratic size",
        "segment_204": "while the compact history dict has a linear size.",
        "segment_206": "reset_counter bool - whether to reset the counter of the number of API calls.",
        "segment_208": "stop_logging\u200b",
        "segment_209": "@classmethoddef stop_logging(cls)",
        "segment_210": "End book keeping.",
        "segment_211": "ChatCompletion Objects\u200b",
        "segment_212": "class ChatCompletion(Completion)",
        "segment_213": "(openai<1) A class for OpenAI API ChatCompletion. Share the same API as Completion.Edit this pagePreviousclientNextopenai_utilsCompletion ObjectsChatCompletion ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def llava_formatter(prompt: str, order_image_tokens: bool = False) -> Tuple[str, List[str]]",
        "segment_2": "Formats the input prompt by replacing image tags and returns the new prompt along with image locations.",
        "segment_3": "Arguments:",
        "segment_5": "prompt (str): The input string that may contain image tags like .",
        "segment_6": "order_image_tokens (bool, optional): Whether to order the image tokens with numbers.",
        "segment_7": "It will be useful for GPT-4V. Defaults to False.",
        "segment_9": "Returns:",
        "segment_11": "Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).",
        "segment_13": "gpt4v_formatter\u200b",
        "segment_14": "def gpt4v_formatter(prompt: str) -> List[Union[str, dict]]",
        "segment_15": "Formats the input prompt by replacing image tags and returns a list of text and images.",
        "segment_16": "Arguments:",
        "segment_18": "prompt (str): The input string that may contain image tags like .",
        "segment_20": "Returns:",
        "segment_22": "List[Union[str, dict]]: A list of alternating text and image dictionary items.",
        "segment_24": "extract_img_paths\u200b",
        "segment_25": "def extract_img_paths(paragraph: str) -> list",
        "segment_26": "Extract image paths (URLs or local paths) from a text paragraph.",
        "segment_27": "Arguments:",
        "segment_29": "paragraph str - The input text paragraph.",
        "segment_31": "Returns:",
        "segment_33": "list - A list of extracted image paths."
    },
    {
        "segment_1": "def solve_problem(problem: str, **config) -> str",
        "segment_2": "(openai<1) Solve the math problem.",
        "segment_3": "Arguments:",
        "segment_5": "problem str - The problem statement.",
        "segment_6": "config Optional, dict - The configuration for the API call.",
        "segment_8": "Returns:",
        "segment_10": "str - The solution to the problem.",
        "segment_12": "remove_boxed\u200b",
        "segment_13": "def remove_boxed(string: str) -> Optional[str]",
        "segment_14": "Source: https://github.com/hendrycks/math",
        "segment_15": "Extract the text within a \\boxed{...} environment.",
        "segment_16": "Example:",
        "segment_17": "> remove_boxed(\"\\boxed{\\frac{2}{3}}\")",
        "segment_18": "\\frac{2}{3}",
        "segment_19": "last_boxed_only_string\u200b",
        "segment_20": "def last_boxed_only_string(string: str) -> Optional[str]",
        "segment_21": "Source: https://github.com/hendrycks/math",
        "segment_22": "Extract the last \\boxed{...} or \\fbox{...} element from a string.",
        "segment_23": "is_equiv\u200b",
        "segment_24": "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float",
        "segment_25": "Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in",
        "segment_27": "units",
        "segment_28": "fractions",
        "segment_29": "square roots",
        "segment_30": "superfluous LaTeX.",
        "segment_31": "Source: https://github.com/hendrycks/math",
        "segment_33": "is_equiv_chain_of_thought\u200b",
        "segment_34": "def is_equiv_chain_of_thought(str1: str, str2: str) -> float",
        "segment_35": "Strips the solution first before calling is_equiv.",
        "segment_36": "eval_math_responses\u200b",
        "segment_37": "def eval_math_responses(responses, solution=None, **args)",
        "segment_38": "Select a response for a math problem using voting, and check if the response is correct if the solution is provided.",
        "segment_39": "Arguments:",
        "segment_41": "responses list - The list of responses.",
        "segment_42": "solution str - The canonical solution.",
        "segment_44": "Returns:",
        "segment_46": "dict - The success metrics.",
        "segment_47": "Edit this pagePreviousgraph_utilsNextretrieve_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "LLM Caching\u200b",
        "segment_2": "To use LLM caching with Redis, you need to install the Python package with",
        "segment_3": "the option redis:",
        "segment_4": "pip install \"pyautogen[redis]\"",
        "segment_5": "See LLM Caching for details.",
        "segment_6": "Docker\u200b",
        "segment_7": "Even if you install AutoGen locally, we highly recommend using Docker for code execution.",
        "segment_8": "To use docker for code execution, you also need to install the python package docker:",
        "segment_9": "pip install docker",
        "segment_10": "You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.:",
        "segment_11": "user_proxy = autogen.UserProxyAgent( name=\"agent\", human_input_mode=\"TERMINATE\", max_consecutive_auto_reply=10, code_execution_config={\"work_dir\":\"_output\", \"use_docker\":\"python:3\"}, llm_config=llm_config, system_message=\"\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")",
        "segment_12": "blendsearch\u200b",
        "segment_13": "pyautogen<0.2 offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. Please install with the [blendsearch] option to use it.",
        "segment_14": "pip install \"pyautogen[blendsearch]<0.2\"",
        "segment_15": "Example notebooks:",
        "segment_16": "Optimize for Code Generation",
        "segment_17": "Optimize for Math",
        "segment_18": "retrievechat\u200b",
        "segment_19": "pyautogen supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the [retrievechat] option to use it.",
        "segment_20": "pip install \"pyautogen[retrievechat]\"",
        "segment_21": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_22": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_23": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_24": "If you install unstructured",
        "segment_25": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_26": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_27": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_28": "Example notebooks:",
        "segment_29": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_30": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_31": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_32": "Teachability\u200b",
        "segment_33": "To use Teachability, please install AutoGen with the [teachable] option.",
        "segment_34": "pip install \"pyautogen[teachable]\"",
        "segment_35": "Example notebook: Chatting with a teachable agent",
        "segment_36": "Large Multimodal Model (LMM) Agents\u200b",
        "segment_37": "We offered Multimodal Conversable Agent and LLaVA Agent. Please install with the [lmm] option to use it.",
        "segment_38": "pip install \"pyautogen[lmm]\"",
        "segment_39": "Example notebooks:",
        "segment_40": "LLaVA Agent",
        "segment_41": "mathchat\u200b",
        "segment_42": "pyautogen<0.2 offers an experimental agent for math problem solving. Please install with the [mathchat] option to use it.",
        "segment_43": "pip install \"pyautogen[mathchat]<0.2\"",
        "segment_44": "Example notebooks:",
        "segment_45": "Using MathChat to Solve Math Problems",
        "segment_46": "Graph\u200b",
        "segment_47": "To use a graph in GroupChat, particularly for graph visualization, please install AutoGen with the [graph] option.",
        "segment_48": "pip install \"pyautogen[graph]\"",
        "segment_49": "Example notebook: Graph Modeling Language with using select_speakerEdit this pagePreviousDockerNextLLM Endpoint ConfigurationLLM CachingDockerblendsearchretrievechatTeachabilityLarge Multimodal Model (LMM) AgentsmathchatGraphCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Automated Multi Agent Chat\u200b",
        "segment_2": "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation via multi-agent conversation.",
        "segment_3": "Please find documentation about this feature here.",
        "segment_4": "Links to notebook examples:",
        "segment_7": "Code Generation, Execution, and Debugging",
        "segment_9": "Automated Task Solving with Code Generation, Execution & Debugging - View Notebook",
        "segment_10": "Automated Code Generation and Question Answering with Retrieval Augmented Agents - View Notebook",
        "segment_11": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents - View Notebook",
        "segment_15": "Multi-Agent Collaboration (>3 Agents)",
        "segment_17": "Automated Task Solving by Group Chat (with 3 group member agents and 1 manager agent) - View Notebook",
        "segment_18": "Automated Data Visualization by Group Chat (with 3 group member agents and 1 manager agent) - View Notebook",
        "segment_19": "Automated Complex Task Solving by Group Chat (with 6 group member agents and 1 manager agent) - View Notebook",
        "segment_20": "Automated Task Solving with Coding & Planning Agents - View Notebook",
        "segment_21": "Automated Task Solving with transition paths specified in a graph - View Notebook",
        "segment_22": "Running a group chat as an inner-monolgue via the SocietyOfMindAgent - View Notebook",
        "segment_26": "Applications",
        "segment_28": "Automated Chess Game Playing & Chitchatting by GPT-4 Agents - View Notebook",
        "segment_29": "Automated Continual Learning from New Data - View Notebook",
        "segment_30": "OptiGuide - Coding, Tool Using, Safeguarding & Question Answering for Supply Chain Optimization",
        "segment_34": "Tool Use",
        "segment_36": "Web Search: Solve Tasks Requiring Web Info - View Notebook",
        "segment_37": "Use Provided Tools as Functions - View Notebook",
        "segment_38": "Use Tools via Sync and Async Function Calling - View Notebook",
        "segment_39": "Task Solving with Langchain Provided Tools as Functions - View Notebook",
        "segment_40": "RAG: Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) - View Notebook",
        "segment_41": "Function Inception: Enable AutoGen agents to update/remove functions during conversations. - View Notebook",
        "segment_42": "Agent Chat with Whisper - View Notebook",
        "segment_43": "Constrained Responses via Guidance - View Notebook",
        "segment_44": "Browse the Web with Agents - View Notebook",
        "segment_45": "SQL: Natural Language Text to SQL Query using the Spider Text-to-SQL Benchmark - View Notebook",
        "segment_49": "Human Involvement",
        "segment_51": "Simple example in ChatGPT style View example",
        "segment_52": "Auto Code Generation, Execution, Debugging and Human Feedback - View Notebook",
        "segment_53": "Automated Task Solving with GPT-4 + Multiple Human Users - View Notebook",
        "segment_54": "Agent Chat with Async Human Inputs - View Notebook",
        "segment_58": "Agent Teaching and Learning",
        "segment_60": "Teach Agents New Skills & Reuse via Automated Chat - View Notebook",
        "segment_61": "Teach Agents New Facts, User Preferences and Skills Beyond Coding - View Notebook",
        "segment_62": "Teach OpenAI Assistants Through GPTAssistantAgent - View Notebook",
        "segment_63": "Agent Optimizer: Train Agents in an Agentic Way - View Notebook",
        "segment_67": "Multi-Agent Chat with OpenAI Assistants in the loop",
        "segment_69": "Hello-World Chat with OpenAi Assistant in AutoGen - View Notebook",
        "segment_70": "Chat with OpenAI Assistant using Function Call - View Notebook",
        "segment_71": "Chat with OpenAI Assistant with Code Interpreter - View Notebook",
        "segment_72": "Chat with OpenAI Assistant with Retrieval Augmentation - View Notebook",
        "segment_73": "OpenAI Assistant in a Group Chat - View Notebook",
        "segment_77": "Multimodal Agent",
        "segment_79": "Multimodal Agent Chat with DALLE and GPT-4V - View Notebook",
        "segment_80": "Multimodal Agent Chat with Llava - View Notebook",
        "segment_81": "Multimodal Agent Chat with GPT-4V - View Notebook",
        "segment_85": "Long Context Handling",
        "segment_87": "Conversations with Chat History Compression Enabled - View Notebook",
        "segment_91": "Evaluation and Assessment",
        "segment_93": "AgentEval: A Multi-Agent System for Assess Utility of LLM-powered Applications - View Notebook",
        "segment_97": "Automatic Agent Building",
        "segment_99": "Automatically Build Multi-agent System with AgentBuilder - View Notebook",
        "segment_100": "Automatically Build Multi-agent System from Agent Library - View Notebook",
        "segment_104": "Enhanced Inferences\u200b",
        "segment_105": "Utilities\u200b",
        "segment_107": "API Unification - View Documentation with Code Example",
        "segment_108": "Utility Functions to Help Managing API configurations effectively - View Notebook",
        "segment_109": "Cost Calculation - View Notebook",
        "segment_111": "Inference Hyperparameters Tuning\u200b",
        "segment_112": "AutoGen offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. The research study finds that tuning hyperparameters can significantly improve the utility of them.",
        "segment_113": "Please find documentation about this feature here.",
        "segment_114": "Links to notebook examples:",
        "segment_116": "Optimize for Code Generation | Open in colab",
        "segment_117": "Optimize for Math | Open in colab",
        "segment_118": "Edit this pageAutomated Multi Agent ChatEnhanced InferencesUtilitiesInference Hyperparameters TuningCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_4": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_5": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_7": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_8": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_9": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_10": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_11": "Experiment Setup\u200b",
        "segment_12": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_14": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_15": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_17": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_19": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_20": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_21": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_22": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_23": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_25": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_26": "Experiment Results\u200b",
        "segment_27": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_28": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_29": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_31": "However, the selected model changes on level 4 Algebra.",
        "segment_33": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_34": "On level 5 the result is similar.",
        "segment_36": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_37": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_38": "Analysis and Discussion\u200b",
        "segment_39": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_40": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_41": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_42": "For Further Reading\u200b",
        "segment_44": "Research paper about the tuning technique",
        "segment_45": "Documentation about inference tuning",
        "segment_47": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostAchieve More, Pay Less - Use GPT-4 SmartlyExperiment SetupExperiment ResultsAnalysis and DiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Docker, an indispensable tool in modern software development, offers a compelling solution for AutoGen's setup. Docker allows you to create consistent environments that are portable and isolated from the host OS. With Docker, everything AutoGen needs to run, from the operating system to specific libraries, is encapsulated in a container, ensuring uniform functionality across different systems. The Dockerfiles necessary for AutoGen are conveniently located in the project's GitHub repository at https://github.com/microsoft/autogen/tree/main/samples/dockers.",
        "segment_2": "Pre-configured DockerFiles: The AutoGen Project offers pre-configured Dockerfiles for your use. These Dockerfiles will run as is, however they can be modified to suit your development needs. Please see the README.md file in autogen/samples/dockers",
        "segment_4": "autogen_base_img: For a basic setup, you can use the autogen_base_img to run simple scripts or applications. This is ideal for general users or those new to AutoGen.",
        "segment_5": "autogen_full_img: Advanced users or those requiring more features can use autogen_full_img. Be aware that this version loads ALL THE THINGS and thus is very large. Take this into consideration if you build your application off of it.",
        "segment_7": "Step 1: Install Docker\u200b",
        "segment_10": "General Installation: Follow the official Docker installation instructions. This is your first step towards a containerized environment, ensuring a consistent and isolated workspace for AutoGen.",
        "segment_13": "For Mac Users: If you encounter issues with the Docker daemon, consider using colima. Colima offers a lightweight alternative to manage Docker containers efficiently on macOS.",
        "segment_16": "Step 2: Build a Docker Image\u200b",
        "segment_17": "AutoGen now provides updated Dockerfiles tailored for different needs. Building a Docker image is akin to setting the foundation for your project's environment:",
        "segment_20": "Autogen Basic: Ideal for general use, this setup includes common Python libraries and essential dependencies. Perfect for those just starting with AutoGen.",
        "segment_21": "docker build -f .devcontainer/Dockerfile -t autogen_base_img https://github.com/microsoft/autogen.git",
        "segment_24": "Autogen Advanced: Advanced users or those requiring all the things that AutoGen has to offer autogen_full_img",
        "segment_25": "docker build -f .devcontainer/full/Dockerfile -t autogen_full_img https://github.com/microsoft/autogen.git",
        "segment_28": "Step 3: Run AutoGen Applications from Docker Image\u200b",
        "segment_29": "Here's how you can run an application built with AutoGen, using the Docker image:",
        "segment_32": "Mount Your Directory: Use the Docker -v flag to mount your local application directory to the Docker container. This allows you to develop on your local machine while running the code in a consistent Docker environment. For example:",
        "segment_33": "docker run -it -v $(pwd)/myapp:/home/autogen/autogen/myapp autogen_base_img:latest python /home/autogen/autogen/myapp/main.py",
        "segment_34": "Here, $(pwd)/myapp is your local directory, and /home/autogen/autogen/myapp is the path in the Docker container where your code will be located.",
        "segment_37": "Mount your code: Now suppose you have your application built with AutoGen in a main script named twoagent.py (example) in a folder named myapp. With the command line below, you can mount your folder and run the application in Docker.",
        "segment_38": "# Mount the local folder `myapp` into docker image and run the script named \"twoagent.py\" in the docker.docker run -it -v `pwd`/myapp:/myapp autogen_img:latest python /myapp/main_twoagent.py",
        "segment_41": "Port Mapping: If your application requires a specific port, use the -p flag to map the container's port to your host. For instance, if your app runs on port 3000 inside Docker and you want it accessible on port 8080 on your host machine:",
        "segment_42": "docker run -it -p 8080:3000 -v $(pwd)/myapp:/myapp autogen_base_img:latest python /myapp",
        "segment_43": "In this command, -p 8080:3000 maps port 3000 from the container to port 8080 on your local machine.",
        "segment_46": "Examples of Running Different Applications: Here is the basic format of the docker run command.",
        "segment_49": "docker run -it -p {WorkstationPortNum}:{DockerPortNum} -v {WorkStation_Dir}:{Docker_DIR} {name_of_the_image} {bash/python} {Docker_path_to_script_to_execute}",
        "segment_52": "Simple Script: Run a Python script located in your local myapp directory.",
        "segment_53": "docker run -it -v `pwd`/myapp:/myapp autogen_base_img:latest python /myapp/my_script.py",
        "segment_56": "Web Application: If your application includes a web server running on port 5000.",
        "segment_57": "docker run -it -p 8080:5000 -v $(pwd)/myapp:/myapp autogen_base_img:latest",
        "segment_60": "Data Processing: For tasks that involve processing data stored in a local directory.",
        "segment_61": "docker run -it -v $(pwd)/data:/data autogen_base_img:latest python /myapp/process_data.py",
        "segment_64": "Additional Resources\u200b",
        "segment_66": "Details on all the Dockerfile options can be found in the Dockerfile README.",
        "segment_67": "For more information on Docker usage and best practices, refer to the official Docker documentation.",
        "segment_68": "Details on how to use the Dockerfile dev version can be found on the Contributing",
        "segment_69": "Edit this pagePreviousInstallationNextOptional DependenciesStep 1: Install DockerStep 2: Build a Docker ImageStep 3: Run AutoGen Applications from Docker ImageAdditional ResourcesCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKReferenceautogenautomldefaultonlinemltunetune.schedulertune.searchertune.sparkanalysissamplespacetrialtrial_runnertuneutilsOn this pagetune.analysisExperimentAnalysis Objects\u200bclass ExperimentAnalysis()CopyAnalyze results from a Tune experiment.best_trial\u200b@propertydef best_trial() -> TrialCopyGet the best trial of the experiment",
        "segment_2": "The best trial is determined by comparing the last trial results",
        "segment_3": "using the metric and mode parameters passed to tune.run().",
        "segment_4": "If you didn't pass these parameters, use",
        "segment_5": "get_best_trial(metric, mode, scope) instead.best_config\u200b@propertydef best_config() -> DictCopyGet the config of the best trial of the experiment",
        "segment_6": "The best trial is determined by comparing the last trial results",
        "segment_7": "using the metric and mode parameters passed to tune.run().",
        "segment_8": "If you didn't pass these parameters, use",
        "segment_9": "get_best_config(metric, mode, scope) instead.results\u200b@propertydef results() -> Dict[str, Dict]CopyGet the last result of all the trials of the experimentget_best_trial\u200bdef get_best_trial(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = \"last\", filter_nan_and_inf: bool = True) -> Optional[Trial]CopyRetrieve the best trial object.",
        "segment_10": "Compares all trials' scores on metric.",
        "segment_11": "If metric is not specified, self.default_metric will be used.",
        "segment_12": "If mode is not specified, self.default_mode will be used.",
        "segment_13": "These values are usually initialized by passing the metric and",
        "segment_14": "mode parameters to tune.run().Arguments:metric str - Key for trial info to order on. Defaults to",
        "segment_15": "self.default_metric.mode str - One of [min, max]. Defaults to self.default_mode.scope str - One of [all, last, avg, last-5-avg, last-10-avg].",
        "segment_16": "If scope=last, only look at each trial's final step for",
        "segment_17": "metric, and compare across trials based on mode=[min,max].",
        "segment_18": "If scope=avg, consider the simple average over all steps",
        "segment_19": "for metric and compare across trials based on",
        "segment_20": "mode=[min,max]. If scope=last-5-avg or scope=last-10-avg,",
        "segment_21": "consider the simple average over the last 5 or 10 steps for",
        "segment_22": "metric and compare across trials based on mode=[min,max].",
        "segment_23": "If scope=all, find each trial's min/max score for metric",
        "segment_24": "based on mode, and compare trials based on mode=[min,max].filter_nan_and_inf bool - If True (default), NaN or infinite",
        "segment_25": "values are disregarded and these trials are never selected as",
        "segment_26": "the best trial.get_best_config\u200bdef get_best_config(metric: Optional[str] = None, mode: Optional[str] = None, scope: str = \"last\") -> Optional[Dict]CopyRetrieve the best config corresponding to the trial.",
        "segment_27": "Compares all trials' scores on metric.",
        "segment_28": "If metric is not specified, self.default_metric will be used.",
        "segment_29": "If mode is not specified, self.default_mode will be used.",
        "segment_30": "These values are usually initialized by passing the metric and",
        "segment_31": "mode parameters to tune.run().Arguments:metric str - Key for trial info to order on. Defaults to",
        "segment_32": "self.default_metric.mode str - One of [min, max]. Defaults to self.default_mode.scope str - One of [all, last, avg, last-5-avg, last-10-avg].",
        "segment_33": "If scope=last, only look at each trial's final step for",
        "segment_34": "metric, and compare across trials based on mode=[min,max].",
        "segment_35": "If scope=avg, consider the simple average over all steps",
        "segment_36": "for metric and compare across trials based on",
        "segment_37": "mode=[min,max]. If scope=last-5-avg or scope=last-10-avg,",
        "segment_38": "consider the simple average over the last 5 or 10 steps for",
        "segment_39": "metric and compare across trials based on mode=[min,max].",
        "segment_40": "If scope=all, find each trial's min/max score for metric",
        "segment_41": "based on mode, and compare trials based on mode=[min,max].best_result\u200b@propertydef best_result() -> DictCopyGet the last result of the best trial of the experiment",
        "segment_42": "The best trial is determined by comparing the last trial results",
        "segment_43": "using the metric and mode parameters passed to tune.run().",
        "segment_44": "If you didn't pass these parameters, use",
        "segment_45": "get_best_trial(metric, mode, scope).last_result instead.Edit this pagePrevious\u00ab utilsNextsample \u00bbExperimentAnalysis ObjectsCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Cache()",
        "segment_2": "A wrapper class for managing cache configuration and instances.",
        "segment_3": "This class provides a unified interface for creating and interacting with",
        "segment_4": "different types of cache (e.g., Redis, Disk). It abstracts the underlying",
        "segment_5": "cache implementation details, providing methods for cache operations.",
        "segment_6": "Attributes:",
        "segment_8": "config Dict[str, Any] - A dictionary containing cache configuration.",
        "segment_9": "cache - The cache instance created based on the provided configuration.",
        "segment_11": "Methods:",
        "segment_12": "redis(cache_seed=42, redis_url=\"redis://localhost:6379/0\"): Static method to create a Redis cache instance.",
        "segment_13": "disk(cache_seed=42, cache_path_root=\".cache\"): Static method to create a Disk cache instance.",
        "segment_14": "init(self, config): Initializes the Cache with the given configuration.",
        "segment_16": "__enter__(self) - Context management entry, returning the cache instance.",
        "segment_17": "exit(self, exc_type, exc_value, traceback): Context management exit.",
        "segment_18": "get(self, key, default=None): Retrieves an item from the cache.",
        "segment_19": "set(self, key, value): Sets an item in the cache.",
        "segment_20": "close(self) - Closes the cache.",
        "segment_22": "redis\u200b",
        "segment_23": "@staticmethoddef redis(cache_seed=42, redis_url=\"redis://localhost:6379/0\")",
        "segment_24": "Create a Redis cache instance.",
        "segment_25": "Arguments:",
        "segment_27": "cache_seed int, optional - A seed for the cache. Defaults to 42.",
        "segment_28": "redis_url str, optional - The URL for the Redis server. Defaults to \"redis://localhost:6379/0\".",
        "segment_30": "Returns:",
        "segment_32": "Cache - A Cache instance configured for Redis.",
        "segment_34": "disk\u200b",
        "segment_35": "@staticmethoddef disk(cache_seed=42, cache_path_root=\".cache\")",
        "segment_36": "Create a Disk cache instance.",
        "segment_37": "Arguments:",
        "segment_39": "cache_seed int, optional - A seed for the cache. Defaults to 42.",
        "segment_40": "cache_path_root str, optional - The root path for the disk cache. Defaults to \".cache\".",
        "segment_42": "Returns:",
        "segment_44": "Cache - A Cache instance configured for Disk caching.",
        "segment_46": "__init__\u200b",
        "segment_47": "def __init__(config: Dict[str, Any])",
        "segment_48": "Initialize the Cache with the given configuration.",
        "segment_49": "Validates the configuration keys and creates the cache instance.",
        "segment_50": "Arguments:",
        "segment_52": "config Dict[str, Any] - A dictionary containing the cache configuration.",
        "segment_54": "Raises:",
        "segment_56": "ValueError - If an invalid configuration key is provided.",
        "segment_58": "__enter__\u200b",
        "segment_59": "def __enter__()",
        "segment_60": "Enter the runtime context related to the cache object.",
        "segment_61": "Returns:",
        "segment_62": "The cache instance for use within a context block.",
        "segment_63": "__exit__\u200b",
        "segment_64": "def __exit__(exc_type, exc_value, traceback)",
        "segment_65": "Exit the runtime context related to the cache object.",
        "segment_66": "Cleans up the cache instance and handles any exceptions that occurred",
        "segment_67": "within the context.",
        "segment_68": "Arguments:",
        "segment_70": "exc_type - The exception type if an exception was raised in the context.",
        "segment_71": "exc_value - The exception value if an exception was raised in the context.",
        "segment_72": "traceback - The traceback if an exception was raised in the context.",
        "segment_74": "get\u200b",
        "segment_75": "def get(key, default=None)",
        "segment_76": "Retrieve an item from the cache.",
        "segment_77": "Arguments:",
        "segment_79": "key str - The key identifying the item in the cache.",
        "segment_80": "default optional - The default value to return if the key is not found.",
        "segment_81": "Defaults to None.",
        "segment_83": "Returns:",
        "segment_84": "The value associated with the key if found, else the default value.",
        "segment_85": "set\u200b",
        "segment_86": "def set(key, value)",
        "segment_87": "Set an item in the cache.",
        "segment_88": "Arguments:",
        "segment_90": "key str - The key under which the item is to be stored.",
        "segment_91": "value - The value to be stored in the cache.",
        "segment_93": "close\u200b",
        "segment_94": "def close()",
        "segment_95": "Close the cache.",
        "segment_96": "Perform any necessary cleanup, such as closing connections or releasing resources.Edit this pagePreviousabstract_cache_baseNextcache_factoryCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class MathUserProxyAgent(UserProxyAgent)",
        "segment_2": "(Experimental) A MathChat agent that can handle math problems.",
        "segment_3": "MAX_CONSECUTIVE_AUTO_REPLY\u200b",
        "segment_4": "maximum number of consecutive auto replies (subject to future change)",
        "segment_5": "__init__\u200b",
        "segment_6": "def __init__(name: Optional[str] = \"MathChatAgent\", is_termination_msg: Optional[Callable[ [Dict], bool]] = _is_termination_msg_mathchat, human_input_mode: Optional[str] = \"NEVER\", default_auto_reply: Optional[Union[str, Dict, None]] = DEFAULT_REPLY, max_invalid_q_per_step=3, **kwargs)",
        "segment_7": "Arguments:",
        "segment_9": "name str - name of the agent",
        "segment_10": "is_termination_msg function - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.",
        "segment_11": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_12": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_13": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_14": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_15": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_16": "or when is_termination_msg is True and there is no human input.",
        "segment_17": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_18": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_19": "(3) (Default) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_20": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_21": "default_auto_reply str or dict or None - the default auto reply message when no code execution or llm based reply is generated.",
        "segment_22": "max_invalid_q_per_step int - (ADDED) the maximum number of invalid queries per step.",
        "segment_23": "**kwargs dict - other kwargs in UserProxyAgent.",
        "segment_25": "generate_init_message\u200b",
        "segment_26": "def generate_init_message(problem, prompt_type=\"default\", customized_prompt=None)",
        "segment_27": "Generate a prompt for the assistant agent with the given problem and prompt.",
        "segment_28": "Arguments:",
        "segment_30": "problem str - the problem to be solved.",
        "segment_31": "prompt_type str - the type of the prompt. Possible values are \"default\", \"python\", \"wolfram\".",
        "segment_32": "(1) \"default\": the prompt that allows the agent to choose between 3 ways to solve a problem:",
        "segment_34": "write a python program to solve it directly.",
        "segment_35": "solve it directly without python.",
        "segment_36": "solve it step by step with python.",
        "segment_37": "(2) \"python\":",
        "segment_38": "a simplified prompt from the third way of the \"default\" prompt, that asks the assistant",
        "segment_39": "to solve the problem step by step with python.",
        "segment_40": "(3) \"two_tools\":",
        "segment_41": "a simplified prompt similar to the \"python\" prompt, but allows the model to choose between",
        "segment_42": "Python and Wolfram Alpha to solve the problem.",
        "segment_45": "customized_prompt str - a customized prompt to be used. If it is not None, the prompt_type will be ignored.",
        "segment_47": "Returns:",
        "segment_49": "str - the generated prompt ready to be sent to the assistant agent.",
        "segment_51": "execute_one_python_code\u200b",
        "segment_52": "def execute_one_python_code(pycode)",
        "segment_53": "Execute python code blocks.",
        "segment_54": "Previous python code will be saved and executed together with the new code.",
        "segment_55": "the \"print\" function will also be added to the last line of the code if needed",
        "segment_56": "execute_one_wolfram_query\u200b",
        "segment_57": "def execute_one_wolfram_query(query: str)",
        "segment_58": "Run one wolfram query and return the output.",
        "segment_59": "Arguments:",
        "segment_61": "query - string of the query.",
        "segment_63": "Returns:",
        "segment_65": "output - string with the output of the query.",
        "segment_66": "is_success - boolean indicating whether the query was successful.",
        "segment_68": "get_from_dict_or_env\u200b",
        "segment_69": "def get_from_dict_or_env(data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None) -> str",
        "segment_70": "Get a value from a dictionary or an environment variable.",
        "segment_71": "WolframAlphaAPIWrapper Objects\u200b",
        "segment_72": "class WolframAlphaAPIWrapper(BaseModel)",
        "segment_73": "Wrapper for Wolfram Alpha.",
        "segment_74": "Docs for using:",
        "segment_76": "Go to wolfram alpha and sign up for a developer account",
        "segment_77": "Create an app and get your APP ID",
        "segment_78": "Save your APP ID into WOLFRAM_ALPHA_APPID env variable",
        "segment_79": "pip install wolframalpha",
        "segment_81": "wolfram_client\u200b",
        "segment_82": ":meta private:",
        "segment_83": "Config Objects\u200b",
        "segment_84": "class Config()",
        "segment_85": "Configuration for this pydantic object.",
        "segment_86": "validate_environment\u200b",
        "segment_87": "@root_validator(skip_on_failure=True)def validate_environment(cls, values: Dict) -> Dict",
        "segment_88": "Validate that api key and python package exists in environment.",
        "segment_89": "run\u200b",
        "segment_90": "def run(query: str) -> Tuple[str, bool]",
        "segment_91": "Run query through WolframAlpha and parse result.Edit this pagePreviousllava_agentNextmultimodal_conversable_agentMathUserProxyAgent ObjectsWolframAlphaAPIWrapper ObjectsConfig ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_0": "t(null!==e?e:\"light\")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith(\"docusaurus-data-\")){var a=t.replace(\"docusaurus-data-\",\"data-\");document.documentElement.setAttribute(a,e)}}catch(t){}}()Skip to main contentAutoGenDocsSDKBlogFAQExamplesResourcesEcosystemGalleryGitHubctrlKRecent postsAutoGen with Custom Models: Empowering Users to Use Their Own Inference MechanismAutoGenBench -- A Tool for Measuring and Evaluating AutoGen AgentsCode execution is now by default inside docker containerAll About Agent DescriptionsAgentOptimizer - An Agentic Way to Train Your LLM AgentAutoGen Studio: Interactively Explore Multi-Agent WorkflowsAgent AutoBuild - Automatically Building Multi-agent SystemsHow to Assess Utility of LLM-powered Applications?AutoGen Meets GPTsEcoAssistant - Using LLM Assistants More Accurately and AffordablyMultimodal with GPT-4V and LLaVAAutoGen's Teachable AgentsRetrieval-Augmented Generation (RAG) Applications with AutoGenUse AutoGen for Local LLMsMathChat - An Conversational Framework to Solve Math ProblemsAchieve More, Pay Less - Use GPT-4 SmartlyDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHMultimodal with GPT-4V and LLaVANovember 6, 2023 \u00b7 3 min readBeibin LiSenior Research Engineer at Microsoft",
        "segment_1": "In Brief:",
        "segment_3": "Introducing the Multimodal Conversable Agent and the LLaVA Agent to enhance LMM functionalities.",
        "segment_4": "Users can input text and images simultaneously using the tag to specify image loading.",
        "segment_5": "Demonstrated through the GPT-4V notebook.",
        "segment_6": "Demonstrated through the LLaVA notebook.",
        "segment_8": "Introduction\u200b",
        "segment_9": "Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.",
        "segment_10": "This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.",
        "segment_11": "We support the gpt-4-vision-preview model from OpenAI and LLaVA model from Microsoft now.",
        "segment_12": "Here, we emphasize the Multimodal Conversable Agent and the LLaVA Agent due to their growing popularity.",
        "segment_13": "GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.",
        "segment_14": "Installation\u200b",
        "segment_15": "Incorporate the lmm feature during AutoGen installation:",
        "segment_16": "pip install \"pyautogen[lmm]\"",
        "segment_17": "Subsequently, import the Multimodal Conversable Agent or LLaVA Agent from AutoGen:",
        "segment_18": "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent # for GPT-4Vfrom autogen.agentchat.contrib.llava_agent import LLaVAAgent # for LLaVA",
        "segment_19": "Usage\u200b",
        "segment_20": "A simple syntax has been defined to incorporate both messages and images within a single string.",
        "segment_21": "Example of an in-context learning prompt:",
        "segment_22": "prompt = \"\"\"You are now an image classifier for facial expressions. Here aresome examples. depicts a happy expression. represents a sad expression. portrays a neutral expression.Now, identify the facial expression of this individual: \"\"\"agent = MultimodalConversableAgent()user = UserProxyAgent()user.initiate_chat(agent, message=prompt)",
        "segment_23": "The MultimodalConversableAgent interprets the input prompt, extracting images from local or internet sources.",
        "segment_24": "Advanced Usage\u200b",
        "segment_25": "Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.",
        "segment_26": "For example, the FigureCreator in our GPT-4V notebook and LLaVA notebook integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).",
        "segment_27": "The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.",
        "segment_28": "With human_input_mode=ALWAYS, you can also contribute suggestions for better visualizations.",
        "segment_29": "Reference\u200b",
        "segment_31": "GPT-4V System Card",
        "segment_32": "LLaVA GitHub",
        "segment_34": "Future Enhancements\u200b",
        "segment_35": "For further inquiries or suggestions, please open an issue in the AutoGen repository or contact me directly at beibin.li@microsoft.com.",
        "segment_36": "AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.Tags:LMMmultimodalAutoGen's Teachable AgentsOctober 26, 2023 \u00b7 17 min readRicky LoyndSenior Research Engineer at Microsoft",
        "segment_37": "TL;DR:",
        "segment_39": "We introduce Teachable Agents so that users can teach their LLM-based assistants new facts, preferences, and skills.",
        "segment_40": "We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.",
        "segment_42": "Introduction\u200b",
        "segment_43": "Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.",
        "segment_44": "Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.",
        "segment_45": "Any instantiated agent that inherits from ConversableAgent can be made teachable by instantiating a Teachability object and calling its add_to_agent(agent) method.",
        "segment_46": "In order to make effective decisions about memo storage and retrieval, the Teachability object calls an instance of TextAnalyzerAgent (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.",
        "segment_47": "Run It Yourself\u200b",
        "segment_48": "AutoGen contains four code examples that use Teachability.",
        "segment_51": "Run chat_with_teachable_agent.py to converse with a teachable agent.",
        "segment_54": "Run test_teachable_agent.py for quick unit testing of a teachable agent.",
        "segment_57": "Use the Jupyter notebook agentchat_teachability.ipynb to step through examples discussed below.",
        "segment_60": "Use the Jupyter notebook agentchat_teachable_oai_assistants.ipynb to make arbitrary OpenAI Assistants teachable through GPTAssistantAgent.",
        "segment_63": "Basic Usage of Teachability\u200b",
        "segment_65": "Install dependencies",
        "segment_67": "Please install pyautogen with the [teachable] option before using Teachability.",
        "segment_68": "pip install \"pyautogen[teachable]\"",
        "segment_70": "Import agents",
        "segment_72": "from autogen import UserProxyAgent, config_list_from_jsonfrom autogen.agentchat.contrib.capabilities.teachability import Teachabilityfrom autogen import ConversableAgent # As an example",
        "segment_74": "Create llm_config",
        "segment_76": "# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_samplefilter_dict = {\"model\": [\"gpt-4\"]} # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", filter_dict=filter_dict)llm_config={\"config_list\": config_list, \"timeout\": 120}",
        "segment_78": "Create the agents",
        "segment_80": "# Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.teachable_agent = ConversableAgent( name=\"teachable_agent\", # The name can be anything. llm_config=llm_config)# Instantiate a Teachability object. Its parameters are all optional.teachability = Teachability( reset_db=False, # Use True to force-reset the memo DB, and False to use an existing DB. path_to_db_dir=\"./tmp/interactive/teachability_db\" # Can be any path, but teachable agents in a group chat require unique paths.)# Now add teachability to the agent.teachability.add_to_agent(teachable_agent)# For this test, create a user proxy agent as usual.user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")",
        "segment_82": "Chat with the teachable agent",
        "segment_84": "# This function will return once the user types 'exit'.teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")",
        "segment_85": "Example 1 - Learning user info\u200b",
        "segment_86": "A user can teach the agent facts about themselves.",
        "segment_87": "(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)",
        "segment_88": "Loading previous memory (if any) from disk.teachable_agent (to user):Greetings, I'm a teachable user assistant! What's on your mind today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Rickyuser (to teachable_agent):My name is Ricky-------------------------------------------------------------------------------->>>>>>>> USING AUTO REPLY...teachable_agent (to user):Hello Ricky! How can I assist you today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit",
        "segment_89": "In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)",
        "segment_90": "--------------------------------------------------------------------------------user (to teachable_agent):what's my name?--------------------------------------------------------------------------------teachable_agent (to user):Hi Ricky! How can I help you today?--------------------------------------------------------------------------------",
        "segment_91": "Example 2 - Learning new facts\u200b",
        "segment_92": "A user can teach the agent more complex, related facts.",
        "segment_93": "--------------------------------------------------------------------------------user (to teachable_agent):What is the Vicuna model?--------------------------------------------------------------------------------teachable_agent (to user):I'm sorry, but I don't have information about the \"Vicuna model\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?--------------------------------------------------------------------------------user (to teachable_agent):Vicuna is a 13B-parameter language model released by Meta.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.--------------------------------------------------------------------------------user (to teachable_agent):What is the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.--------------------------------------------------------------------------------user (to teachable_agent):Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.",
        "segment_94": "Then in a later chat the teachable agent can answer questions about the facts it has been taught.",
        "segment_95": "(Remember to first close the previous chat by typing 'exit'.)",
        "segment_96": "--------------------------------------------------------------------------------user (to teachable_agent):How does the Vicuna model compare to the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.--------------------------------------------------------------------------------",
        "segment_97": "Example 3 - Learning user preferences\u200b",
        "segment_98": "A user can teach the agent how they prefer to have things done.",
        "segment_99": "Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.",
        "segment_100": "Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.",
        "segment_101": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.--------------------------------------------------------------------------------Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points: the title, the innovation, and the key empirical results.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.--------------------------------------------------------------------------------",
        "segment_102": "Then in later chats the teacher doesn't need to reiterate their detailed preferences.",
        "segment_103": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.Sparks of Artificial General Intelligence: Early experiments with GPT-4S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi ZhangArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.--------------------------------------------------------------------------------teachable_agent (to user):- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.--------------------------------------------------------------------------------",
        "segment_104": "Example 4 - Learning new skills\u200b",
        "segment_105": "Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.",
        "segment_106": "The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.",
        "segment_107": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------Step 1: Identify the current equation and the target value on the right-hand side.Current equation: 9 * 4 + 6 * 6 = 72Target value: 99Step 2: Determine what numbers can be changed on the left-hand side.Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.9 -> If changed, it would create a larger product.4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.Step 4: Write down the solution.The solution is: 9 * 4 + 6 * 7--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".Here's some advice that may help:1. Let E denote the original number on the right.2. Let F denote the final number on the right.3. Calculate the difference between the two, G = F - E.4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.5. Calculate J = G / H. This is the number of times that H divides into G.6. Verify that J is an integer, and that H * J = G.7. Find the number on the left which is multiplied by H, and call it K.8. Change K to K + J.9. Recompute the value on the left, and verify that it equals F.Finally, write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Step 1: Identify the original number (E) and the final number (F) on the right-hand side.E = 72F = 99Step 2: Calculate the difference between the two, G = F - E.G = 99 - 72 = 27Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.9 (divides evenly into 27)4 (does not)6 (first occurrence, does not)6 (second occurrence, does not)Step 4: Calculate J = G / H. This is the number of times that H divides into G.H = 9 (since 9 divides evenly into 27)J = 27 / 9 = 3Step 5: Verify that J is an integer, and that H * J = G.H * J = 9 * 3 = 27J = 3 (which is an integer)Step 6: Find the number on the left which is multiplied by H, and call it K.Number on the leftside of the equation that is multiplied by 9: 4Therefore, K = 4Step 7: Change K to K + J.K + J = 4 + 3 = 7Step 8: Recompute the value on the left, and verify that it equals F.9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)Step 9: Write down the solution.The solution is: 9 * 7 + 6 * 6--------------------------------------------------------------------------------",
        "segment_108": "In a later chat the user doesn't need to repeat the detailed advice.",
        "segment_109": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:8 * 3 + 7 * 9 = 87Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:1. Let E denote the original number on the right: E = 87.2. Let F denote the final number on the right: F = 59.3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).5. Calculate J = G / H: J = -28 / 7 = -4.6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.8. Change K to K + J: K_new = 9 + (-4) = 5.9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.The solution is: 8 * 3 + 7 * 5--------------------------------------------------------------------------------",
        "segment_110": "Planned improvements\u200b",
        "segment_112": "Understanding user instructions distributed over multiple turns.",
        "segment_113": "Learning from the agent's own experience, to reduce dependence on explicit user teachings.",
        "segment_114": "Learning skills built on top of previously learned skills.",
        "segment_116": "Conclusion\u200b",
        "segment_117": "Teachability is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our Discord channel. We look forward to seeing how you and the rest of the community can use and improve teachable agents in AutoGen!Tags:LLMteachRetrieval-Augmented Generation (RAG) Applications with AutoGenOctober 18, 2023 \u00b7 10 min readLi JiangSenior Software Engineer at Microsoft",
        "segment_118": "TL;DR:",
        "segment_120": "We introduce RetrieveUserProxyAgent and RetrieveAssistantAgent, RAG agents of AutoGen that",
        "segment_121": "allows retrieval-augmented generation, and its basic usage.",
        "segment_122": "We showcase customizations of RAG agents, such as customizing the embedding function, the text",
        "segment_123": "split function and vector database.",
        "segment_124": "We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat",
        "segment_125": "application with Gradio.",
        "segment_127": "Introduction\u200b",
        "segment_128": "Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic",
        "segment_129": "limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of",
        "segment_130": "AutoGen that allows retrieval-augmented generation. The system consists of two agents: a",
        "segment_131": "Retrieval-augmented User Proxy agent, called RetrieveUserProxyAgent, and a Retrieval-augmented Assistant",
        "segment_132": "agent, called RetrieveAssistantAgent, both of which are extended from built-in agents from AutoGen.",
        "segment_133": "The overall architecture of the RAG agents is shown in the figure above.",
        "segment_134": "To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented",
        "segment_135": "User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy",
        "segment_136": "necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented",
        "segment_137": "User Proxy can download the documents, segment them into chunks of a specific size, compute",
        "segment_138": "embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively",
        "segment_139": "engage in code generation or question-answering adhering to the procedures outlined below:",
        "segment_141": "The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,",
        "segment_142": "and sends them along with the question to the Retrieval-Augmented Assistant.",
        "segment_143": "The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based",
        "segment_144": "on the question and context provided. If the LLM is unable to produce a satisfactory response, it",
        "segment_145": "is instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.",
        "segment_146": "If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and",
        "segment_147": "sends the output as feedback. If there are no code blocks or instructions to update the context, it",
        "segment_148": "terminates the conversation. Otherwise, it updates the context and forwards the question along",
        "segment_149": "with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation",
        "segment_150": "is enabled, individuals can proactively send any feedback, including Update Context\u201d, to the",
        "segment_151": "Retrieval-Augmented Assistant.",
        "segment_152": "If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar",
        "segment_153": "chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it",
        "segment_154": "generates new code or text based on the feedback and chat history. If the LLM fails to generate",
        "segment_155": "an answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.",
        "segment_156": "The conversation terminates if no more documents are available for the context.",
        "segment_158": "Basic Usage of RAG Agents\u200b",
        "segment_160": "Install dependencies",
        "segment_162": "Please install pyautogen with the [retrievechat] option before using RAG agents.",
        "segment_163": "pip install \"pyautogen[retrievechat]\"",
        "segment_164": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_165": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_166": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_167": "If you install unstructured",
        "segment_168": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_169": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_170": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_172": "Import Agents",
        "segment_174": "import autogenfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgentfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent",
        "segment_176": "Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"",
        "segment_178": "assistant = RetrieveAssistantAgent( name=\"assistant\", system_message=\"You are a helpful assistant.\", llm_config=llm_config,)ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", },)",
        "segment_180": "Initialize Chat and ask a question",
        "segment_182": "assistant.reset()ragproxyagent.initiate_chat(assistant, problem=\"What is autogen?\")",
        "segment_183": "Output is like:",
        "segment_184": "--------------------------------------------------------------------------------assistant (to ragproxyagent):AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.--------------------------------------------------------------------------------",
        "segment_186": "Create a UserProxyAgent and ask the same question",
        "segment_188": "assistant.reset()userproxyagent = autogen.UserProxyAgent(name=\"userproxyagent\")userproxyagent.initiate_chat(assistant, message=\"What is autogen?\")",
        "segment_189": "Output is like:",
        "segment_190": "--------------------------------------------------------------------------------assistant (to userproxyagent):In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.--------------------------------------------------------------------------------",
        "segment_191": "You can see that the output of UserProxyAgent is not related to our autogen since the latest info of",
        "segment_192": "autogen is not in ChatGPT's training data. The output of RetrieveUserProxyAgent is correct as it can",
        "segment_193": "perform retrieval-augmented generation based on the given documentation file.",
        "segment_194": "Customizing RAG Agents\u200b",
        "segment_195": "RetrieveUserProxyAgent is customizable with retrieve_config. There are several parameters to configure",
        "segment_196": "based on different use cases. In this section, we'll show how to customize embedding function, text split",
        "segment_197": "function and vector database.",
        "segment_198": "Customizing Embedding Function\u200b",
        "segment_199": "By default, Sentence Transformers and its pretrained models will be used to",
        "segment_200": "compute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.",
        "segment_202": "OpenAI",
        "segment_204": "from chromadb.utils import embedding_functionsopenai_ef = embedding_functions.OpenAIEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"text-embedding-ada-002\" )ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"embedding_function\": openai_ef, },)",
        "segment_206": "HuggingFace",
        "segment_208": "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction( api_key=\"YOUR_API_KEY\", model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
        "segment_209": "More examples can be found here.",
        "segment_210": "Customizing Text Split Function\u200b",
        "segment_211": "Before we can store the documents into a vector database, we need to split the texts into chunks. Although",
        "segment_212": "we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.",
        "segment_213": "There are also some existing text split tools which are good to reuse.",
        "segment_214": "For example, you can use all the text splitters in langchain.",
        "segment_215": "from langchain.text_splitter import RecursiveCharacterTextSplitterrecur_spliter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\r\", \"\\t\"])ragproxyagent = RetrieveUserProxyAgent( name=\"ragproxyagent\", retrieve_config={ \"task\": \"qa\", \"docs_path\": \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\", \"custom_text_split_function\": recur_spliter.split_text, },)",
        "segment_216": "Customizing Vector Database\u200b",
        "segment_217": "We are using chromadb as the default vector database, you can also replace it with any other vector database",
        "segment_218": "by simply overriding the function retrieve_docs of RetrieveUserProxyAgent.",
        "segment_219": "For example, you can use Qdrant as below:",
        "segment_220": "# Creating qdrant clientfrom qdrant_client import QdrantClientclient = QdrantClient(url=\"***\", api_key=\"***\")# Wrapping RetrieveUserProxyAgentfrom litellm import embedding as test_embeddingfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgentfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchTextclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent): def query_vector_db( self, query_texts: List[str], n_results: int = 10, search_string: str = \"\", **kwargs, ) -> Dict[str, Union[List[str], List[List[str]]]]: # define your own query function here embed_response = test_embedding('text-embedding-ada-002', input=query_texts) all_embeddings: List[List[float]] = [] for item in embed_response['data']: all_embeddings.append(item['embedding']) search_queries: List[SearchRequest] = [] for embedding in all_embeddings: search_queries.append( SearchRequest( vector=embedding, filter=Filter( must=[ FieldCondition( key=\"page_content\", match=MatchText( text=search_string, ) ) ] ), limit=n_results, with_payload=True, ) ) search_response = client.search_batch( collection_name=\"{your collection name}\", requests=search_queries, ) return { \"ids\": [[scored_point.id for scored_point in batch] for batch in search_response], \"documents\": [[scored_point.payload.get('page_content', '') for scored_point in batch] for batch in search_response], \"metadatas\": [[scored_point.payload.get('metadata', {}) for scored_point in batch] for batch in search_response] } def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs): results = self.query_vector_db( query_texts=[problem], n_results=n_results, search_string=search_string, **kwargs, ) self._results = results# Use QdrantRetrieveUserProxyAgentqdrantragagent = QdrantRetrieveUserProxyAgent( name=\"ragproxyagent\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=2, retrieve_config={ \"task\": \"qa\", },)qdrantragagent.retrieve_docs(\"What is Autogen?\", n_results=10, search_string=\"autogen\")",
        "segment_221": "Advanced Usage of RAG Agents\u200b",
        "segment_222": "Integrate with other agents in a group chat\u200b",
        "segment_223": "To use RetrieveUserProxyAgent in a group chat is almost the same as you use it in a two agents chat. The only thing is that",
        "segment_224": "you need to initialize the chat with RetrieveUserProxyAgent. The RetrieveAssistantAgent is not necessary in a group chat.",
        "segment_225": "However, you may want to initialize the chat with another agent in some cases. To leverage the best of RetrieveUserProxyAgent,",
        "segment_226": "you'll need to call it from a function.",
        "segment_227": "llm_config = { \"functions\": [ { \"name\": \"retrieve_content\", \"description\": \"retrieve content for code generation and question answering.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"message\": { \"type\": \"string\", \"description\": \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\", } }, \"required\": [\"message\"], }, }, ], \"config_list\": config_list, \"timeout\": 60, \"seed\": 42,}boss = autogen.UserProxyAgent( name=\"Boss\", is_termination_msg=termination_msg, human_input_mode=\"TERMINATE\", system_message=\"The boss who ask questions and give tasks.\",)boss_aid = RetrieveUserProxyAgent( name=\"Boss_Assistant\", is_termination_msg=termination_msg, system_message=\"Assistant who has extra content retrieval power for solving difficult problems.\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=3, retrieve_config={ \"task\": \"qa\", }, code_execution_config=False, # we don't want to execute code in this case.)coder = AssistantAgent( name=\"Senior_Python_Engineer\", is_termination_msg=termination_msg, system_message=\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)pm = autogen.AssistantAgent( name=\"Product_Manager\", is_termination_msg=termination_msg, system_message=\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)reviewer = autogen.AssistantAgent( name=\"Code_Reviewer\", is_termination_msg=termination_msg, system_message=\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\", llm_config=llm_config,)def retrieve_content(message, n_results=3): boss_aid.n_results = n_results # Set the number of results to be retrieved. # Check if we need to update the context. update_context_case1, update_context_case2 = boss_aid._check_update_context(message) if (update_context_case1 or update_context_case2) and boss_aid.update_context: boss_aid.problem = message if not hasattr(boss_aid, \"problem\") else boss_aid.problem _, ret_msg = boss_aid._generate_retrieve_user_reply(message) else: ret_msg = boss_aid.generate_init_message(message, n_results=n_results) return ret_msg if ret_msg else messagefor agent in [boss, coder, pm, reviewer]: # register functions for all agents. agent.register_function( function_map={ \"retrieve_content\": retrieve_content, } )groupchat = autogen.GroupChat( agents=[boss, coder, pm, reviewer], messages=[], max_round=12)manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)# Start chatting with the boss as this is the user proxy agent.boss.initiate_chat( manager, message=\"How to use spark for parallel training in FLAML? Give me sample code.\",)",
        "segment_228": "Build a Chat application with Gradio\u200b",
        "segment_229": "Now, let's wrap it up and make a Chat application with AutoGen and Gradio.",
        "segment_231": "# Initialize Agentsdef initialize_agents(config_list, docs_path=None): ... return assistant, ragproxyagent# Initialize Chatdef initiate_chat(config_list, problem, queue, n_results=3): ... assistant.reset() try: ragproxyagent.a_initiate_chat( assistant, problem=problem, silent=False, n_results=n_results ) messages = ragproxyagent.chat_messages messages = [messages[k] for k in messages.keys()][0] messages = [m[\"content\"] for m in messages if m[\"role\"] == \"user\"] print(\"messages: \", messages) except Exception as e: messages = [str(e)] queue.put(messages)# Wrap AutoGen part into a functiondef chatbot_reply(input_text): \"\"\"Chat with the agent through terminal.\"\"\" queue = mp.Queue() process = mp.Process( target=initiate_chat, args=(config_list, input_text, queue), ) process.start() try: messages = queue.get(timeout=TIMEOUT) except Exception as e: messages = [str(e) if len(str(e)) > 0 else \"Invalid Request to OpenAI, please check your API keys.\"] finally: try: process.terminate() except: pass return messages...# Set up UI with Gradiowith gr.Blocks() as demo: ... assistant, ragproxyagent = initialize_agents(config_list) chatbot = gr.Chatbot( [], elem_id=\"chatbot\", bubble_full_width=False, avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"autogen.png\"))), # height=600, ) txt_input = gr.Textbox( scale=4, show_label=False, placeholder=\"Enter text and press enter\", container=False, ) with gr.Row(): txt_model = gr.Dropdown( label=\"Model\", choices=[ \"gpt-4\", \"gpt-35-turbo\", \"gpt-3.5-turbo\", ], allow_custom_value=True, value=\"gpt-35-turbo\", container=True, ) txt_oai_key = gr.Textbox( label=\"OpenAI API Key\", placeholder=\"Enter key and press enter\", max_lines=1, show_label=True, value=os.environ.get(\"OPENAI_API_KEY\", \"\"), container=True, type=\"password\", ) ... clear = gr.ClearButton([txt_input, chatbot])...if __name__ == \"__main__\": demo.launch(share=True)",
        "segment_232": "The online app and the source code are hosted in HuggingFace. Feel free to give it a try!",
        "segment_233": "Read More\u200b",
        "segment_234": "You can check out more example notebooks for RAG use cases:",
        "segment_236": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_237": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_238": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_239": "Tags:LLMRAGUse AutoGen for Local LLMsJuly 14, 2023 \u00b7 3 min readJiale LiuUndergraduate student at Xidian UniversityTL;DR:",
        "segment_240": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_241": "Preparations\u200b",
        "segment_242": "Clone FastChat\u200b",
        "segment_243": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_244": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_245": "Download checkpoint\u200b",
        "segment_246": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_247": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_248": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_249": "Initiate server\u200b",
        "segment_250": "First, launch the controller",
        "segment_251": "python -m fastchat.serve.controller",
        "segment_252": "Then, launch the model worker(s)",
        "segment_253": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_254": "Finally, launch the RESTful API server",
        "segment_255": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_256": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_257": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_258": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_259": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_260": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_261": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_262": "interacting with multiple local LLMs\u200b",
        "segment_263": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_264": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_265": "The inference code would be:",
        "segment_266": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_267": "For Further Reading\u200b",
        "segment_269": "Documentation about autogen.",
        "segment_270": "Documentation about FastChat.",
        "segment_271": "Tags:LLMMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_272": "TL;DR:",
        "segment_274": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_275": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_276": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_278": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_279": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_280": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_281": "The MathChat Framework\u200b",
        "segment_282": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_283": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_286": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_289": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_291": "Writing a Python program to solve the problem directly.",
        "segment_292": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_293": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_297": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_300": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_301": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_302": "(The conversation is modified for readability.)",
        "segment_303": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_304": "Experiment Setup\u200b",
        "segment_305": "We evaluate the improvement brought by MathChat.",
        "segment_306": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_307": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_308": "We also evaluate the following methods for comparison:",
        "segment_311": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_314": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_317": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_320": "Experiment Results\u200b",
        "segment_321": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_323": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_324": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_325": "The code for experiments can be found at this repository.",
        "segment_326": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_327": "Future Directions\u200b",
        "segment_328": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_329": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_331": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_332": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_333": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_335": "For Further Reading\u200b",
        "segment_337": "Research paper of MathChat",
        "segment_338": "Documentation about autogen",
        "segment_340": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_341": "TL;DR:",
        "segment_343": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_345": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_346": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_347": "Observations\u200b",
        "segment_349": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_350": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_352": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_353": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_354": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_355": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_356": "What else can we do? We notice that:",
        "segment_357": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_358": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_359": "Solution\u200b",
        "segment_360": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_362": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_363": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_366": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_367": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_369": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_370": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_371": "GPT-4, n=1, temperature=0",
        "segment_372": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_373": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_375": "Experiment Results\u200b",
        "segment_376": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_377": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_378": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_380": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_382": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_384": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_385": "Solved by GPT-4, n=1, temperature=0:",
        "segment_387": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_389": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_391": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_393": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_395": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_396": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_398": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_399": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_401": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_402": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_403": "Discussion\u200b",
        "segment_404": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_405": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_407": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_408": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_410": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_411": "Different configurations have complementary effect.",
        "segment_415": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_416": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_417": "There are many directions of extensions in research and development:",
        "segment_419": "Generalize the way to provide feedback.",
        "segment_420": "Automate the process of optimizing the configurations.",
        "segment_421": "Build adaptive agents for different applications.",
        "segment_423": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_424": "For Further Reading\u200b",
        "segment_426": "Documentation about autogen and Research paper.",
        "segment_427": "Blog post about a related study for math.",
        "segment_428": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_429": "TL;DR:",
        "segment_431": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_432": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_433": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_435": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_436": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_437": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_438": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_439": "Experiment Setup\u200b",
        "segment_440": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_442": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_443": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_445": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_447": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_448": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_449": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_450": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_451": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_453": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_454": "Experiment Results\u200b",
        "segment_455": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_456": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_457": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_459": "However, the selected model changes on level 4 Algebra.",
        "segment_461": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_462": "On level 5 the result is similar.",
        "segment_464": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_465": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_466": "Analysis and Discussion\u200b",
        "segment_467": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_468": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_469": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_470": "For Further Reading\u200b",
        "segment_472": "Research paper about the tuning technique",
        "segment_473": "Documentation about inference tuning",
        "segment_475": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer EntriesCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class ModelClient(Protocol)",
        "segment_2": "A client class must implement the following methods:",
        "segment_4": "create must return a response object that implements the ModelClientResponseProtocol",
        "segment_5": "cost must return the cost of the response",
        "segment_6": "get_usage must return a dict with the following keys:",
        "segment_8": "prompt_tokens",
        "segment_9": "completion_tokens",
        "segment_10": "total_tokens",
        "segment_11": "cost",
        "segment_12": "model",
        "segment_16": "This class is used to create a client that can be used by OpenAIWrapper.",
        "segment_17": "The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed.",
        "segment_18": "The message_retrieval method must be implemented to return a list of str or a list of messages from the response.",
        "segment_19": "message_retrieval\u200b",
        "segment_20": "def message_retrieval( response: ModelClientResponseProtocol) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]",
        "segment_21": "Retrieve and return a list of strings or a list of Choice.Message from the response.",
        "segment_22": "NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,",
        "segment_23": "since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.",
        "segment_24": "get_usage\u200b",
        "segment_25": "@staticmethoddef get_usage(response: ModelClientResponseProtocol) -> Dict",
        "segment_26": "Return usage summary of the response using RESPONSE_USAGE_KEYS.",
        "segment_27": "OpenAIClient Objects\u200b",
        "segment_28": "class OpenAIClient()",
        "segment_29": "Follows the Client protocol and wraps the OpenAI client.",
        "segment_30": "message_retrieval\u200b",
        "segment_31": "def message_retrieval( response: Union[ChatCompletion, Completion]) -> Union[List[str], List[ChatCompletionMessage]]",
        "segment_32": "Retrieve the messages from the response.",
        "segment_33": "create\u200b",
        "segment_34": "def create(params: Dict[str, Any]) -> ChatCompletion",
        "segment_35": "Create a completion for a given config using openai's client.",
        "segment_36": "Arguments:",
        "segment_38": "client - The openai client.",
        "segment_39": "params - The params for the completion.",
        "segment_41": "Returns:",
        "segment_42": "The completion.",
        "segment_43": "cost\u200b",
        "segment_44": "def cost(response: Union[ChatCompletion, Completion]) -> float",
        "segment_45": "Calculate the cost of the response.",
        "segment_46": "OpenAIWrapper Objects\u200b",
        "segment_47": "class OpenAIWrapper()",
        "segment_48": "A wrapper class for openai client.",
        "segment_49": "__init__\u200b",
        "segment_50": "def __init__(*, config_list: Optional[List[Dict[str, Any]]] = None, **base_config: Any)",
        "segment_51": "Arguments:",
        "segment_53": "config_list - a list of config dicts to override the base_config.",
        "segment_54": "They can contain additional kwargs as allowed in the create method. E.g.,",
        "segment_56": "config_list=[ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-03-15-preview\", }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"api_type\": \"openai\", \"base_url\": \"https://api.openai.com/v1\", }, { \"model\": \"llama-7B\", \"base_url\": \"http://127.0.0.1:8080\", }]",
        "segment_58": "base_config - base config. It can contain both keyword arguments for openai client",
        "segment_59": "and additional kwargs.",
        "segment_61": "register_model_client\u200b",
        "segment_62": "def register_model_client(model_client_cls: ModelClient, **kwargs)",
        "segment_63": "Register a model client.",
        "segment_64": "Arguments:",
        "segment_66": "model_client_cls - A custom client class that follows the ModelClient interface",
        "segment_67": "**kwargs - The kwargs for the custom client class to be initialized with",
        "segment_69": "create\u200b",
        "segment_70": "def create(**config: Any) -> ModelClient.ModelClientResponseProtocol",
        "segment_71": "Make a completion for a given config using available clients.",
        "segment_72": "Besides the kwargs allowed in openai's [or other] client, we allow the following additional kwargs.",
        "segment_73": "The config in each client will be overridden by the config.",
        "segment_74": "Arguments:",
        "segment_76": "context (Dict | None): The context to instantiate the prompt or messages. Default to None.",
        "segment_77": "It needs to contain keys that are used by the prompt template or the filter function.",
        "segment_78": "E.g., prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}.",
        "segment_79": "The actual prompt will be:",
        "segment_80": "\"Complete the following sentence: Today I feel\".",
        "segment_81": "More examples can be found at templating.",
        "segment_82": "cache (Cache | None): A Cache object to use for response cache. Default to None.",
        "segment_83": "Note that the cache argument overrides the legacy cache_seed argument: if this argument is provided,",
        "segment_84": "then the cache_seed argument is ignored. If this argument is not provided or None,",
        "segment_85": "then the cache_seed argument is used.",
        "segment_86": "(Legacy) cache_seed (int | None) for using the DiskCache. Default to 41.",
        "segment_87": "An integer cache_seed is useful when implementing \"controlled randomness\" for the completion.",
        "segment_88": "None for no caching.",
        "segment_89": "Note - this is a legacy argument. It is only used when the cache argument is not provided.",
        "segment_91": "filter_func (Callable | None): A function that takes in the context and the response",
        "segment_92": "and returns a boolean to indicate whether the response is valid. E.g.,",
        "segment_96": "def yes_or_no_filter(context, response): return context.get(\"yes_or_no_choice\", False) is False or any( text in [\"Yes.\", \"No.\"] for text in client.extract_text_or_completion_object(response) )",
        "segment_98": "allow_format_str_template (bool | None): Whether to allow format string template in the config. Default to false.",
        "segment_99": "api_version (str | None): The api version. Default to None. E.g., \"2023-08-01-preview\".",
        "segment_101": "Raises:",
        "segment_103": "RuntimeError: If all declared custom model clients are not registered",
        "segment_104": "APIError: If any model client create call raises an APIError",
        "segment_106": "print_usage_summary\u200b",
        "segment_107": "def print_usage_summary( mode: Union[str, List[str]] = [\"actual\", \"total\"]) -> None",
        "segment_108": "Print the usage summary.",
        "segment_109": "clear_usage_summary\u200b",
        "segment_110": "def clear_usage_summary() -> None",
        "segment_111": "Clear the usage summary.",
        "segment_112": "extract_text_or_completion_object\u200b",
        "segment_113": "@classmethoddef extract_text_or_completion_object( cls, response: ModelClient.ModelClientResponseProtocol) -> Union[List[str], List[ModelClient.ModelClientResponseProtocol.Choice.Message]]",
        "segment_114": "Extract the text or ChatCompletion objects from a completion or chat response.",
        "segment_115": "Arguments:",
        "segment_117": "response ChatCompletion | Completion - The response from openai.",
        "segment_119": "Returns:",
        "segment_120": "A list of text, or a list of ChatCompletion objects if function_call/tool_calls are present.Edit this pagePreviousredis_cacheNextcompletionModelClient ObjectsOpenAIClient ObjectsOpenAIWrapper ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "LLM Caching\u200b",
        "segment_2": "To use LLM caching with Redis, you need to install the Python package with",
        "segment_3": "the option redis:",
        "segment_4": "pip install \"pyautogen[redis]\"",
        "segment_5": "See LLM Caching for details.",
        "segment_6": "Docker\u200b",
        "segment_7": "Even if you install AutoGen locally, we highly recommend using Docker for code execution.",
        "segment_8": "To use docker for code execution, you also need to install the python package docker:",
        "segment_9": "pip install docker",
        "segment_10": "You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.:",
        "segment_11": "user_proxy = autogen.UserProxyAgent( name=\"agent\", human_input_mode=\"TERMINATE\", max_consecutive_auto_reply=10, code_execution_config={\"work_dir\":\"_output\", \"use_docker\":\"python:3\"}, llm_config=llm_config, system_message=\"\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")",
        "segment_12": "blendsearch\u200b",
        "segment_13": "pyautogen<0.2 offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. Please install with the [blendsearch] option to use it.",
        "segment_14": "pip install \"pyautogen[blendsearch]<0.2\"",
        "segment_15": "Example notebooks:",
        "segment_16": "Optimize for Code Generation",
        "segment_17": "Optimize for Math",
        "segment_18": "retrievechat\u200b",
        "segment_19": "pyautogen supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the [retrievechat] option to use it.",
        "segment_20": "pip install \"pyautogen[retrievechat]\"",
        "segment_21": "RetrieveChat can handle various types of documents. By default, it can process",
        "segment_22": "plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',",
        "segment_23": "'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.",
        "segment_24": "If you install unstructured",
        "segment_25": "(pip install \"unstructured[all-docs]\"), additional document types such as 'docx',",
        "segment_26": "'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported.",
        "segment_27": "You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS.",
        "segment_28": "Example notebooks:",
        "segment_29": "Automated Code Generation and Question Answering with Retrieval Augmented Agents",
        "segment_30": "Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)",
        "segment_31": "Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents",
        "segment_32": "Teachability\u200b",
        "segment_33": "To use Teachability, please install AutoGen with the [teachable] option.",
        "segment_34": "pip install \"pyautogen[teachable]\"",
        "segment_35": "Example notebook: Chatting with a teachable agent",
        "segment_36": "Large Multimodal Model (LMM) Agents\u200b",
        "segment_37": "We offered Multimodal Conversable Agent and LLaVA Agent. Please install with the [lmm] option to use it.",
        "segment_38": "pip install \"pyautogen[lmm]\"",
        "segment_39": "Example notebooks:",
        "segment_40": "LLaVA Agent",
        "segment_41": "mathchat\u200b",
        "segment_42": "pyautogen<0.2 offers an experimental agent for math problem solving. Please install with the [mathchat] option to use it.",
        "segment_43": "pip install \"pyautogen[mathchat]<0.2\"",
        "segment_44": "Example notebooks:",
        "segment_45": "Using MathChat to Solve Math Problems",
        "segment_46": "Graph\u200b",
        "segment_47": "To use a graph in GroupChat, particularly for graph visualization, please install AutoGen with the [graph] option.",
        "segment_48": "pip install \"pyautogen[graph]\"",
        "segment_49": "Example notebook: Graph Modeling Language with using select_speakerEdit this pagePreviousDockerNextLLM Endpoint ConfigurationLLM CachingDockerblendsearchretrievechatTeachabilityLarge Multimodal Model (LMM) AgentsmathchatGraphCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class LLaVAAgent(MultimodalConversableAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name: str, system_message: Optional[Tuple[str, List]] = DEFAULT_LLAVA_SYS_MSG, *args, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - agent name.",
        "segment_7": "system_message str - system message for the ChatCompletion inference.",
        "segment_8": "Please override this attribute if you want to reprogram the agent.",
        "segment_9": "**kwargs dict - Please refer to other kwargs in",
        "segment_10": "ConversableAgent.",
        "segment_12": "llava_call\u200b",
        "segment_13": "def llava_call(prompt: str, llm_config: dict) -> str",
        "segment_14": "Makes a call to the LLaVA service to generate text based on a given promptEdit this pagePreviousimg_utilsNextmath_user_proxy_agentLLaVAAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class RetrieveUserProxyAgent(UserProxyAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name=\"RetrieveChatAgent\", human_input_mode: Optional[str] = \"ALWAYS\", is_termination_msg: Optional[Callable[[Dict], bool]] = None, retrieve_config: Optional[Dict] = None, **kwargs)",
        "segment_4": "Arguments:",
        "segment_7": "name str - name of the agent.",
        "segment_10": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_11": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_12": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_13": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_14": "or when is_termination_msg is True and there is no human input.",
        "segment_15": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_16": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_17": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_18": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_21": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_22": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_23": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_26": "retrieve_config dict or None - config for the retrieve agent.",
        "segment_27": "To use default config, set to None. Otherwise, set to a dictionary with the following keys:",
        "segment_29": "task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System",
        "segment_30": "prompt will be different for different tasks. The default value is default, which supports both code and qa.",
        "segment_31": "client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client chromadb.Client()",
        "segment_32": "will be used. If you want to use other vector db, extend this class and override the retrieve_docs function.",
        "segment_33": "docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file,",
        "segment_34": "the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.",
        "segment_35": "extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite.,",
        "segment_36": "when set to true it enables the system to assign unique IDs starting from \"length+i\" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..",
        "segment_37": "By default, \"extra_docs\" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.",
        "segment_38": "collection_name (Optional, str): the name of the collection.",
        "segment_39": "If key not provided, a default name autogen-docs will be used.",
        "segment_40": "model (Optional, str): the model to use for the retrieve chat.",
        "segment_41": "If key not provided, a default model gpt-4 will be used.",
        "segment_42": "chunk_token_size (Optional, int): the chunk token size for the retrieve chat.",
        "segment_43": "If key not provided, a default size max_tokens * 0.4 will be used.",
        "segment_44": "context_max_tokens (Optional, int): the context max token size for the retrieve chat.",
        "segment_45": "If key not provided, a default size human_input_mode0 will be used.",
        "segment_46": "chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are",
        "segment_47": "\"multi_lines\" and \"one_line\". If key not provided, a default mode human_input_mode1 will be used.",
        "segment_48": "must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.",
        "segment_49": "If chunk_mode is \"one_line\", this parameter will be ignored.",
        "segment_50": "embedding_model (Optional, str): the embedding model to use for the retrieve chat.",
        "segment_51": "If key not provided, a default model human_input_mode2 will be used. All available models",
        "segment_52": "can be found at human_input_mode3. The default model is a",
        "segment_53": "fast model. If you want to use a high performance model, human_input_mode4 is recommended.",
        "segment_54": "embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None,",
        "segment_55": "SentenceTransformer with the given human_input_mode5 will be used. If you want to use OpenAI, Cohere, HuggingFace or",
        "segment_56": "other embedding functions, you can pass it here, follow the examples in human_input_mode6.",
        "segment_57": "customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.",
        "segment_58": "customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".",
        "segment_59": "If not \"\" and the customized_answer_prefix is not in the answer, human_input_mode7 will be triggered.",
        "segment_60": "update_context (Optional, bool): if False, will not apply human_input_mode7 for interactive retrieval. Default is True.",
        "segment_61": "get_or_create (Optional, bool): if True, will create/return a collection for the retrieve chat. This is the same as that used in chromadb.",
        "segment_62": "Default is False. Will raise ValueError if the collection already exists and get_or_create is False. Will be set to True if docs_path is None.",
        "segment_63": "custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.",
        "segment_64": "The function should take (text:str, model:str) as input and return the token_count(int). the retrieve_config[\"model\"] will be passed in the function.",
        "segment_65": "Default is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.",
        "segment_66": "custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.",
        "segment_67": "Default is None, will use the default function in human_input_mode9.",
        "segment_68": "custom_text_types (Optional, List[str]): a list of file types to be processed. Default is is_termination_msg0.",
        "segment_69": "This only applies to files under the directories in is_termination_msg1. Explicitly included files and urls will be chunked regardless of their types.",
        "segment_70": "recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.",
        "segment_74": "is_termination_msg2 dict - other kwargs in UserProxyAgent.",
        "segment_75": "Example of overriding retrieve_docs:",
        "segment_76": "If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code.",
        "segment_77": "is_termination_msg3",
        "segment_80": "retrieve_docs\u200b",
        "segment_81": "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")",
        "segment_82": "Retrieve docs based on the given problem and assign the results to the class property _results.",
        "segment_83": "In case you want to customize the retrieval process, such as using a different vector db whose APIs are not",
        "segment_84": "compatible with chromadb or filter results with metadata, you can override this function. Just keep the current",
        "segment_85": "parameters and add your own parameters with default values, and keep the results in below type.",
        "segment_86": "Type of the results: Dict[str, List[List[Any]]], should have keys \"ids\" and \"documents\", \"ids\" for the ids of",
        "segment_87": "the retrieved docs and \"documents\" for the contents of the retrieved docs. Any other keys are optional. Refer",
        "segment_88": "to chromadb.api.types.QueryResult as an example.",
        "segment_89": "ids: List[string]",
        "segment_90": "documents: List[List[string]]",
        "segment_91": "Arguments:",
        "segment_93": "problem str - the problem to be solved.",
        "segment_94": "n_results int - the number of results to be retrieved. Default is 20.",
        "segment_95": "search_string str - only docs that contain an exact match of this string will be retrieved. Default is \"\".",
        "segment_97": "generate_init_message\u200b",
        "segment_98": "def generate_init_message(problem: str, n_results: int = 20, search_string: str = \"\")",
        "segment_99": "Generate an initial message with the given problem and prompt.",
        "segment_100": "Arguments:",
        "segment_102": "problem str - the problem to be solved.",
        "segment_103": "n_results int - the number of results to be retrieved.",
        "segment_104": "search_string str - only docs containing this string will be retrieved.",
        "segment_106": "Returns:",
        "segment_108": "str - the generated prompt ready to be sent to the assistant agent."
    },
    {
        "segment_1": "For technical details, please check our technical report and research publications.",
        "segment_3": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.",
        "segment_5": "@inproceedings{wu2023autogen, title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework}, author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang}, year={2023}, eprint={2308.08155}, archivePrefix={arXiv}, primaryClass={cs.AI}}",
        "segment_7": "Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference. Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah. AutoML'23.",
        "segment_9": "@inproceedings{wang2023EcoOptiGen, title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference}, author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah}, year={2023}, booktitle={AutoML'23},}",
        "segment_11": "An Empirical Study on Challenging Math Problem Solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).",
        "segment_13": "@inproceedings{wu2023empirical, title={An Empirical Study on Challenging Math Problem Solving with GPT-4}, author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang}, year={2023}, booktitle={ArXiv preprint arXiv:2306.01337},}",
        "segment_15": "EcoAssistant: Using LLM Assistant More Affordably and Accurately. Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang. ArXiv preprint arXiv:2310.03046 (2023).",
        "segment_17": "@inproceedings{zhang2023ecoassistant, title={EcoAssistant: Using LLM Assistant More Affordably and Accurately}, author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi}, year={2023}, booktitle={ArXiv preprint arXiv:2310.03046},}Edit this pagePreviousContributingNextMigration GuideCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "We introduce Teachable Agents so that users can teach their LLM-based assistants new facts, preferences, and skills.",
        "segment_4": "We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.",
        "segment_6": "Introduction\u200b",
        "segment_7": "Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.",
        "segment_8": "Teachability addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.",
        "segment_9": "Any instantiated agent that inherits from ConversableAgent can be made teachable by instantiating a Teachability object and calling its add_to_agent(agent) method.",
        "segment_10": "In order to make effective decisions about memo storage and retrieval, the Teachability object calls an instance of TextAnalyzerAgent (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.",
        "segment_11": "Run It Yourself\u200b",
        "segment_12": "AutoGen contains four code examples that use Teachability.",
        "segment_15": "Run chat_with_teachable_agent.py to converse with a teachable agent.",
        "segment_18": "Run test_teachable_agent.py for quick unit testing of a teachable agent.",
        "segment_21": "Use the Jupyter notebook agentchat_teachability.ipynb to step through examples discussed below.",
        "segment_24": "Use the Jupyter notebook agentchat_teachable_oai_assistants.ipynb to make arbitrary OpenAI Assistants teachable through GPTAssistantAgent.",
        "segment_27": "Basic Usage of Teachability\u200b",
        "segment_29": "Install dependencies",
        "segment_31": "Please install pyautogen with the [teachable] option before using Teachability.",
        "segment_32": "pip install \"pyautogen[teachable]\"",
        "segment_34": "Import agents",
        "segment_36": "from autogen import UserProxyAgent, config_list_from_jsonfrom autogen.agentchat.contrib.capabilities.teachability import Teachabilityfrom autogen import ConversableAgent # As an example",
        "segment_38": "Create llm_config",
        "segment_40": "# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_samplefilter_dict = {\"model\": [\"gpt-4\"]} # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", filter_dict=filter_dict)llm_config={\"config_list\": config_list, \"timeout\": 120}",
        "segment_42": "Create the agents",
        "segment_44": "# Start by instantiating any agent that inherits from ConversableAgent, which we use directly here for simplicity.teachable_agent = ConversableAgent( name=\"teachable_agent\", # The name can be anything. llm_config=llm_config)# Instantiate a Teachability object. Its parameters are all optional.teachability = Teachability( reset_db=False, # Use True to force-reset the memo DB, and False to use an existing DB. path_to_db_dir=\"./tmp/interactive/teachability_db\" # Can be any path, but teachable agents in a group chat require unique paths.)# Now add teachability to the agent.teachability.add_to_agent(teachable_agent)# For this test, create a user proxy agent as usual.user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")",
        "segment_46": "Chat with the teachable agent",
        "segment_48": "# This function will return once the user types 'exit'.teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")",
        "segment_49": "Example 1 - Learning user info\u200b",
        "segment_50": "A user can teach the agent facts about themselves.",
        "segment_51": "(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)",
        "segment_52": "Loading previous memory (if any) from disk.teachable_agent (to user):Greetings, I'm a teachable user assistant! What's on your mind today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: My name is Rickyuser (to teachable_agent):My name is Ricky-------------------------------------------------------------------------------->>>>>>>> USING AUTO REPLY...teachable_agent (to user):Hello Ricky! How can I assist you today?--------------------------------------------------------------------------------Provide feedback to teachable_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit",
        "segment_53": "In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)",
        "segment_54": "--------------------------------------------------------------------------------user (to teachable_agent):what's my name?--------------------------------------------------------------------------------teachable_agent (to user):Hi Ricky! How can I help you today?--------------------------------------------------------------------------------",
        "segment_55": "Example 2 - Learning new facts\u200b",
        "segment_56": "A user can teach the agent more complex, related facts.",
        "segment_57": "--------------------------------------------------------------------------------user (to teachable_agent):What is the Vicuna model?--------------------------------------------------------------------------------teachable_agent (to user):I'm sorry, but I don't have information about the \"Vicuna model\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?--------------------------------------------------------------------------------user (to teachable_agent):Vicuna is a 13B-parameter language model released by Meta.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.--------------------------------------------------------------------------------user (to teachable_agent):What is the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta's advanced language model. While I don't have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.--------------------------------------------------------------------------------user (to teachable_agent):Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.--------------------------------------------------------------------------------teachable_agent (to user):Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.Orca's advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.",
        "segment_58": "Then in a later chat the teachable agent can answer questions about the facts it has been taught.",
        "segment_59": "(Remember to first close the previous chat by typing 'exit'.)",
        "segment_60": "--------------------------------------------------------------------------------user (to teachable_agent):How does the Vicuna model compare to the Orca model?--------------------------------------------------------------------------------teachable_agent (to user):The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.--------------------------------------------------------------------------------",
        "segment_61": "Example 3 - Learning user preferences\u200b",
        "segment_62": "A user can teach the agent how they prefer to have things done.",
        "segment_63": "Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.",
        "segment_64": "Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.",
        "segment_65": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.--------------------------------------------------------------------------------Please summarize this abstract. When I'm summarizing an abstract, I try to make the summary contain just three short bullet points: the title, the innovation, and the key empirical results.AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent ConversationQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi WangAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.--------------------------------------------------------------------------------teachable_agent (to user):- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.--------------------------------------------------------------------------------",
        "segment_66": "Then in later chats the teacher doesn't need to reiterate their detailed preferences.",
        "segment_67": "--------------------------------------------------------------------------------user (to teachable_agent):Please summarize this abstract.Sparks of Artificial General Intelligence: Early experiments with GPT-4S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi ZhangArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.--------------------------------------------------------------------------------teachable_agent (to user):- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.--------------------------------------------------------------------------------",
        "segment_68": "Example 4 - Learning new skills\u200b",
        "segment_69": "Users can extend the teachable agent's capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.",
        "segment_70": "The Sparks of AGI paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4's success rate above 95%.",
        "segment_71": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------Step 1: Identify the current equation and the target value on the right-hand side.Current equation: 9 * 4 + 6 * 6 = 72Target value: 99Step 2: Determine what numbers can be changed on the left-hand side.Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.9 -> If changed, it would create a larger product.4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.Step 4: Write down the solution.The solution is: 9 * 4 + 6 * 7--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:9 * 4 + 6 * 6 = 72Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".Here's some advice that may help:1. Let E denote the original number on the right.2. Let F denote the final number on the right.3. Calculate the difference between the two, G = F - E.4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.5. Calculate J = G / H. This is the number of times that H divides into G.6. Verify that J is an integer, and that H * J = G.7. Find the number on the left which is multiplied by H, and call it K.8. Change K to K + J.9. Recompute the value on the left, and verify that it equals F.Finally, write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Step 1: Identify the original number (E) and the final number (F) on the right-hand side.E = 72F = 99Step 2: Calculate the difference between the two, G = F - E.G = 99 - 72 = 27Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.9 (divides evenly into 27)4 (does not)6 (first occurrence, does not)6 (second occurrence, does not)Step 4: Calculate J = G / H. This is the number of times that H divides into G.H = 9 (since 9 divides evenly into 27)J = 27 / 9 = 3Step 5: Verify that J is an integer, and that H * J = G.H * J = 9 * 3 = 27J = 3 (which is an integer)Step 6: Find the number on the left which is multiplied by H, and call it K.Number on the leftside of the equation that is multiplied by 9: 4Therefore, K = 4Step 7: Change K to K + J.K + J = 4 + 3 = 7Step 8: Recompute the value on the left, and verify that it equals F.9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)Step 9: Write down the solution.The solution is: 9 * 7 + 6 * 6--------------------------------------------------------------------------------",
        "segment_72": "In a later chat the user doesn't need to repeat the detailed advice.",
        "segment_73": "--------------------------------------------------------------------------------user (to teachable_agent):Consider the identity:8 * 3 + 7 * 9 = 87Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".--------------------------------------------------------------------------------teachable_agent (to user):Let's follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:1. Let E denote the original number on the right: E = 87.2. Let F denote the final number on the right: F = 59.3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).5. Calculate J = G / H: J = -28 / 7 = -4.6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.8. Change K to K + J: K_new = 9 + (-4) = 5.9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.The solution is: 8 * 3 + 7 * 5--------------------------------------------------------------------------------",
        "segment_74": "Planned improvements\u200b",
        "segment_76": "Understanding user instructions distributed over multiple turns.",
        "segment_77": "Learning from the agent's own experience, to reduce dependence on explicit user teachings.",
        "segment_78": "Learning skills built on top of previously learned skills.",
        "segment_80": "Conclusion\u200b",
        "segment_81": "Teachability is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our Discord channel. We look forward to seeing how you and the rest of the community can use and improve teachable agents in AutoGen!Tags:LLMteachNewer PostMultimodal with GPT-4V and LLaVAOlder PostRetrieval-Augmented Generation (RAG) Applications with AutoGenIntroductionRun It YourselfBasic Usage of TeachabilityExample 1 - Learning user infoExample 2 - Learning new factsExample 3 - Learning user preferencesExample 4 - Learning new skillsPlanned improvementsConclusionCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_2": "Set your API endpoints",
        "segment_4": "Use the constructed configuration list in agents",
        "segment_5": "Unexpected keyword argument 'base_url'",
        "segment_6": "How does an agent decide which model to pick out of the list?",
        "segment_7": "Can I use non-OpenAI models?",
        "segment_10": "Handle Rate Limit Error and Timeout Error",
        "segment_11": "How to continue a finished conversation",
        "segment_12": "How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?",
        "segment_13": "Why is code not saved as file?",
        "segment_14": "Code execution",
        "segment_16": "Enable Python 3 docker image",
        "segment_17": "Agents keep thanking each other when using gpt-3.5-turbo",
        "segment_20": "ChromaDB fails in codespaces because of old version of sqlite3",
        "segment_21": "How to register a reply function",
        "segment_22": "How to get last message?",
        "segment_23": "How to get each agent message?",
        "segment_24": "When using autogen docker, is it always necessary to reinstall modules?",
        "segment_25": "Agents are throwing due to docker not running, how can I resolve this?",
        "segment_27": "Set your API endpoints\u200b",
        "segment_28": "Autogen relies on 3rd party API endpoints for LLM inference. It works great with OpenAI and Azure OpenAI models, and can work with any model (self-hosted or local) that is accessible through an inference server compatible with OpenAI Chat Completions API.",
        "segment_29": "An agent requires a list of configuration dictionaries for setting up model endpoints. Each configuration dictionary can contain the following keys:",
        "segment_31": "model (str): Required. The identifier of the model to be used, such as 'gpt-4', 'gpt-3.5-turbo', or 'llama-7B'.",
        "segment_32": "api_key (str): Optional. The API key required for authenticating requests to the model's API endpoint.",
        "segment_33": "api_type (str): Optional. The type of API service being used. This could be 'azure' for Azure Cognitive Services, 'openai' for OpenAI, or other custom types.",
        "segment_34": "base_url (str): Optional. The base URL of the API endpoint. This is the root address where API calls are directed.",
        "segment_35": "api_version (str): Optional. The version of the Azure API you wish to use",
        "segment_37": "For example:",
        "segment_38": "config_list = [ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-12-01-preview\", }, { \"model\": \"llama-7B\", \"base_url\": \"http://127.0.0.1:8080\", \"api_type\": \"openai\", }]",
        "segment_39": "In autogen module there are multiple helper functions allowing to construct configurations using different sources:",
        "segment_41": "get_config_list: Generates configurations for API calls, primarily from provided API keys.",
        "segment_42": "config_list_openai_aoai: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files.",
        "segment_43": "config_list_from_json: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria.",
        "segment_44": "config_list_from_models: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration.",
        "segment_45": "config_list_from_dotenv: Constructs a configuration list from a .env file, offering a consolidated way to manage multiple API configurations and keys from a single file.",
        "segment_47": "We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints.",
        "segment_48": "Use the constructed configuration list in agents\u200b",
        "segment_49": "Make sure the \"config_list\" is included in the llm_config in the constructor of the LLM-based agent. For example,",
        "segment_50": "assistant = autogen.AssistantAgent( name=\"assistant\", llm_config={\"config_list\": config_list})",
        "segment_51": "The llm_config is used in the create function for LLM inference.",
        "segment_52": "When llm_config is not provided, the agent will rely on other openai settings such as openai.api_key or the environment variable OPENAI_API_KEY, which can also work when you'd like to use a single endpoint.",
        "segment_53": "You can also explicitly specify that by:",
        "segment_54": "assistant = autogen.AssistantAgent(name=\"assistant\", llm_config={\"api_key\": ...})",
        "segment_55": "How does an agent decide which model to pick out of the list?\u200b",
        "segment_56": "An agent uses the very first model available in the \"config_list\" and makes LLM calls against this model. If the model fail (e.g. API throttling) the agent will retry the request against the 2nd model and so on until prompt completion is received (or throws an error if none of the models successfully completes the request). There's no implicit/hidden logic inside agents that is used to pick \"the best model for the task\". It is developers responsibility to pick the right models and use them with agents.",
        "segment_57": "Besides throttling/rotating models the 'config_list' can be useful for:",
        "segment_59": "Having a single global list of models and filtering it based on certain keys (e.g. name, tag) in order to pass select models into a certain agent (e.g. use cheaper GPT 3.5 for agents solving easier tasks)",
        "segment_60": "Using more advanced features for special purposes related to inference, such as filter_func with OpenAIWrapper or inference optimization",
        "segment_62": "Unexpected keyword argument 'base_url'\u200b",
        "segment_63": "In version >=1, OpenAI renamed their api_base parameter to base_url. So for older versions, use api_base but for newer versions use base_url.",
        "segment_64": "Can I use non-OpenAI models?\u200b",
        "segment_65": "Yes. You currently have two options:",
        "segment_67": "Autogen can work with any API endpoint which complies with OpenAI-compatible RESTful APIs - e.g. serving local LLM via FastChat or LM Studio. Please check https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs for an example.",
        "segment_68": "You can supply your own custom model implementation and use it with Autogen. Please check https://microsoft.github.io/autogen/blog/2024/01/26/Custom-Models for more information.",
        "segment_70": "Handle Rate Limit Error and Timeout Error\u200b",
        "segment_71": "You can set max_retries to handle rate limit error. And you can set timeout to handle timeout error. They can all be specified in llm_config for an agent, which will be used in the OpenAI client for LLM inference. They can be set differently for different clients if they are set in the config_list.",
        "segment_73": "max_retries (int): the total number of times allowed for retrying failed requests for a single client.",
        "segment_74": "timeout (int): the timeout (in seconds) for a single client.",
        "segment_76": "Please refer to the documentation for more info.",
        "segment_77": "How to continue a finished conversation\u200b",
        "segment_78": "When you call initiate_chat the conversation restarts by default. You can use send or initiate_chat(clear_history=False) to continue the conversation.",
        "segment_79": "How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?\u200b",
        "segment_80": "Each agent can be customized. You can use LLMs, tools, or humans behind each agent. If you use an LLM for an agent, use the one best suited for its role. There is no limit of the number of agents, but start from a small number like 2, 3. The more capable is the LLM and the fewer roles you need, the fewer agents you need.",
        "segment_81": "The default user proxy agent doesn't use LLM. If you'd like to use an LLM in UserProxyAgent, the use case could be to simulate user's behavior.",
        "segment_82": "The default assistant agent is instructed to use both coding and language skills. It doesn't have to do coding, depending on the tasks. And you can customize the system message. So if you want to use it for coding, use a model that's good at coding.",
        "segment_83": "Why is code not saved as file?\u200b",
        "segment_84": "If you are using a custom system message for the coding agent, please include something like:",
        "segment_85": "If you want the user to save the code in a file before executing it, put # filename: inside the code block as the first line.",
        "segment_86": "in the system message. This line is in the default system message of the AssistantAgent.",
        "segment_87": "If the # filename doesn't appear in the suggested code still, consider adding explicit instructions such as \"save the code to disk\" in the initial user message in initiate_chat.",
        "segment_88": "The AssistantAgent doesn't save all the code by default, because there are cases in which one would just like to finish a task without saving the code.",
        "segment_89": "Code execution\u200b",
        "segment_90": "We strongly recommend using docker to execute code. There are two ways to use docker:",
        "segment_92": "Run AutoGen in a docker container. For example, when developing in GitHub codespace, AutoGen runs in a docker container. If you are not developing in Github codespace, follow instructions here to install and run AutoGen in docker.",
        "segment_93": "Run AutoGen outside of a docker, while performing code execution with a docker container. For this option, make sure docker is up and running. If you want to run the code locally (not recommended) then use_docker can be set to False in code_execution_config for each code-execution agent, or set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_95": "Enable Python 3 docker image\u200b",
        "segment_96": "You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.:",
        "segment_97": "user_proxy = autogen.UserProxyAgent( name=\"agent\", human_input_mode=\"TERMINATE\", max_consecutive_auto_reply=10, code_execution_config={\"work_dir\":\"_output\", \"use_docker\":\"python:3\"}, llm_config=llm_config, system_message=\"\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\")",
        "segment_98": "If you have problems with agents running pip install or get errors similar to Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'), you can choose 'python:3' as image as shown in the code example above and that should solve the problem.",
        "segment_99": "Agents keep thanking each other when using gpt-3.5-turbo\u200b",
        "segment_100": "When using gpt-3.5-turbo you may often encounter agents going into a \"gratitude loop\", meaning when they complete a task they will begin congratulating and thanking each other in a continuous loop. This is a limitation in the performance of gpt-3.5-turbo, in contrast to gpt-4 which has no problem remembering instructions. This can hinder the experimentation experience when trying to test out your own use case with cheaper models.",
        "segment_101": "A workaround is to add an additional termination notice to the prompt. This acts a \"little nudge\" for the LLM to remember that they need to terminate the conversation when their task is complete. You can do this by appending a string such as the following to your user input string:",
        "segment_102": "prompt = \"Some user query\"termination_notice = ( '\\n\\nDo not show appreciation in your responses, say only what is necessary. ' 'if \"Thank you\" or \"You\\'re welcome\" are said in the conversation, then say TERMINATE ' 'to indicate the conversation is finished and this is your last message.')prompt += termination_notice",
        "segment_103": "Note: This workaround gets the job done around 90% of the time, but there are occurrences where the LLM still forgets to terminate the conversation.",
        "segment_104": "ChromaDB fails in codespaces because of old version of sqlite3\u200b",
        "segment_105": "(from issue #251)",
        "segment_106": "Code examples that use chromadb (like retrieval) fail in codespaces due to a sqlite3 requirement.",
        "segment_107": ">>> import chromadbTraceback (most recent call last): File \"\", line 1, in File \"/home/vscode/.local/lib/python3.10/site-packages/chromadb/__init__.py\", line 69, in raise RuntimeError(RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade.",
        "segment_108": "Workaround:",
        "segment_110": "pip install pysqlite3-binary",
        "segment_111": "mkdir /home/vscode/.local/lib/python3.10/site-packages/google/colab",
        "segment_113": "Explanation: Per this gist, linked from the official chromadb docs, adding this folder triggers chromadb to use pysqlite3 instead of the default.",
        "segment_114": "How to register a reply function\u200b",
        "segment_115": "(from issue #478)",
        "segment_116": "See here https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#register_reply",
        "segment_117": "For example, you can register a reply function that gets called when generate_reply is called for an agent.",
        "segment_118": "def print_messages(recipient, messages, sender, config): if \"callback\" in config and config[\"callback\"] is not None: callback = config[\"callback\"] callback(sender, recipient, messages[-1]) print(f\"Messages sent to: {recipient.name} | num messages: {len(messages)}\") return False, None # required to ensure the agent communication flow continuesuser_proxy.register_reply( [autogen.Agent, None], reply_func=print_messages, config={\"callback\": None},)assistant.register_reply( [autogen.Agent, None], reply_func=print_messages, config={\"callback\": None},)",
        "segment_119": "In the above, we register a print_messages function that is called each time the agent's generate_reply is triggered after receiving a message.",
        "segment_120": "How to get last message ?\u200b",
        "segment_121": "Refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#last_message",
        "segment_122": "How to get each agent message ?\u200b",
        "segment_123": "Please refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#chat_messages",
        "segment_124": "When using autogen docker, is it always necessary to reinstall modules?\u200b",
        "segment_125": "The \"use_docker\" arg in an agent's code_execution_config will be set to the name of the image containing the change after execution, when the conversation finishes.",
        "segment_126": "You can save that image name. For a new conversation, you can set \"use_docker\" to the saved name of the image to start execution there.",
        "segment_127": "Database locked error\u200b",
        "segment_128": "When using VMs such as Azure Machine Learning compute instances,",
        "segment_129": "you may encounter a \"database locked error\". This is because the",
        "segment_130": "LLM cache",
        "segment_131": "is trying to write to a location that the application does not have access to.",
        "segment_132": "You can set the cache_path_root to a location where the application has access.",
        "segment_133": "For example,",
        "segment_134": "from autogen import Cachewith Cache.disk(cache_path_root=\"/tmp/.cache\") as cache: agent_a.initate_chat(agent_b, ..., cache=cache)",
        "segment_135": "You can also use Redis cache instead of disk cache. For example,",
        "segment_136": "from autogen import Cachewith Cache.redis(redis_url=...) as cache: agent_a.initate_chat(agent_b, ..., cache=cache)",
        "segment_137": "You can also disable the cache. See here for details.",
        "segment_138": "Agents are throwing due to docker not running, how can I resolve this?\u200b",
        "segment_139": "If running AutoGen locally the default for agents who execute code is for them to try and perform code execution within a docker container. If docker is not running, this will cause the agent to throw an error. To resolve this you have some options.",
        "segment_140": "If you want to disable code execution entirely\u200b",
        "segment_142": "Set code_execution_config to False for each code-execution agent. E.g.:",
        "segment_144": "user_proxy = autogen.UserProxyAgent( name=\"agent\", llm_config=llm_config, code_execution_config=False)",
        "segment_145": "If you want to run code execution in docker\u200b",
        "segment_147": "Recommended: Make sure docker is up and running.",
        "segment_149": "If you want to run code execution locally\u200b",
        "segment_151": "use_docker can be set to False in code_execution_config for each code-execution agent.",
        "segment_152": "To set it for all code-execution agents at once: set AUTOGEN_USE_DOCKER to False as an environment variable.",
        "segment_154": "E.g.:",
        "segment_155": "user_proxy = autogen.UserProxyAgent( name=\"agent\", llm_config=llm_config, code_execution_config={\"work_dir\":\"coding\", \"use_docker\":False})Edit this pageSet your API endpointsUse the constructed configuration list in agentsHow does an agent decide which model to pick out of the list?Unexpected keyword argument 'base_url'Can I use non-OpenAI models?Handle Rate Limit Error and Timeout ErrorHow to continue a finished conversationHow do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?Why is code not saved as file?Code executionEnable Python 3 docker imageAgents keep thanking each other when using gpt-3.5-turboChromaDB fails in codespaces because of old version of sqlite3How to register a reply functionHow to get last message ?How to get each agent message ?When using autogen docker, is it always necessary to reinstall modules?Database locked errorAgents are throwing due to docker not running, how can I resolve this?If you want to disable code execution entirelyIf you want to run code execution in dockerIf you want to run code execution locallyCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_2": "Introducing AgentOptimizer, a new class for training LLM agents in the era of LLMs as a service.",
        "segment_3": "AgentOptimizer is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.",
        "segment_4": "Checkout one implementation for AgentOptimizer on MATH dataset",
        "segment_5": "here.",
        "segment_6": "Paper is coming soon!",
        "segment_7": "Introduction\u200b",
        "segment_8": "In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?",
        "segment_9": "Here, we take an initial step towards the agent training.",
        "segment_10": "Inspired by the function calling capabilities provided by OpenAI,",
        "segment_11": "we draw an analogy between model weights and agent functions/skills, and update an agent\u2019s functions/skills based on its historical performance on a training set.",
        "segment_12": "Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents\u2019 functions as a set of function calls,",
        "segment_13": "to support iteratively adding, revising, and removing existing functions.",
        "segment_14": "As an agentic way of training an agent, our approach helps enhance the agents\u2019 abilities without requiring access to the LLMs weights.",
        "segment_15": "AgentOptimizer\u200b",
        "segment_16": "AgentOptimizer is a class designed to optimize the agents by improving their function calls.",
        "segment_17": "It contains two core methods:",
        "segment_19": "step(): step() takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).",
        "segment_21": "actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)",
        "segment_22": "It has two outputs actions and updated_functions. actions is a series of actions to manipulate the current functions. And updated_functions is the updated functions after the actions are applied (including code implementation).",
        "segment_24": "update_function_call():",
        "segment_25": "This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from step().",
        "segment_26": "For AssistantAgent, it first uses update_function_signature to update the function signatures.",
        "segment_27": "Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from step().",
        "segment_29": "Sometimes, the function signatures (JSON schema) returned by the step() may not be valid, and the generated code may also face syntax errors.",
        "segment_30": "AgentOptimizer includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.",
        "segment_31": "Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.",
        "segment_32": "Pseudocode for the optimization process\u200b",
        "segment_33": "The optimization process is as follows:",
        "segment_34": "for - in range(EPOCH): history, statistic, current_functions = solve_problems(train_problems) actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions) AgentOptimizer.update_function_call(actions)",
        "segment_35": "Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.",
        "segment_36": "The functions are then improved using AgentOptimizer.",
        "segment_37": "Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.",
        "segment_38": "After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks",
        "segment_39": "The implementation technology behind the AgentOptimizer\u200b",
        "segment_40": "To obtain stable and structured function signatures and code implementations from AgentOptimizer,",
        "segment_41": "we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.",
        "segment_42": "Specifically, we introduce three function calls to manipulate the current functions at each step: add_function, remove_function, and revise_function.",
        "segment_43": "These calls add, remove, and revise functions in the existing function list, respectively.",
        "segment_44": "This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.",
        "segment_45": "Below is the JSON schema of these function calls:",
        "segment_47": "add_function: Add one new function that may be used in the future tasks.",
        "segment_49": "ADD_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"add_function\", \"description\": \"Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_51": "revise_function: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.",
        "segment_53": "REVISE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"revise_function\", \"description\": \"Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_55": "remove_function: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.",
        "segment_57": "REMOVE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"remove_function\", \"description\": \"Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" } }, \"required\": [\"name\"] } }}",
        "segment_58": "Limitation & Future work\u200b",
        "segment_60": "Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.",
        "segment_61": "When the training epoch is small, the agent\u2019s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.",
        "segment_62": "The Current implementation of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.",
        "segment_63": "It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.",
        "segment_64": "Currently, it only supports optimizing the multi-agent system in solving problems from MATH dataset. We will integrate it into autogen with more general interface in the future.",
        "segment_65": "Tags:LLMresearchAgent AutoBuild - Automatically Building Multi-agent SystemsNovember 26, 2023 \u00b7 7 min readLinxin SongMS student at Waseda UniversityJieyu ZhangPhD student at University of Washington",
        "segment_66": "TL;DR:",
        "segment_67": "Introducing AutoBuild, building multi-agent system automatically, fast, and easily for complex tasks with minimal",
        "segment_68": "user prompt required, powered by a new designed class AgentBuilder. AgentBuilder also supports open-source LLMs by",
        "segment_69": "leveraging vLLM and FastChat.",
        "segment_70": "Checkout example notebooks and source code for reference:",
        "segment_72": "AutoBuild Examples",
        "segment_73": "AgentBuilder",
        "segment_75": "Introduction\u200b",
        "segment_76": "In this blog, we introduce AutoBuild, a pipeline that can automatically build multi-agent systems for complex tasks.",
        "segment_77": "Specifically, we design a new class called AgentBuilder, which will complete the generation of participant expert agents",
        "segment_78": "and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.",
        "segment_79": "AgentBuilder supports open-source models on Hugging Face powered by vLLM",
        "segment_80": "and FastChat. Once the user chooses to use open-source LLM, AgentBuilder will set",
        "segment_81": "up an endpoint server automatically without any user participation.",
        "segment_82": "Installation\u200b",
        "segment_84": "AutoGen:",
        "segment_86": "pip install pyautogen[autobuild]",
        "segment_88": "(Optional: if you want to use open-source LLMs) vLLM and FastChat",
        "segment_90": "pip install vllm fastchat",
        "segment_91": "Basic Example\u200b",
        "segment_92": "In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.",
        "segment_93": "Step 1: prepare configurations\u200b",
        "segment_94": "First, we need to prepare the Agent configurations.",
        "segment_95": "Specifically, a config path containing the model name and API key, and a default config for each agent, are required.",
        "segment_96": "config_file_or_env = '/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST' # modify pathdefault_llm_config = { 'temperature': 0}",
        "segment_97": "Step 2: create an AgentBuilder instance\u200b",
        "segment_98": "Then, we create an AgentBuilder instance with the config path and default config.",
        "segment_99": "You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.",
        "segment_100": "from autogen.agentchat.contrib.agent_builder import AgentBuilderbuilder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model='gpt-4-1106-preview', agent_model='gpt-4-1106-preview')",
        "segment_101": "Step 3: specify the building task\u200b",
        "segment_102": "Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.",
        "segment_103": "Note that your building task should have a general description of the task. Adding some specific examples is better.",
        "segment_104": "building_task = \"Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"",
        "segment_105": "Step 4: build group chat agents\u200b",
        "segment_106": "Use build() to let the build manager (with a builder_model as backbone) complete the group chat agents generation.",
        "segment_107": "If you think coding is necessary for your task, you can use coding=True to add a user proxy (a local code interpreter) into the agent list as:",
        "segment_108": "agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)",
        "segment_109": "If coding is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.",
        "segment_110": "The generated agent_list is a list of AssistantAgent instances.",
        "segment_111": "If coding is true, a user proxy (a UserProxyAssistant instance) will be added as the first element to the agent_list.",
        "segment_112": "agent_configs is a list of agent configurations including agent name, backbone LLM model, and system message.",
        "segment_113": "For example",
        "segment_114": "// an example of agent_configs. AgentBuilder will generate agents with the following configurations.[ { \"name\": \"ArXiv_Data_Scraper_Developer\", \"model\": \"gpt-4-1106-preview\", \"system_message\": \"You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \\n\\nDuring your mission, it\\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\\n\\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\\n\\nWrap up your participation by confirming the user's need has been satisfied with the data scraping solutions you've provided. Indicate the completion of your task by replying \\\"TERMINATE\\\" in the group chat.\", \"description\": \"ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.\" }, ...]",
        "segment_115": "Step 5: execute the task\u200b",
        "segment_116": "Let agents generated in build() complete the task collaboratively in a group chat.",
        "segment_117": "import autogendef start_task(execution_task: str, agent_list: list, llm_config: dict): config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={\"model\": [\"gpt-4-1106-preview\"]}) group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12) manager = autogen.GroupChatManager( groupchat=group_chat, llm_config={\"config_list\": config_list, **llm_config} ) agent_list[0].initiate_chat(manager, message=execution_task)start_task( execution_task=\"Find a recent paper about gpt-4 on arxiv and find its potential applications in software.\", agent_list=agent_list, llm_config=default_llm_config)",
        "segment_118": "Step 6 (Optional): clear all agents and prepare for the next task\u200b",
        "segment_119": "You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.",
        "segment_120": "builder.clear_all_agents(recycle_endpoint=True)",
        "segment_121": "If the agent's backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.",
        "segment_122": "If necessary, you can use recycle_endpoint=False to retain the previous open-source LLM's endpoint server.",
        "segment_123": "Save and Load\u200b",
        "segment_124": "You can save all necessary information of the built group chat agents by",
        "segment_125": "saved_path = builder.save()",
        "segment_126": "Configurations will be saved in JSON format with the following content:",
        "segment_127": "// FILENAME: save_config_TASK_MD5.json{ \"building_task\": \"Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.\", \"agent_configs\": [ { \"name\": \"...\", \"model\": \"...\", \"system_message\": \"...\", \"description\": \"...\" }, ... ], \"manager_system_message\": \"...\", \"code_execution_config\": {...}, \"default_llm_config\": {...}}",
        "segment_128": "You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename save_config_TASK_MD5.json.",
        "segment_129": "You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.",
        "segment_130": "new_builder = AgentBuilder(config_file_or_env=config_file_or_env)agent_list, agent_config = new_builder.load(saved_path)start_task(...) # skip build()",
        "segment_131": "Use OpenAI Assistant\u200b",
        "segment_132": "Assistants API allows you to build AI assistants within your own applications.",
        "segment_133": "An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.",
        "segment_134": "AutoBuild also supports the assistant API by adding use_oai_assistant=True to build().",
        "segment_135": "# Transfer to the OpenAI Assistant API.agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)...",
        "segment_136": "(Experimental) Use Open-source LLM\u200b",
        "segment_137": "AutoBuild supports open-source LLM by vLLM and FastChat.",
        "segment_138": "Check the supported model list here.",
        "segment_139": "After satisfying the requirements, you can add an open-source LLM's huggingface repository to the config file,",
        "segment_140": "// Add the LLM's huggingface repo to your config file and use EMPTY as the api_key.[ ... { \"model\": \"meta-llama/Llama-2-13b-chat-hf\", \"api_key\": \"EMPTY\" }]",
        "segment_141": "and specify it when initializing AgentBuilder.",
        "segment_142": "AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.",
        "segment_143": "Future work/Roadmap\u200b",
        "segment_145": "Let the builder select the best agents from a given library/database to solve the task.",
        "segment_147": "Summary\u200b",
        "segment_148": "We propose AutoBuild with a new class AgentBuilder.",
        "segment_149": "AutoBuild can help user solve their complex task with an automatically built multi-agent system.",
        "segment_150": "AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.",
        "segment_151": "More advanced features are coming soon.Tags:LLMresearchMathChat - An Conversational Framework to Solve Math ProblemsJune 28, 2023 \u00b7 8 min readYiran WuPhD student at Pennsylvania State University",
        "segment_152": "TL;DR:",
        "segment_154": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_155": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_156": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_158": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_159": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_160": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_161": "The MathChat Framework\u200b",
        "segment_162": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_163": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_166": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_169": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_171": "Writing a Python program to solve the problem directly.",
        "segment_172": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_173": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_177": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_180": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_181": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_182": "(The conversation is modified for readability.)",
        "segment_183": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_184": "Experiment Setup\u200b",
        "segment_185": "We evaluate the improvement brought by MathChat.",
        "segment_186": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_187": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_188": "We also evaluate the following methods for comparison:",
        "segment_191": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_194": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_197": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_200": "Experiment Results\u200b",
        "segment_201": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_203": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_204": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_205": "The code for experiments can be found at this repository.",
        "segment_206": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_207": "Future Directions\u200b",
        "segment_208": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_209": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_211": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_212": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_213": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_215": "For Further Reading\u200b",
        "segment_217": "Research paper of MathChat",
        "segment_218": "Documentation about autogen",
        "segment_220": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchAchieve More, Pay Less - Use GPT-4 SmartlyMay 18, 2023 \u00b7 8 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_221": "TL;DR:",
        "segment_223": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_225": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_226": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_227": "Observations\u200b",
        "segment_229": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_230": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_232": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_233": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_234": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_235": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_236": "What else can we do? We notice that:",
        "segment_237": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_238": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_239": "Solution\u200b",
        "segment_240": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_242": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_243": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_246": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_247": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_249": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_250": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_251": "GPT-4, n=1, temperature=0",
        "segment_252": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_253": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_255": "Experiment Results\u200b",
        "segment_256": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_257": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_258": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_260": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_262": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_264": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_265": "Solved by GPT-4, n=1, temperature=0:",
        "segment_267": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_269": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_271": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_273": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_275": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_276": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_278": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_279": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_281": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_282": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_283": "Discussion\u200b",
        "segment_284": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_285": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_287": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_288": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_290": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_291": "Different configurations have complementary effect.",
        "segment_295": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_296": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_297": "There are many directions of extensions in research and development:",
        "segment_299": "Generalize the way to provide feedback.",
        "segment_300": "Automate the process of optimizing the configurations.",
        "segment_301": "Build adaptive agents for different applications.",
        "segment_303": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_304": "For Further Reading\u200b",
        "segment_306": "Documentation about autogen and Research paper.",
        "segment_307": "Blog post about a related study for math.",
        "segment_308": "Tags:LLMGPTresearchDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHApril 21, 2023 \u00b7 6 min readChi WangPrincipal Researcher at Microsoft Research",
        "segment_309": "TL;DR:",
        "segment_311": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_312": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_313": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_315": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_316": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_317": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_318": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_319": "Experiment Setup\u200b",
        "segment_320": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_322": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_323": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_325": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_327": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_328": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_329": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_330": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_331": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_333": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_334": "Experiment Results\u200b",
        "segment_335": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_336": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_337": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_339": "However, the selected model changes on level 4 Algebra.",
        "segment_341": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_342": "On level 5 the result is similar.",
        "segment_344": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_345": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_346": "Analysis and Discussion\u200b",
        "segment_347": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_348": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_349": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_350": "For Further Reading\u200b",
        "segment_352": "Research paper about the tuning technique",
        "segment_353": "Documentation about inference tuning",
        "segment_355": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.",
        "segment_5": "GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, HumanEval, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?",
        "segment_6": "In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.",
        "segment_7": "Observations\u200b",
        "segment_9": "GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.",
        "segment_10": "If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.",
        "segment_12": "The obstacle of leveraging these observations is that we do not know a priori which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.",
        "segment_13": "To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:",
        "segment_14": "def vowels_count(s): \"\"\"Write a function vowels_count which takes a string representing a word as input and returns the number of vowels in the string. Vowels in this case are 'a', 'e', 'i', 'o', 'u'. Here, 'y' is also a vowel, but only when it is at the end of the given word. Example: >>> vowels_count(\"abcde\") 2 >>> vowels_count(\"ACEDY\") 3 \"\"\"",
        "segment_15": "Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.",
        "segment_16": "What else can we do? We notice that:",
        "segment_17": "It's \"easier\" to verify a given solution than finding a correct solution from scratch.",
        "segment_18": "Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.",
        "segment_19": "Solution\u200b",
        "segment_20": "Combining these observations, we can design a solution with two intuitive ideas:",
        "segment_22": "Make use of auto-generated feedback, i.e., code execution results, to filter responses.",
        "segment_23": "Try inference configurations one by one, until one response can pass the filter.",
        "segment_26": "This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.",
        "segment_27": "An implementation of this solution is provided in autogen. It uses the following sequence of configurations:",
        "segment_29": "GPT-3.5-Turbo, n=1, temperature=0",
        "segment_30": "GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_31": "GPT-4, n=1, temperature=0",
        "segment_32": "GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_33": "GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]",
        "segment_35": "Experiment Results\u200b",
        "segment_36": "The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.",
        "segment_37": "The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.",
        "segment_38": "Here are a few examples of function definitions which are solved by different configurations in the portfolio.",
        "segment_40": "Solved by GPT-3.5-Turbo, n=1, temperature=0",
        "segment_42": "def compare(game,guess): \"\"\"I think we all remember that feeling when the result of some long-awaited event is finally known. The feelings and thoughts you have at that moment are definitely worth noting down and comparing. Your task is to determine if a person correctly guessed the results of a number of matches. You are given two arrays of scores and guesses of equal length, where each index shows a match. Return an array of the same length denoting how far off each guess was. If they have guessed correctly, the value is 0, and if not, the value is the absolute difference between the guess and the score. example: compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3] compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6] \"\"\"",
        "segment_44": "Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]: the vowels_count function presented earlier.",
        "segment_45": "Solved by GPT-4, n=1, temperature=0:",
        "segment_47": "def string_xor(a: str, b: str) -> str: \"\"\" Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string. >>> string_xor('010', '110') '100' \"\"\"",
        "segment_49": "Solved by GPT-4, n=2, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_51": "def is_palindrome(string: str) -> bool: \"\"\" Test if given string is a palindrome \"\"\" return string == string[::-1]def make_palindrome(string: str) -> str: \"\"\" Find the shortest palindrome that begins with a supplied string. Algorithm idea is simple: - Find the longest postfix of supplied string that is a palindrome. - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix. >>> make_palindrome('') '' >>> make_palindrome('cat') 'catac' >>> make_palindrome('cata') 'catac' \"\"\"",
        "segment_53": "Solved by GPT-4, n=1, temperature=1, stop=[\"\\nclass\", \"\\ndef\", \"\\nif\", \"\\nprint\"]:",
        "segment_55": "def sort_array(arr): \"\"\" In this Kata, you have to sort an array of non-negative integers according to number of ones in their binary representation in ascending order. For similar number of ones, sort based on decimal value. It must be implemented like this: >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5] >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2] >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4] \"\"\"",
        "segment_56": "The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:",
        "segment_58": "Our adaptive solution has a certain degree of fault tolerance.",
        "segment_59": "The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.",
        "segment_61": "It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.",
        "segment_62": "An example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.",
        "segment_63": "Discussion\u200b",
        "segment_64": "Our solution is quite simple to implement using a generic interface offered in autogen, yet the result is quite encouraging.",
        "segment_65": "While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:",
        "segment_67": "Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.",
        "segment_68": "Consider multiple configurations to generate responses - especially useful when:",
        "segment_70": "Model and other inference parameter choice affect the utility-cost tradeoff; or",
        "segment_71": "Different configurations have complementary effect.",
        "segment_75": "A previous blog post provides evidence that these ideas are relevant in solving math problems too.",
        "segment_76": "autogen uses a technique EcoOptiGen to support inference parameter tuning and model selection.",
        "segment_77": "There are many directions of extensions in research and development:",
        "segment_79": "Generalize the way to provide feedback.",
        "segment_80": "Automate the process of optimizing the configurations.",
        "segment_81": "Build adaptive agents for different applications.",
        "segment_83": "Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.",
        "segment_84": "For Further Reading\u200b",
        "segment_86": "Documentation about autogen and Research paper.",
        "segment_87": "Blog post about a related study for math.",
        "segment_88": "Tags:LLMGPTresearchNewer PostMathChat - An Conversational Framework to Solve Math ProblemsOlder PostDoes Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATHObservationsSolutionExperiment ResultsDiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def gather_usage_summary( agents: List[Agent]) -> Tuple[Dict[str, any], Dict[str, any]]",
        "segment_2": "Gather usage summary from all agents.",
        "segment_3": "Arguments:",
        "segment_5": "agents - (list): List of agents.",
        "segment_7": "Returns:",
        "segment_10": "tuple - (total_usage_summary, actual_usage_summary)",
        "segment_11": "Example return:",
        "segment_12": "total_usage_summary = {",
        "segment_15": "'total_cost' - 0.0006090000000000001,",
        "segment_16": "'gpt-35-turbo':",
        "segment_17": "{",
        "segment_20": "'cost' - 0.0006090000000000001,",
        "segment_23": "'prompt_tokens' - 242,",
        "segment_26": "'completion_tokens' - 123,",
        "segment_29": "'total_tokens' - 365",
        "segment_30": "}",
        "segment_31": "}",
        "segment_32": "actual_usage_summary follows the same format.",
        "segment_33": "If none of the agents incurred any cost (not having a client), then the total_usage_summary and actual_usage_summary will be {'total_cost': 0}.",
        "segment_35": "Edit this pagePreviousopenai_utilsNextbrowser_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Skip to main contentFLAMLDocsSDKBlogFAQGitHub\ud83c\udf1c\ud83c\udf1ectrlKGetting StartedInstallationUse CasesExamplesAutoGen - Automated Multi Agent ChatAutoGen - Tune GPT ModelsAutoML - ClassificationAutoML - NLPAutoML - RankAutoML - RegressionAutoML - Time Series ForecastAutoML for LightGBMAutoML for XGBoostDefault - Flamlized EstimatorIntegrate - AzureMLIntegrate - Scikit-learn PipelineIntegrate - SparkTune - AzureML pipelineTune - HuggingFaceTune - Lexicographic ObjectivesTune - PyTorchContributingResearchOn this pageTune - HuggingFaceThis example uses flaml to finetune a transformer model from Huggingface transformers library.Note: flaml.AutoML has built-in support for certain finetuning tasks with a",
        "segment_2": "higher-level API.",
        "segment_3": "It may be easier to use that API unless you have special requirements not handled by that API.Requirements\u200bThis example requires GPU. Install dependencies:pip install torch transformers datasets \"flaml[blendsearch,ray]\"CopyPrepare for tuning\u200bTokenizer\u200bfrom transformers import AutoTokenizerMODEL_NAME = \"distilbert-base-uncased\"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)COLUMN_NAME = \"sentence\"def tokenize(examples): return tokenizer(examples[COLUMN_NAME], truncation=True)CopyDefine training method\u200bimport flamlimport datasetsfrom transformers import AutoModelForSequenceClassificationTASK = \"cola\"NUM_LABELS = 2def train_distilbert(config: dict): # Load CoLA dataset and apply tokenizer cola_raw = datasets.load_dataset(\"glue\", TASK) cola_encoded = cola_raw.map(tokenize, batched=True) train_dataset, eval_dataset = cola_encoded[\"train\"], cola_encoded[\"validation\"] model = AutoModelForSequenceClassification.from_pretrained( MODEL_NAME, num_labels=NUM_LABELS ) metric = datasets.load_metric(\"glue\", TASK) def compute_metrics(eval_pred): predictions, labels = eval_pred predictions = np.argmax(predictions, axis=1) return metric.compute(predictions=predictions, references=labels) training_args = TrainingArguments( output_dir='.', do_eval=False, disable_tqdm=True, logging_steps=20000, save_total_limit=0, **config, ) trainer = Trainer( model, training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer, compute_metrics=compute_metrics, ) # train model trainer.train() # evaluate model eval_output = trainer.evaluate() # report the metric to optimize & the metric to log flaml.tune.report( loss=eval_output[\"eval_loss\"], matthews_correlation=eval_output[\"eval_matthews_correlation\"], )CopyDefine the search\u200bWe are now ready to define our search. This includes:The search_space for our hyperparametersThe metric and the mode ('max' or 'min') for optimizationThe constraints (n_cpus, n_gpus, num_samples, and time_budget_s)max_num_epoch = 64search_space = { # You can mix constants with search space objects. \"num_train_epochs\": flaml.tune.loguniform(1, max_num_epoch), \"learning_rate\": flaml.tune.loguniform(1e-6, 1e-4), \"adam_epsilon\": flaml.tune.loguniform(1e-9, 1e-7), \"adam_beta1\": flaml.tune.uniform(0.8, 0.99), \"adam_beta2\": flaml.tune.loguniform(98e-2, 9999e-4),}# optimization objectiveHP_METRIC, MODE = \"matthews_correlation\", \"max\"# resourcesnum_cpus = 4num_gpus = 4 # change according to your GPU resources# constraintsnum_samples = -1 # number of trials, -1 means unlimitedtime_budget_s = 3600 # time budget in secondsCopyLaunch the tuning\u200bWe are now ready to launch the tuning using flaml.tune.run:import rayray.init(num_cpus=num_cpus, num_gpus=num_gpus)print(\"Tuning started...\")analysis = flaml.tune.run( train_distilbert, search_alg=flaml.CFO( space=search_space, metric=HP_METRIC, mode=MODE, low_cost_partial_config={\"num_train_epochs\": 1}), resources_per_trial={\"gpu\": num_gpus, \"cpu\": num_cpus}, local_dir='logs/', num_samples=num_samples, time_budget_s=time_budget_s, use_ray=True,)CopyThis will run tuning for one hour. At the end we will see a summary.== Status ==Memory usage on this node: 32.0/251.6 GiBUsing FIFO scheduling algorithm.Resources requested: 0/4 CPUs, 0/4 GPUs, 0.0/150.39 GiB heap, 0.0/47.22 GiB objects (0/1.0 accelerator_type:V100)Result logdir: /home/chiw/FLAML/notebook/logs/train_distilbert_2021-05-07_02-35-58Number of trials: 22/infinite (22 TERMINATED)Trial name status loc adam_beta1 adam_beta2 adam_epsilon learning_rate num_train_epochs iter total time (s) loss matthews_correlationtrain_distilbert_a0c303d0 TERMINATED 0.939079 0.991865 7.96945e-08 5.61152e-06 1 1 55.6909 0.587986 0train_distilbert_a0c303d1 TERMINATED 0.811036 0.997214 2.05111e-09 2.05134e-06 1.44427 1 71.7663 0.603018 0train_distilbert_c39b2ef0 TERMINATED 0.909395 0.993715 1e-07 5.26543e-06 1 1 53.7619 0.586518 0train_distilbert_f00776e2 TERMINATED 0.968763 0.990019 4.38943e-08 5.98035e-06 1.02723 1 56.8382 0.581313 0train_distilbert_11ab3900 TERMINATED 0.962198 0.991838 7.09296e-08 5.06608e-06 1 1 54.0231 0.585576 0train_distilbert_353025b6 TERMINATED 0.91596 0.991892 8.95426e-08 6.21568e-06 2.15443 1 98.3233 0.531632 0.388893train_distilbert_5728a1de TERMINATED 0.926933 0.993146 1e-07 1.00902e-05 1 1 55.3726 0.538505 0.280558train_distilbert_9394c2e2 TERMINATED 0.928106 0.990614 4.49975e-08 3.45674e-06 2.72935 1 121.388 0.539177 0.327295train_distilbert_b6543fec TERMINATED 0.876896 0.992098 1e-07 7.01176e-06 1.59538 1 76.0244 0.527516 0.379177train_distilbert_0071f998 TERMINATED 0.955024 0.991687 7.39776e-08 5.50998e-06 2.90939 1 126.871 0.516225 0.417157train_distilbert_2f830be6 TERMINATED 0.886931 0.989628 7.6127e-08 4.37646e-06 1.53338 1 73.8934 0.551629 0.0655887train_distilbert_7ce03f12 TERMINATED 0.984053 0.993956 8.70144e-08 7.82557e-06 4.08775 1 174.027 0.523732 0.453549train_distilbert_aaab0508 TERMINATED 0.940707 0.993946 1e-07 8.91979e-06 3.40243 1 146.249 0.511288 0.45085train_distilbert_14262454 TERMINATED 0.99 0.991696 4.60093e-08 4.83405e-06 3.4954 1 152.008 0.53506 0.400851train_distilbert_6d211fe6 TERMINATED 0.959277 0.994556 5.40791e-08 1.17333e-05 6.64995 1 271.444 0.609851 0.526802train_distilbert_c980bae4 TERMINATED 0.99 0.993355 1e-07 5.21929e-06 2.51275 1 111.799 0.542276 0.324968train_distilbert_6d0d29d6 TERMINATED 0.965773 0.995182 9.9752e-08 1.15549e-05 13.694 1 527.944 0.923802 0.549474train_distilbert_b16ea82a TERMINATED 0.952781 0.993931 2.93182e-08 1.19145e-05 3.2293 1 139.844 0.533466 0.451307train_distilbert_eddf7cc0 TERMINATED 0.99 0.997109 8.13498e-08 1.28515e-05 15.5807 1 614.789 0.983285 0.56993train_distilbert_43008974 TERMINATED 0.929089 0.993258 1e-07 1.03892e-05 12.0357 1 474.387 0.857461 0.520022train_distilbert_b3408a4e TERMINATED 0.99 0.993809 4.67441e-08 1.10418e-05 11.9165 1 474.126 0.828205 0.526164train_distilbert_cfbfb220 TERMINATED 0.979454 0.9999 1e-07 1.49578e-05 20.3715CopyRetrieve the results\u200bbest_trial = analysis.get_best_trial(HP_METRIC, MODE, \"all\")metric = best_trial.metric_analysis[HP_METRIC][MODE]print(f\"n_trials={len(analysis.trials)}\")print(f\"time={time.time()-start_time}\")print(f\"Best model eval {HP_METRIC}: {metric:.4f}\")print(f\"Best model parameters: {best_trial.config}\")# n_trials=22# time=3999.769361972809# Best model eval matthews_correlation: 0.5699# Best model parameters: {'num_train_epochs': 15.580684188655825, 'learning_rate': 1.2851507818900338e-05, 'adam_epsilon': 8.134982521948352e-08, 'adam_beta1': 0.99, 'adam_beta2': 0.9971094424784387}CopyLink to notebook | Open in colabEdit this pagePrevious\u00ab Tune - AzureML pipelineNextTune - Lexicographic Objectives \u00bbRequirementsPrepare for tuningDefine the searchLaunch the tuningRetrieve the resultsCommunityDiscordCopyright \u00a9 2023 FLAML Authors. Built with Docusaurus."
    },
    {
        "segment_1": "class GPTAssistantAgent(ConversableAgent)",
        "segment_2": "An experimental AutoGen agent class that leverages the OpenAI Assistant API for conversational capabilities.",
        "segment_3": "This agent is unique in its reliance on the OpenAI Assistant for state management, differing from other agents like ConversableAgent.",
        "segment_4": "__init__\u200b",
        "segment_5": "def __init__(name=\"GPT Assistant\", instructions: Optional[str] = None, llm_config: Optional[Union[Dict, bool]] = None, overwrite_instructions: bool = False, overwrite_tools: bool = False, **kwargs)",
        "segment_6": "Arguments:",
        "segment_8": "name str - name of the agent. It will be used to find the existing assistant by name. Please remember to delete an old assistant with the same name if you intend to create a new assistant with the same name.",
        "segment_9": "instructions str - instructions for the OpenAI assistant configuration.",
        "segment_10": "When instructions is not None, the system message of the agent will be",
        "segment_11": "set to the provided instructions and used in the assistant run, irrespective",
        "segment_12": "of the overwrite_instructions flag. But when instructions is None,",
        "segment_13": "and the assistant does not exist, the system message will be set to",
        "segment_14": "AssistantAgent.DEFAULT_SYSTEM_MESSAGE. If the assistant exists, the",
        "segment_15": "system message will be set to the existing assistant instructions.",
        "segment_16": "llm_config dict or False - llm inference configuration.",
        "segment_18": "assistant_id: ID of the assistant to use. If None, a new assistant will be created.",
        "segment_19": "model: Model to use for the assistant (gpt-4-1106-preview, gpt-3.5-turbo-1106).",
        "segment_20": "check_every_ms: check thread run status interval",
        "segment_21": "tools: Give Assistants access to OpenAI-hosted tools like Code Interpreter and Knowledge Retrieval,",
        "segment_22": "or build your own tools using Function calling. ref https://platform.openai.com/docs/assistants/tools",
        "segment_23": "file_ids: files used by retrieval in run",
        "segment_26": "overwrite_instructions bool - whether to overwrite the instructions of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.",
        "segment_27": "overwrite_tools bool - whether to overwrite the tools of an existing assistant. This parameter is in effect only when assistant_id is specified in llm_config.",
        "segment_28": "kwargs dict - Additional configuration options for the agent.",
        "segment_30": "verbose (bool): If set to True, enables more detailed output from the assistant thread.",
        "segment_31": "Other kwargs: Except verbose, others are passed directly to ConversableAgent.",
        "segment_35": "can_execute_function\u200b",
        "segment_36": "def can_execute_function(name: str) -> bool",
        "segment_37": "Whether the agent can execute the function.",
        "segment_38": "reset\u200b",
        "segment_39": "def reset()",
        "segment_40": "Resets the agent, clearing any existing conversation thread and unread message indices.",
        "segment_41": "clear_history\u200b",
        "segment_42": "def clear_history(agent: Optional[Agent] = None)",
        "segment_43": "Clear the chat history of the agent.",
        "segment_44": "Arguments:",
        "segment_46": "agent - the agent with whom the chat history to clear. If None, clear the chat history with all agents.",
        "segment_48": "pretty_print_thread\u200b",
        "segment_49": "def pretty_print_thread(thread)",
        "segment_50": "Pretty print the thread.",
        "segment_51": "oai_threads\u200b",
        "segment_52": "@propertydef oai_threads() -> Dict[Agent, Any]",
        "segment_53": "Return the threads of the agent.",
        "segment_54": "assistant_id\u200b",
        "segment_55": "@propertydef assistant_id()",
        "segment_56": "Return the assistant id",
        "segment_57": "get_assistant_instructions\u200b",
        "segment_58": "def get_assistant_instructions()",
        "segment_59": "Return the assistant instructions from OAI assistant API",
        "segment_60": "delete_assistant\u200b",
        "segment_61": "def delete_assistant()",
        "segment_62": "Delete the assistant from OAI assistant API",
        "segment_63": "find_matching_assistant\u200b",
        "segment_64": "def find_matching_assistant(candidate_assistants, instructions, tools, file_ids)",
        "segment_65": "Find the matching assistant from a list of candidate assistants.",
        "segment_66": "Filter out candidates with the same name but different instructions, file IDs, and function names.",
        "segment_67": "TODO: implement accurate match based on assistant metadata fields."
    },
    {
        "segment_1": "class AbstractCache(ABC)",
        "segment_2": "Abstract base class for cache implementations.",
        "segment_3": "This class defines the basic interface for cache operations.",
        "segment_4": "Implementing classes should provide concrete implementations for",
        "segment_5": "these methods to handle caching mechanisms.",
        "segment_6": "get\u200b",
        "segment_7": "@abstractmethoddef get(key, default=None)",
        "segment_8": "Retrieve an item from the cache.",
        "segment_9": "Abstract method that must be implemented by subclasses to",
        "segment_10": "retrieve an item from the cache.",
        "segment_11": "Arguments:",
        "segment_13": "key str - The key identifying the item in the cache.",
        "segment_14": "default optional - The default value to return if the key is not found.",
        "segment_15": "Defaults to None.",
        "segment_17": "Returns:",
        "segment_18": "The value associated with the key if found, else the default value.",
        "segment_19": "Raises:",
        "segment_21": "NotImplementedError - If the subclass does not implement this method.",
        "segment_23": "set\u200b",
        "segment_24": "@abstractmethoddef set(key, value)",
        "segment_25": "Set an item in the cache.",
        "segment_26": "Abstract method that must be implemented by subclasses to",
        "segment_27": "store an item in the cache.",
        "segment_28": "Arguments:",
        "segment_30": "key str - The key under which the item is to be stored.",
        "segment_31": "value - The value to be stored in the cache.",
        "segment_33": "Raises:",
        "segment_35": "NotImplementedError - If the subclass does not implement this method.",
        "segment_37": "close\u200b",
        "segment_38": "@abstractmethoddef close()",
        "segment_39": "Close the cache.",
        "segment_40": "Abstract method that should be implemented by subclasses to",
        "segment_41": "perform any necessary cleanup, such as closing network connections or",
        "segment_42": "releasing resources.",
        "segment_43": "Raises:",
        "segment_45": "NotImplementedError - If the subclass does not implement this method.",
        "segment_47": "__enter__\u200b",
        "segment_48": "@abstractmethoddef __enter__()",
        "segment_49": "Enter the runtime context related to this object.",
        "segment_50": "The with statement will bind this method\u2019s return value to the target(s)",
        "segment_51": "specified in the as clause of the statement, if any.",
        "segment_52": "Raises:",
        "segment_54": "NotImplementedError - If the subclass does not implement this method.",
        "segment_56": "__exit__\u200b",
        "segment_57": "@abstractmethoddef __exit__(exc_type, exc_value, traceback)",
        "segment_58": "Exit the runtime context and close the cache.",
        "segment_59": "Abstract method that should be implemented by subclasses to handle",
        "segment_60": "the exit from a with statement. It is responsible for resource",
        "segment_61": "release and cleanup.",
        "segment_62": "Arguments:",
        "segment_64": "exc_type - The exception type if an exception was raised in the context.",
        "segment_65": "exc_value - The exception value if an exception was raised in the context.",
        "segment_66": "traceback - The traceback if an exception was raised in the context.",
        "segment_68": "Raises:",
        "segment_70": "NotImplementedError - If the subclass does not implement this method.",
        "segment_71": "Edit this pagePrevioususer_proxy_agentNextcacheAbstractCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class NoEligibleSpeakerException(Exception)",
        "segment_2": "Exception raised for early termination of a GroupChat.",
        "segment_3": "GroupChat Objects\u200b",
        "segment_4": "@dataclassclass GroupChat()",
        "segment_5": "(In preview) A group chat class that contains the following data fields:",
        "segment_8": "agents: a list of participating agents.",
        "segment_11": "messages: a list of messages in the group chat.",
        "segment_14": "max_round: the maximum number of rounds.",
        "segment_17": "admin_name: the name of the admin agent if there is one. Default is \"Admin\".",
        "segment_18": "KeyBoardInterrupt will make the admin agent take over.",
        "segment_21": "func_call_filter: whether to enforce function call filter. Default is True.",
        "segment_22": "When set to True and when a message is a function call suggestion,",
        "segment_23": "the next speaker will be chosen from an agent which contains the corresponding function name",
        "segment_24": "in its function_map.",
        "segment_27": "speaker_selection_method: the method for selecting the next speaker. Default is \"auto\".",
        "segment_28": "Could be any of the following (case insensitive), will raise ValueError if not recognized:",
        "segment_30": "\"auto\": the next speaker is selected automatically by LLM.",
        "segment_31": "\"manual\": the next speaker is selected manually by user input.",
        "segment_32": "\"random\": the next speaker is selected randomly.",
        "segment_33": "\"round_robin\": the next speaker is selected in a round robin fashion, i.e., iterating in the same order as provided in agents.",
        "segment_37": "allow_repeat_speaker: whether to allow the same speaker to speak consecutively. Default is True, in which case all speakers are allowed to speak consecutively. If allow_repeat_speaker is a list of Agents, then only those listed agents are allowed to repeat. If set to False, then no speakers are allowed to repeat. allow_repeat_speaker and allowed_or_disallowed_speaker_transitions are mutually exclusive.",
        "segment_40": "allowed_or_disallowed_speaker_transitions: a dictionary of keys and list as values. The keys are the source agents, and the values are the agents that the key agent can transition to. Default is None, in which case a fully connected allowed_speaker_transitions_dict is assumed. allow_repeat_speaker and allowed_or_disallowed_speaker_transitions are mutually exclusive.",
        "segment_43": "speaker_transitions_type: whether the speaker_transitions_type is a dictionary containing lists of allowed agents or disallowed agents. allowed means the allowed_or_disallowed_speaker_transitions is a dictionary containing lists of allowed agents. If set to disallowed, then the allowed_or_disallowed_speaker_transitions is a dictionary containing lists of disallowed agents. Must be supplied if allowed_or_disallowed_speaker_transitions is not None.",
        "segment_46": "enable_clear_history: enable possibility to clear history of messages for agents manually by providing",
        "segment_47": "\"clear history\" phrase in user prompt. This is experimental feature.",
        "segment_48": "See description of GroupChatManager.clear_agents_history function for more info.",
        "segment_51": "allow_repeat_speaker\u200b",
        "segment_52": "It would be set to True if allowed_or_disallowed_speaker_transitions is None",
        "segment_53": "agent_names\u200b",
        "segment_54": "@propertydef agent_names() -> List[str]",
        "segment_55": "Return the names of the agents in the group chat.",
        "segment_56": "reset\u200b",
        "segment_57": "def reset()",
        "segment_58": "Reset the group chat.",
        "segment_59": "append\u200b",
        "segment_60": "def append(message: Dict, speaker: Agent)",
        "segment_61": "Append a message to the group chat.",
        "segment_62": "We cast the content to str here so that it can be managed by text-based",
        "segment_63": "model.",
        "segment_64": "agent_by_name\u200b",
        "segment_65": "def agent_by_name(name: str) -> Agent",
        "segment_66": "Returns the agent with a given name.",
        "segment_67": "next_agent\u200b",
        "segment_68": "def next_agent(agent: Agent, agents: Optional[List[Agent]] = None) -> Agent",
        "segment_69": "Return the next agent in the list.",
        "segment_70": "select_speaker_msg\u200b",
        "segment_71": "def select_speaker_msg(agents: Optional[List[Agent]] = None) -> str",
        "segment_72": "Return the system message for selecting the next speaker. This is always the first message in the context.",
        "segment_73": "select_speaker_prompt\u200b",
        "segment_74": "def select_speaker_prompt(agents: Optional[List[Agent]] = None) -> str",
        "segment_75": "Return the floating system prompt selecting the next speaker. This is always the last message in the context.",
        "segment_76": "manual_select_speaker\u200b",
        "segment_77": "def manual_select_speaker( agents: Optional[List[Agent]] = None) -> Union[Agent, None]",
        "segment_78": "Manually select the next speaker.",
        "segment_79": "random_select_speaker\u200b",
        "segment_80": "def random_select_speaker( agents: Optional[List[Agent]] = None) -> Union[Agent, None]",
        "segment_81": "Randomly select the next speaker.",
        "segment_82": "select_speaker\u200b",
        "segment_83": "def select_speaker(last_speaker: Agent, selector: ConversableAgent) -> Agent",
        "segment_84": "Select the next speaker.",
        "segment_85": "a_select_speaker\u200b",
        "segment_86": "async def a_select_speaker(last_speaker: Agent, selector: ConversableAgent) -> Agent",
        "segment_87": "Select the next speaker.",
        "segment_88": "GroupChatManager Objects\u200b",
        "segment_89": "class GroupChatManager(ConversableAgent)",
        "segment_90": "(In preview) A chat manager agent that can manage a group chat of multiple agents.",
        "segment_91": "run_chat\u200b",
        "segment_92": "def run_chat(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[GroupChat] = None) -> Tuple[bool, Optional[str]]",
        "segment_93": "Run a group chat.",
        "segment_94": "a_run_chat\u200b",
        "segment_95": "async def a_run_chat(messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[GroupChat] = None)",
        "segment_96": "Run a group chat asynchronously.",
        "segment_97": "clear_agents_history\u200b",
        "segment_98": "def clear_agents_history(reply: str, groupchat: GroupChat) -> str",
        "segment_99": "Clears history of messages for all agents or selected one. Can preserve selected number of last messages.",
        "segment_100": "That function is called when user manually provide \"clear history\" phrase in his reply.",
        "segment_101": "When \"clear history\" is provided, the history of messages for all agents is cleared.",
        "segment_102": "When \"clear history \" is provided, the history of messages for selected agent is cleared.",
        "segment_103": "When \"clear history \" is provided, the history of messages for all agents is cleared",
        "segment_104": "except last messages.",
        "segment_105": "When \"clear history \" is provided, the history of messages for selected",
        "segment_106": "agent is cleared except last messages.",
        "segment_107": "Phrase \"clear history\" and optional arguments are cut out from the reply before it passed to the chat.",
        "segment_108": "Arguments:",
        "segment_110": "reply str - Admin reply to analyse.",
        "segment_111": "groupchat GroupChat - GroupChat object."
    },
    {
        "segment_1": "class RetrieveAssistantAgent(AssistantAgent)",
        "segment_2": "(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.",
        "segment_3": "RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.",
        "segment_4": "The default system message is designed to solve a task with LLM,",
        "segment_5": "including suggesting python code blocks and debugging.",
        "segment_6": "human_input_mode is default to \"NEVER\"",
        "segment_7": "and code_execution_config is default to False.",
        "segment_8": "This agent doesn't execute code by default, and expects the user to execute the code.Edit this pagePreviousqdrant_retrieve_user_proxy_agentNextretrieve_user_proxy_agentRetrieveAssistantAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class RedisCache(AbstractCache)",
        "segment_2": "Implementation of AbstractCache using the Redis database.",
        "segment_3": "This class provides a concrete implementation of the AbstractCache",
        "segment_4": "interface using the Redis database for caching data.",
        "segment_5": "Attributes:",
        "segment_7": "seed str - A seed or namespace used as a prefix for cache keys.",
        "segment_8": "cache redis.Redis - The Redis client used for caching.",
        "segment_10": "Methods:",
        "segment_11": "init(self, seed, redis_url): Initializes the RedisCache with the given seed and Redis URL.",
        "segment_12": "_prefixed_key(self, key): Internal method to get a namespaced cache key.",
        "segment_13": "get(self, key, default=None): Retrieves an item from the cache.",
        "segment_14": "set(self, key, value): Sets an item in the cache.",
        "segment_16": "close(self) - Closes the Redis client.",
        "segment_17": "__enter__(self) - Context management entry.",
        "segment_18": "exit(self, exc_type, exc_value, traceback): Context management exit.",
        "segment_20": "__init__\u200b",
        "segment_21": "def __init__(seed, redis_url)",
        "segment_22": "Initialize the RedisCache instance.",
        "segment_23": "Arguments:",
        "segment_25": "seed str - A seed or namespace for the cache. This is used as a prefix for all cache keys.",
        "segment_26": "redis_url str - The URL for the Redis server.",
        "segment_28": "get\u200b",
        "segment_29": "def get(key, default=None)",
        "segment_30": "Retrieve an item from the Redis cache.",
        "segment_31": "Arguments:",
        "segment_33": "key str - The key identifying the item in the cache.",
        "segment_34": "default optional - The default value to return if the key is not found.",
        "segment_35": "Defaults to None.",
        "segment_37": "Returns:",
        "segment_38": "The deserialized value associated with the key if found, else the default value.",
        "segment_39": "set\u200b",
        "segment_40": "def set(key, value)",
        "segment_41": "Set an item in the Redis cache.",
        "segment_42": "Arguments:",
        "segment_44": "key str - The key under which the item is to be stored.",
        "segment_45": "value - The value to be stored in the cache.",
        "segment_47": "Notes:",
        "segment_48": "The value is serialized using pickle before being stored in Redis.",
        "segment_49": "close\u200b",
        "segment_50": "def close()",
        "segment_51": "Close the Redis client.",
        "segment_52": "Perform any necessary cleanup, such as closing network connections.",
        "segment_53": "__enter__\u200b",
        "segment_54": "def __enter__()",
        "segment_55": "Enter the runtime context related to the object.",
        "segment_56": "Returns:",
        "segment_58": "self - The instance itself.",
        "segment_60": "__exit__\u200b",
        "segment_61": "def __exit__(exc_type, exc_value, traceback)",
        "segment_62": "Exit the runtime context related to the object.",
        "segment_63": "Perform cleanup actions such as closing the Redis client.",
        "segment_64": "Arguments:",
        "segment_66": "exc_type - The exception type if an exception was raised in the context.",
        "segment_67": "exc_value - The exception value if an exception was raised in the context.",
        "segment_68": "traceback - The traceback if an exception was raised in the context.",
        "segment_69": "Edit this pagePreviousdisk_cacheNextclientRedisCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXNewer PostAgentOptimizer - An Agentic Way to Train Your LLM AgentOlder PostAgent AutoBuild - Automatically Building Multi-agent SystemsTLDRIntroductionGetting Started with AutoGen StudioConfiguring an LLM ProviderInstallationRunning the ApplicationWhat Can You Do with AutoGen Studio?BuildPlaygroundGalleryThe AutoGen Studio APIRoad Map and Next StepsContribution GuideFAQCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen offers a unified multi-agent conversation framework as a high-level abstraction of using foundation models. It features capable, customizable and conversable agents which integrate LLMs, tools, and humans via automated agent chat.",
        "segment_2": "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.",
        "segment_3": "This framework simplifies the orchestration, automation and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcome their weaknesses. It enables building next-gen LLM applications based on multi-agent conversations with minimal effort.",
        "segment_4": "Agents\u200b",
        "segment_5": "AutoGen abstracts and implements conversable agents",
        "segment_6": "designed to solve tasks through inter-agent conversations. Specifically, the agents in AutoGen have the following notable features:",
        "segment_9": "Conversable: Agents in AutoGen are conversable, which means that any agent can send",
        "segment_10": "and receive messages from other agents to initiate or continue a conversation",
        "segment_13": "Customizable: Agents in AutoGen can be customized to integrate LLMs, humans, tools, or a combination of them.",
        "segment_16": "The figure below shows the built-in agents in AutoGen.",
        "segment_18": "We have designed a generic ConversableAgent",
        "segment_19": "class for Agents that are capable of conversing with each other through the exchange of messages to jointly finish a task. An agent can communicate with other agents and perform actions. Different agents can differ in what actions they perform after receiving messages. Two representative subclasses are AssistantAgent and UserProxyAgent",
        "segment_22": "The AssistantAgent is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM inference configuration can be configured via [llm_config].",
        "segment_25": "The UserProxyAgent is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions or tools. The UserProxyAgent triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the code_execution_config parameter to False. LLM-based response is disabled by default. It can be enabled by setting llm_config to a dict corresponding to the inference configuration. When llm_config is set as a dictionary, UserProxyAgent can generate replies using an LLM when code execution is not performed.",
        "segment_28": "The auto-reply capability of ConversableAgent allows for more autonomous multi-agent communication while retaining the possibility of human intervention.",
        "segment_29": "One can also easily extend it by registering reply functions with the register_reply() method.",
        "segment_30": "In the following code, we create an AssistantAgent named \"assistant\" to serve as the assistant and a UserProxyAgent named \"user_proxy\" to serve as a proxy for the human user. We will later employ these two agents to solve a task.",
        "segment_31": "from autogen import AssistantAgent, UserProxyAgent# create an AssistantAgent instance named \"assistant\"assistant = AssistantAgent(name=\"assistant\")# create a UserProxyAgent instance named \"user_proxy\"user_proxy = UserProxyAgent(name=\"user_proxy\")",
        "segment_32": "Tool calling\u200b",
        "segment_33": "Tool calling enables agents to interact with external tools and APIs more efficiently.",
        "segment_34": "This feature allows the AI model to intelligently choose to output a JSON object containing",
        "segment_35": "arguments to call specific tools based on the user's input. A tool to be called is",
        "segment_36": "specified with a JSON schema describing its parameters and their types. Writing such JSON schema",
        "segment_37": "is complex and error-prone and that is why AutoGen framework provides two high level function decorators for automatically generating such schema using type hints on standard Python datatypes",
        "segment_38": "or Pydantic models:",
        "segment_41": "ConversableAgent.register_for_llm is used to register the function as a Tool in the llm_config of a ConversableAgent. The ConversableAgent agent can propose execution of a registered Tool, but the actual execution will be performed by a UserProxy agent.",
        "segment_44": "ConversableAgent.register_for_execution is used to register the function in the function_map of a UserProxy agent.",
        "segment_47": "The following examples illustrates the process of registering a custom function for currency exchange calculation that uses type hints and standard Python datatypes:",
        "segment_49": "First, we import necessary libraries and configure models using autogen.config_list_from_json function:",
        "segment_51": "from typing import Literalfrom pydantic import BaseModel, Fieldfrom typing_extensions import Annotatedimport autogenconfig_list = autogen.config_list_from_json( \"OAI_CONFIG_LIST\", filter_dict={ \"model\": [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"], },)",
        "segment_53": "We create an assistant agent and user proxy. The assistant will be responsible for suggesting which functions to call and the user proxy for the actual execution of a proposed function:",
        "segment_55": "llm_config = { \"config_list\": config_list, \"timeout\": 120,}chatbot = autogen.AssistantAgent( name=\"chatbot\", system_message=\"For currency exchange tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\", llm_config=llm_config,)# create a UserProxyAgent instance named \"user_proxy\"user_proxy = autogen.UserProxyAgent( name=\"user_proxy\", is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), human_input_mode=\"NEVER\", max_consecutive_auto_reply=10,)",
        "segment_57": "We define the function currency_calculator below as follows and decorate it with two decorators:",
        "segment_59": "@user_proxy.register_for_execution() adding the function currency_calculator to user_proxy.function_map, and",
        "segment_60": "@chatbot.register_for_llm adding a generated JSON schema of the function to llm_config of chatbot.",
        "segment_64": "CurrencySymbol = Literal[\"USD\", \"EUR\"]def exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float: if base_currency == quote_currency: return 1.0 elif base_currency == \"USD\" and quote_currency == \"EUR\": return 1 / 1.1 elif base_currency == \"EUR\" and quote_currency == \"USD\": return 1.1 else: raise ValueError(f\"Unknown currencies {base_currency}, {quote_currency}\")# NOTE: for Azure OpenAI, please use API version 2023-12-01-preview or later as# support for earlier versions will be deprecated.# For API versions 2023-10-01-preview or earlier you may# need to set `api_style=\"function\"` in the decorator if the default value does not work:# `register_for_llm(description=..., api_style=\"function\")`.@user_proxy.register_for_execution()@chatbot.register_for_llm(description=\"Currency exchange calculator.\")def currency_calculator( base_amount: Annotated[float, \"Amount of currency in base_currency\"], base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\", quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",) -> str: quote_amount = exchange_rate(base_currency, quote_currency) * base_amount return f\"{quote_amount} {quote_currency}\"",
        "segment_65": "Notice the use of Annotated to specify the type and the description of each parameter. The return value of the function must be either string or serializable to string using the json.dumps() or Pydantic model dump to JSON (both version 1.x and 2.x are supported).",
        "segment_66": "You can check the JSON schema generated by the decorator chatbot.llm_config[\"tools\"]:",
        "segment_67": "[{'type': 'function', 'function': {'description': 'Currency exchange calculator.', 'name': 'currency_calculator', 'parameters': {'type': 'object', 'properties': {'base_amount': {'type': 'number', 'description': 'Amount of currency in base_currency'}, 'base_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'USD', 'description': 'Base currency'}, 'quote_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'EUR', 'description': 'Quote currency'}}, 'required': ['base_amount']}}}]",
        "segment_68": "Python decorators are functions themselves. If you do not want to use the",
        "segment_69": "@chatbot.register... decorator syntax,",
        "segment_70": "you can call the decorators as functions:",
        "segment_71": "# Register the function with the chatbot's llm_config.currency_calculator = chatbot.register_for_llm(description=\"Currency exchange calculator.\")(currency_calculator)# Register the function with the user_proxy's function_map.user_proxy.register_for_execution()(currency_calculator)",
        "segment_72": "Alternatevely, you can also use autogen.agentchat.register_function() instead as follows:",
        "segment_73": "def currency_calculator( base_amount: Annotated[float, \"Amount of currency in base_currency\"], base_currency: Annotated[CurrencySymbol, \"Base currency\"] = \"USD\", quote_currency: Annotated[CurrencySymbol, \"Quote currency\"] = \"EUR\",) -> str: quote_amount = exchange_rate(base_currency, quote_currency) * base_amount return f\"{quote_amount} {quote_currency}\"autogen.agentchat.register_function( currency_calculator, agent=chatbot, executor=user_proxy, description=\"Currency exchange calculator.\",)",
        "segment_75": "Agents can now use the function as follows:",
        "segment_77": "user_proxy.initiate_chat( chatbot, message=\"How much is 123.45 USD in EUR?\",)",
        "segment_78": "Output:",
        "segment_79": "user_proxy (to chatbot):How much is 123.45 USD in EUR?--------------------------------------------------------------------------------chatbot (to user_proxy):***** Suggested tool Call: currency_calculator *****Arguments:{\"base_amount\":123.45,\"base_currency\":\"USD\",\"quote_currency\":\"EUR\"}********************************************************-------------------------------------------------------------------------------->>>>>>>> EXECUTING FUNCTION currency_calculator...user_proxy (to chatbot):***** Response from calling function \"currency_calculator\" *****112.22727272727272 EUR****************************************************************--------------------------------------------------------------------------------chatbot (to user_proxy):123.45 USD is equivalent to approximately 112.23 EUR....TERMINATE",
        "segment_80": "Use of Pydantic models further simplifies writing of such functions. Pydantic models can be used",
        "segment_81": "for both the parameters of a function and for its return type. Parameters of such functions will",
        "segment_82": "be constructed from JSON provided by an AI model, while the output will be serialized as JSON",
        "segment_83": "encoded string automatically.",
        "segment_84": "The following example shows how we could rewrite our currency exchange calculator example:",
        "segment_85": "# defines a Pydantic modelclass Currency(BaseModel): # parameter of type CurrencySymbol currency: Annotated[CurrencySymbol, Field(..., description=\"Currency symbol\")] # parameter of type float, must be greater or equal to 0 with default value 0 amount: Annotated[float, Field(0, description=\"Amount of currency\", ge=0)]def currency_calculator( base: Annotated[Currency, \"Base currency: amount and currency symbol\"], quote_currency: Annotated[CurrencySymbol, \"Quote currency symbol\"] = \"USD\",) -> Currency: quote_amount = exchange_rate(base.currency, quote_currency) * base.amount return Currency(amount=quote_amount, currency=quote_currency)autogen.agentchat.register_function( currency_calculator, agent=chatbot, executor=user_proxy, description=\"Currency exchange calculator.\",)",
        "segment_86": "The generated JSON schema has additional properties such as minimum value encoded:",
        "segment_87": "[{'type': 'function', 'function': {'description': 'Currency exchange calculator.', 'name': 'currency_calculator', 'parameters': {'type': 'object', 'properties': {'base': {'properties': {'currency': {'description': 'Currency symbol', 'enum': ['USD', 'EUR'], 'title': 'Currency', 'type': 'string'}, 'amount': {'default': 0, 'description': 'Amount of currency', 'minimum': 0.0, 'title': 'Amount', 'type': 'number'}}, 'required': ['currency'], 'title': 'Currency', 'type': 'object', 'description': 'Base currency: amount and currency symbol'}, 'quote_currency': {'enum': ['USD', 'EUR'], 'type': 'string', 'default': 'USD', 'description': 'Quote currency symbol'}}, 'required': ['base']}}}]",
        "segment_88": "For more in-depth examples, please check the following:",
        "segment_91": "Currency calculator examples - View Notebook",
        "segment_94": "Use Provided Tools as Functions - View Notebook",
        "segment_97": "Use Tools via Sync and Async Function Calling - View Notebook",
        "segment_100": "Multi-agent Conversations\u200b",
        "segment_101": "A Basic Two-Agent Conversation Example\u200b",
        "segment_102": "Once the participating agents are constructed properly, one can start a multi-agent conversation session by an initialization step as shown in the following code:",
        "segment_103": "# the assistant receives a message from the user, which contains the task descriptionuser_proxy.initiate_chat( assistant, message=\"\"\"What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?\"\"\",)",
        "segment_104": "After the initialization step, the conversation could proceed automatically. Find a visual illustration of how the user_proxy and assistant collaboratively solve the above task autonomously below:",
        "segment_107": "The assistant receives a message from the user_proxy, which contains the task description.",
        "segment_108": "The assistant then tries to write Python code to solve the task and sends the response to the user_proxy.",
        "segment_109": "Once the user_proxy receives a response from the assistant, it tries to reply by either soliciting human input or preparing an automatically generated reply. If no human input is provided, the user_proxy executes the code and uses the result as the auto-reply.",
        "segment_110": "The assistant then generates a further response for the user_proxy. The user_proxy can then decide whether to terminate the conversation. If not, steps 3 and 4 are repeated.",
        "segment_112": "Supporting Diverse Conversation Patterns\u200b",
        "segment_113": "Conversations with different levels of autonomy, and human-involvement patterns\u200b",
        "segment_114": "On the one hand, one can achieve fully autonomous conversations after an initialization step. On the other hand, AutoGen can be used to implement human-in-the-loop problem-solving by configuring human involvement levels and patterns (e.g., setting the human_input_mode to ALWAYS), as human involvement is expected and/or desired in many applications.",
        "segment_115": "Static and dynamic conversations\u200b",
        "segment_116": "AutoGen, by integrating conversation-driven control utilizing both programming and natural language, inherently supports dynamic conversations. This dynamic nature allows the agent topology to adapt based on the actual conversation flow under varying input problem scenarios. Conversely, static conversations adhere to a predefined topology. Dynamic conversations are particularly beneficial in complex settings where interaction patterns cannot be predetermined.",
        "segment_118": "Registered auto-reply",
        "segment_119": "With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. For example:",
        "segment_122": "Hierarchical chat like in OptiGuide.",
        "segment_123": "Dynamic Group Chat which is a special form of hierarchical chat. In the system, we register a reply function in the group chat manager, which broadcasts messages and decides who the next speaker will be in a group chat setting.",
        "segment_124": "Finite state machine (FSM) based group chat which is a special form of dynamic group chat. In this approach, a directed transition matrix is fed into group chat. Users can specify legal transitions or specify disallowed transitions.",
        "segment_125": "Nested chat like in conversational chess.",
        "segment_128": "LLM-Based Function Call",
        "segment_129": "Another approach involves LLM-based function calls, where LLM decides if a specific function should be invoked based on the conversation's status during each inference. This approach enables dynamic multi-agent conversations, as seen in scenarios like multi-user math problem solving scenario, where a student assistant automatically seeks expertise via function calls.",
        "segment_131": "LLM Caching\u200b",
        "segment_132": "Since version 0.2.8, a configurable context manager allows you to easily",
        "segment_133": "configure LLM cache, using either DiskCache or Redis. All agents inside the",
        "segment_134": "context manager will use the same cache.",
        "segment_135": "from autogen import Cache# Use Redis as cachewith Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)# Use DiskCache as cachewith Cache.disk() as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_136": "You can vary the cache_seed parameter to get different LLM output while",
        "segment_137": "still using cache.",
        "segment_138": "# Setting the cache_seed to 1 will use a different cache from the default one# and you will see different output.with Cache.disk(cache_seed=1) as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_139": "By default DiskCache uses .cache for storage. To change the cache directory,",
        "segment_140": "set cache_path_root:",
        "segment_141": "with Cache.disk(cache_path_root=\"/tmp/autogen_cache\") as cache: user.initiate_chat(assistant, message=coding_task, cache=cache)",
        "segment_142": "For backward compatibility, DiskCache is on by default with cache_seed set to 41.",
        "segment_143": "To disable caching completely, set cache_seed to None in the llm_config of the agent.",
        "segment_144": "assistant = AssistantAgent( \"coding_agent\", llm_config={ \"cache_seed\": None, \"config_list\": OAI_CONFIG_LIST, \"max_tokens\": 1024, },)",
        "segment_145": "Diverse Applications Implemented with AutoGen\u200b",
        "segment_146": "The figure below shows six examples of applications built using AutoGen.",
        "segment_148": "Find a list of examples in this page: Automated Agent Chat Examples",
        "segment_149": "For Further Reading\u200b",
        "segment_150": "Interested in the research that leads to this package? Please check the following papers.",
        "segment_153": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023.",
        "segment_156": "An Empirical Study on Challenging Math Problem Solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).",
        "segment_158": "Edit this pagePreviousLLM Endpoint ConfigurationNextEnhanced InferenceAgentsMulti-agent ConversationsA Basic Two-Agent Conversation ExampleSupporting Diverse Conversation PatternsLLM CachingDiverse Applications Implemented with AutoGenFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "autogen.OpenAIWrapper provides enhanced LLM inference for openai>=1.",
        "segment_2": "autogen.Completion is a drop-in replacement of openai.Completion and openai.ChatCompletion for enhanced LLM inference using openai<1.",
        "segment_3": "There are a number of benefits of using autogen to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on.",
        "segment_4": "Tune Inference Parameters (for openai<1)\u200b",
        "segment_5": "Find a list of examples in this page: Tune Inference Parameters Examples",
        "segment_6": "Choices to optimize\u200b",
        "segment_7": "The cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined. From the perspective of an application builder using foundation models, the use case is to maximize the utility of the generated text under an inference budget constraint (e.g., measured by the average dollar cost needed to solve a coding problem). This can be achieved by optimizing the hyperparameters of the inference,",
        "segment_8": "which can significantly affect both the utility and the cost of the generated text.",
        "segment_9": "The tunable hyperparameters include:",
        "segment_11": "model - this is a required input, specifying the model ID to use.",
        "segment_12": "prompt/messages - the input prompt/messages to the model, which provides the context for the text generation task.",
        "segment_13": "max_tokens - the maximum number of tokens (words or word pieces) to generate in the output.",
        "segment_14": "temperature - a value between 0 and 1 that controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable text.",
        "segment_15": "top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation. A lower top_p value will make it more likely to generate text based on the most likely tokens, while a higher value will allow the model to explore a wider range of possible tokens.",
        "segment_16": "n - the number of responses to generate for a given prompt. Generating multiple responses can provide more diverse and potentially more useful output, but it also increases the cost of the request.",
        "segment_17": "stop - a list of strings that, when encountered in the generated text, will cause the generation to stop. This can be used to control the length or the validity of the output.",
        "segment_18": "presence_penalty, frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrases in the generated text.",
        "segment_19": "best_of - the number of responses to generate server-side when selecting the \"best\" (the one with the highest log probability per token) response for a given prompt.",
        "segment_21": "The cost and utility of text generation are intertwined with the joint effect of these hyperparameters.",
        "segment_22": "There are also complex interactions among subsets of the hyperparameters. For example,",
        "segment_23": "the temperature and top_p are not recommended to be altered from their default values together because they both control the randomness of the generated text, and changing both at the same time can result in conflicting effects; n and best_of are rarely tuned together because if the application can process multiple outputs, filtering on the server side causes unnecessary information loss; both n and max_tokens will affect the total number of tokens generated, which in turn will affect the cost of the request.",
        "segment_24": "These interactions and trade-offs make it difficult to manually determine the optimal hyperparameter settings for a given text generation task.",
        "segment_25": "Do the choices matter? Check this blogpost to find example tuning results about gpt-3.5-turbo and gpt-4.",
        "segment_26": "With AutoGen, the tuning can be performed with the following information:",
        "segment_28": "Validation data.",
        "segment_29": "Evaluation function.",
        "segment_30": "Metric to optimize.",
        "segment_31": "Search space.",
        "segment_32": "Budgets: inference and optimization respectively.",
        "segment_34": "Validation data\u200b",
        "segment_35": "Collect a diverse set of instances. They can be stored in an iterable of dicts. For example, each instance dict can contain \"problem\" as a key and the description str of a math problem as the value; and \"solution\" as a key and the solution str as the value.",
        "segment_36": "Evaluation function\u200b",
        "segment_37": "The evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. For example,",
        "segment_38": "def eval_math_responses(responses: List[str], solution: str, **args) -> Dict: # select a response from the list of responses answer = voted_answer(responses) # check whether the answer is correct return {\"success\": is_equivalent(answer, solution)}",
        "segment_39": "autogen.code_utils and autogen.math_utils offer some example evaluation functions for code generation and math problem solving.",
        "segment_40": "Metric to optimize\u200b",
        "segment_41": "The metric to optimize is usually an aggregated metric over all the tuning data instances. For example, users can specify \"success\" as the metric and \"max\" as the optimization mode. By default, the aggregation function is taking the average. Users can provide a customized aggregation function if needed.",
        "segment_42": "Search space\u200b",
        "segment_43": "Users can specify the (optional) search range for each hyperparameter.",
        "segment_45": "model. Either a constant str, or multiple choices specified by flaml.tune.choice.",
        "segment_46": "prompt/messages. Prompt is either a str or a list of strs, of the prompt templates. messages is a list of dicts or a list of lists, of the message templates.",
        "segment_47": "Each prompt/message template will be formatted with each data instance. For example, the prompt template can be:",
        "segment_48": "\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\"",
        "segment_49": "And {problem} will be replaced by the \"problem\" field of each data instance.",
        "segment_50": "max_tokens, n, best_of. They can be constants, or specified by flaml.tune.randint, flaml.tune.qrandint, flaml.tune.lograndint or flaml.qlograndint. By default, max_tokens is searched in [50, 1000); n is searched in [1, 100); and best_of is fixed to 1.",
        "segment_51": "stop. It can be a str or a list of strs, or a list of lists of strs or None. Default is None.",
        "segment_52": "temperature or top_p. One of them can be specified as a constant or by flaml.tune.uniform or flaml.tune.loguniform etc.",
        "segment_53": "Please don't provide both. By default, each configuration will choose either a temperature or a top_p in [0, 1] uniformly.",
        "segment_54": "presence_penalty, frequency_penalty. They can be constants or specified by flaml.tune.uniform etc. Not tuned by default.",
        "segment_56": "Budgets\u200b",
        "segment_57": "One can specify an inference budget and an optimization budget.",
        "segment_58": "The inference budget refers to the average inference cost per data instance.",
        "segment_59": "The optimization budget refers to the total budget allowed in the tuning process. Both are measured by dollars and follow the price per 1000 tokens.",
        "segment_60": "Perform tuning\u200b",
        "segment_61": "Now, you can use autogen.Completion.tune for tuning. For example,",
        "segment_62": "import autogenconfig, analysis = autogen.Completion.tune( data=tune_data, metric=\"success\", mode=\"max\", eval_func=eval_func, inference_budget=0.05, optimization_budget=3, num_samples=-1,)",
        "segment_63": "num_samples is the number of configurations to sample. -1 means unlimited (until optimization budget is exhausted).",
        "segment_64": "The returned config contains the optimized configuration and analysis contains an ExperimentAnalysis object for all the tried configurations and results.",
        "segment_65": "The tuned config can be used to perform inference.",
        "segment_66": "API unification\u200b",
        "segment_67": "autogen.OpenAIWrapper.create() can be used to create completions for both chat and non-chat models, and both OpenAI API and Azure OpenAI API.",
        "segment_68": "from autogen import OpenAIWrapper# OpenAI endpointclient = OpenAIWrapper()# ChatCompletionresponse = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}], model=\"gpt-3.5-turbo\")# extract the response textprint(client.extract_text_or_completion_object(response))# get cost of this completionprint(response.cost)# Azure OpenAI endpointclient = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type=\"azure\")# Completionresponse = client.create(prompt=\"2+2=\", model=\"gpt-3.5-turbo-instruct\")# extract the response textprint(client.extract_text_or_completion_object(response))",
        "segment_69": "For local LLMs, one can spin up an endpoint using a package like FastChat, and then use the same API to send a request. See here for examples on how to make inference with local LLMs.",
        "segment_70": "For custom model clients, one can register the client with autogen.OpenAIWrapper.register_model_client and then use the same API to send a request. See here for examples on how to make inference with custom model clients.",
        "segment_71": "Usage Summary\u200b",
        "segment_72": "The OpenAIWrapper from autogen tracks token counts and costs of your API calls. Use the create() method to initiate requests and print_usage_summary() to retrieve a detailed usage report, including total cost and token usage for both cached and actual requests.",
        "segment_74": "mode=[\"actual\", \"total\"] (default): print usage summary for all completions and non-caching completions.",
        "segment_75": "mode='actual': only print non-cached usage.",
        "segment_76": "mode='total': only print all usage (including cache).",
        "segment_78": "Reset your session's usage data with clear_usage_summary() when needed. View Notebook",
        "segment_79": "Example usage:",
        "segment_80": "from autogen import OpenAIWrapperclient = OpenAIWrapper()client.create(messages=[{\"role\": \"user\", \"content\": \"Python learning tips.\"}], model=\"gpt-3.5-turbo\")client.print_usage_summary() # Display usageclient.clear_usage_summary() # Reset usage data",
        "segment_81": "Sample output:",
        "segment_82": "Usage summary excluding cached usage:Total cost: 0.00015* Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83Usage summary including cached usage:Total cost: 0.00027* Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150",
        "segment_83": "Note: if using a custom model client (see here for details) and if usage summary is not implemented, then the usage summary will not be available.",
        "segment_84": "Caching\u200b",
        "segment_85": "API call results are cached locally and reused when the same request is issued.",
        "segment_86": "This is useful when repeating or continuing experiments for reproducibility and cost saving.",
        "segment_87": "Starting version 0.2.8, a configurable context manager allows you to easily configure",
        "segment_88": "the cache, using either DiskCache or Redis.",
        "segment_89": "All OpenAIWrapper created inside the context manager can use the same cache",
        "segment_90": "through the constructor.",
        "segment_91": "from autogen import Cachewith Cache.redis(redis_url=\"redis://localhost:6379/0\") as cache: client = OpenAIWrapper(..., cache=cache) client.create(...)with Cache.disk() as cache: client = OpenAIWrapper(..., cache=cache) client.create(...)",
        "segment_92": "You can also set a cache directly in the create() method.",
        "segment_93": "client = OpenAIWrapper(...)with Cache.disk() as cache: client.create(..., cache=cache)",
        "segment_94": "You can vary the cache_seed parameter to get different LLM output while",
        "segment_95": "still using cache.",
        "segment_96": "# Setting the cache_seed to 1 will use a different cache from the default one# and you will see different output.with Cache.disk(cache_seed=1) as cache: client.create(..., cache=cache)",
        "segment_97": "By default DiskCache uses .cache for storage. To change the cache directory,",
        "segment_98": "set cache_path_root:",
        "segment_99": "with Cache.disk(cache_path_root=\"/tmp/autogen_cache\") as cache: client.create(..., cache=cache)",
        "segment_100": "Turnning off cache\u200b",
        "segment_101": "For backward compatibility, DiskCache is always enabled by default",
        "segment_102": "with cache_seed set to 41. To fully disable it, set cache_seed to None.",
        "segment_103": "# Turn off cache in constructor,client = OpenAIWrapper(..., cache_seed=None)# or directly in create().client.create(..., cache_seed=None)",
        "segment_104": "Difference between cache_seed and openai's seed parameter\u200b",
        "segment_105": "openai v1.1 introduces a new param seed.",
        "segment_106": "The differences between autogen's cache_seed and openai's seed:",
        "segment_107": "- autogen uses local disk cache to guarantee the exactly same output is produced",
        "segment_108": "for the same input and when cache is hit, no openai api call will be made.",
        "segment_109": "- openai's seed is a best-effort deterministic sampling with no guarantee",
        "segment_110": "of determinism. When using openai's seed with cache_seed set to None,",
        "segment_111": "even for the same input, an openai api call will be made and there is",
        "segment_112": "no guarantee for getting exactly the same output.",
        "segment_113": "Error handling\u200b",
        "segment_114": "Runtime error\u200b",
        "segment_115": "One can pass a list of configurations of different models/endpoints to mitigate the rate limits and other runtime error. For example,",
        "segment_116": "client = OpenAIWrapper( config_list=[ { \"model\": \"gpt-4\", \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"), \"api_type\": \"azure\", \"base_url\": os.environ.get(\"AZURE_OPENAI_API_BASE\"), \"api_version\": \"2023-08-01-preview\", }, { \"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"base_url\": \"https://api.openai.com/v1\", }, { \"model\": \"llama2-chat-7B\", \"base_url\": \"http://127.0.0.1:8080\", }, { \"model\": \"microsoft/phi-2\", \"model_client_cls\": \"CustomModelClient\" } ],)",
        "segment_117": "client.create() will try querying Azure OpenAI gpt-4, OpenAI gpt-3.5-turbo, a locally hosted llama2-chat-7B, and phi-2 using a custom model client class named CustomModelClient, one by one,",
        "segment_118": "until a valid result is returned. This can speed up the development process where the rate limit is a bottleneck. An error will be raised if the last choice fails. So make sure the last choice in the list has the best availability.",
        "segment_119": "For convenience, we provide a number of utility functions to load config lists.",
        "segment_121": "get_config_list: Generates configurations for API calls, primarily from provided API keys.",
        "segment_122": "config_list_openai_aoai: Constructs a list of configurations using both Azure OpenAI and OpenAI endpoints, sourcing API keys from environment variables or local files.",
        "segment_123": "config_list_from_json: Loads configurations from a JSON structure, either from an environment variable or a local JSON file, with the flexibility of filtering configurations based on given criteria.",
        "segment_124": "config_list_from_models: Creates configurations based on a provided list of models, useful when targeting specific models without manually specifying each configuration.",
        "segment_125": "config_list_from_dotenv: Constructs a configuration list from a .env file, offering a consolidated way to manage multiple API configurations and keys from a single file.",
        "segment_127": "We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints.",
        "segment_128": "Logic error\u200b",
        "segment_129": "Another type of error is that the returned response does not satisfy a requirement. For example, if the response is required to be a valid json string, one would like to filter the responses that are not. This can be achieved by providing a list of configurations and a filter function. For example,",
        "segment_130": "def valid_json_filter(response, **_): for text in OpenAIWrapper.extract_text_or_completion_object(response): try: json.loads(text) return True except ValueError: pass return Falseclient = OpenAIWrapper( config_list=[{\"model\": \"text-ada-001\"}, {\"model\": \"gpt-3.5-turbo-instruct\"}, {\"model\": \"text-davinci-003\"}],)response = client.create( prompt=\"How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.\", filter_func=valid_json_filter,)",
        "segment_131": "The example above will try to use text-ada-001, gpt-3.5-turbo-instruct, and text-davinci-003 iteratively, until a valid json string is returned or the last config is used. One can also repeat the same model in the list for multiple times (with different seeds) to try one model multiple times for increasing the robustness of the final response.",
        "segment_132": "Advanced use case: Check this blogpost to find how to improve GPT-4's coding performance from 68% to 90% while reducing the inference cost.",
        "segment_133": "Templating\u200b",
        "segment_134": "If the provided prompt or message is a template, it will be automatically materialized with a given context. For example,",
        "segment_135": "response = client.create( context={\"problem\": \"How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?\"}, prompt=\"{problem} Solve the problem carefully.\", allow_format_str_template=True, **config)",
        "segment_136": "A template is either a format str, like the example above, or a function which produces a str from several input fields, like the example below.",
        "segment_137": "def content(turn, context): return \"\\n\".join( [ context[f\"user_message_{turn}\"], context[f\"external_info_{turn}\"] ] )messages = [ { \"role\": \"system\", \"content\": \"You are a teaching assistant of math.\", }, { \"role\": \"user\", \"content\": partial(content, turn=0), },]context = { \"user_message_0\": \"Could you explain the solution to Problem 1?\", \"external_info_0\": \"Problem 1: ...\",}response = client.create(context=context, messages=messages, **config)messages.append( { \"role\": \"assistant\", \"content\": client.extract_text(response)[0] })messages.append( { \"role\": \"user\", \"content\": partial(content, turn=1), },)context.append( { \"user_message_1\": \"Why can't we apply Theorem 1 to Equation (2)?\", \"external_info_1\": \"Theorem 1: ...\", })response = client.create(context=context, messages=messages, **config)",
        "segment_138": "Logging (for openai<1)\u200b",
        "segment_139": "When debugging or diagnosing an LLM-based system, it is often convenient to log the API calls and analyze them. autogen.Completion and autogen.ChatCompletion offer an easy way to collect the API call histories. For example, to log the chat histories, simply run:",
        "segment_140": "autogen.ChatCompletion.start_logging()",
        "segment_141": "The API calls made after this will be automatically logged. They can be retrieved at any time by:",
        "segment_142": "autogen.ChatCompletion.logged_history",
        "segment_143": "There is a function that can be used to print usage summary (total cost, and token count usage from each model):",
        "segment_144": "autogen.ChatCompletion.print_usage_summary()",
        "segment_145": "To stop logging, use",
        "segment_146": "autogen.ChatCompletion.stop_logging()",
        "segment_147": "If one would like to append the history to an existing dict, pass the dict like:",
        "segment_148": "autogen.ChatCompletion.start_logging(history_dict=existing_history_dict)",
        "segment_149": "By default, the counter of API calls will be reset at start_logging(). If no reset is desired, set reset_counter=False.",
        "segment_150": "There are two types of logging formats: compact logging and individual API call logging. The default format is compact.",
        "segment_151": "Set compact=False in start_logging() to switch.",
        "segment_153": "Example of a history dict with compact logging.",
        "segment_155": "{ \"\"\" [ { 'role': 'system', 'content': system_message, }, { 'role': 'user', 'content': user_message_1, }, { 'role': 'assistant', 'content': assistant_message_1, }, { 'role': 'user', 'content': user_message_2, }, { 'role': 'assistant', 'content': assistant_message_2, }, ]\"\"\": { \"created_at\": [0, 1], \"cost\": [0.1, 0.2], }}",
        "segment_157": "Example of a history dict with individual API call logging.",
        "segment_159": "{ 0: { \"request\": { \"messages\": [ { \"role\": \"system\", \"content\": system_message, }, { \"role\": \"user\", \"content\": user_message_1, } ], ... # other parameters in the request }, \"response\": { \"choices\": [ \"messages\": { \"role\": \"assistant\", \"content\": assistant_message_1, }, ], ... # other fields in the response } }, 1: { \"request\": { \"messages\": [ { \"role\": \"system\", \"content\": system_message, }, { \"role\": \"user\", \"content\": user_message_1, }, { \"role\": \"assistant\", \"content\": assistant_message_1, }, { \"role\": \"user\", \"content\": user_message_2, }, ], ... # other parameters in the request }, \"response\": { \"choices\": [ \"messages\": { \"role\": \"assistant\", \"content\": assistant_message_2, }, ], ... # other fields in the response } },}",
        "segment_161": "Example of printing for usage summary",
        "segment_163": "Total cost: Token count summary for model : prompt_tokens: , completion_tokens: , total_tokens:",
        "segment_164": "It can be seen that the individual API call history contains redundant information of the conversation. For a long conversation the degree of redundancy is high.",
        "segment_165": "The compact history is more efficient and the individual API call history contains more details.Edit this pagePreviousMulti-agent Conversation FrameworkNextContributingTune Inference Parameters (for openai<1)Choices to optimizeValidation dataEvaluation functionMetric to optimizeSearch spaceBudgetsPerform tuningAPI unificationUsage SummaryCachingTurnning off cacheDifference between cache_seed and openai's seed parameterError handlingRuntime errorLogic errorTemplatingLogging (for openai<1)CommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class DiskCache(AbstractCache)",
        "segment_2": "Implementation of AbstractCache using the DiskCache library.",
        "segment_3": "This class provides a concrete implementation of the AbstractCache",
        "segment_4": "interface using the diskcache library for caching data on disk.",
        "segment_5": "Attributes:",
        "segment_7": "cache diskcache.Cache - The DiskCache instance used for caching.",
        "segment_9": "Methods:",
        "segment_10": "init(self, seed): Initializes the DiskCache with the given seed.",
        "segment_11": "get(self, key, default=None): Retrieves an item from the cache.",
        "segment_12": "set(self, key, value): Sets an item in the cache.",
        "segment_14": "close(self) - Closes the cache.",
        "segment_15": "__enter__(self) - Context management entry.",
        "segment_16": "exit(self, exc_type, exc_value, traceback): Context management exit.",
        "segment_18": "__init__\u200b",
        "segment_19": "def __init__(seed)",
        "segment_20": "Initialize the DiskCache instance.",
        "segment_21": "Arguments:",
        "segment_23": "seed str - A seed or namespace for the cache. This is used to create",
        "segment_24": "a unique storage location for the cache data.",
        "segment_26": "get\u200b",
        "segment_27": "def get(key, default=None)",
        "segment_28": "Retrieve an item from the cache.",
        "segment_29": "Arguments:",
        "segment_31": "key str - The key identifying the item in the cache.",
        "segment_32": "default optional - The default value to return if the key is not found.",
        "segment_33": "Defaults to None.",
        "segment_35": "Returns:",
        "segment_36": "The value associated with the key if found, else the default value.",
        "segment_37": "set\u200b",
        "segment_38": "def set(key, value)",
        "segment_39": "Set an item in the cache.",
        "segment_40": "Arguments:",
        "segment_42": "key str - The key under which the item is to be stored.",
        "segment_43": "value - The value to be stored in the cache.",
        "segment_45": "close\u200b",
        "segment_46": "def close()",
        "segment_47": "Close the cache.",
        "segment_48": "Perform any necessary cleanup, such as closing file handles or",
        "segment_49": "releasing resources.",
        "segment_50": "__enter__\u200b",
        "segment_51": "def __enter__()",
        "segment_52": "Enter the runtime context related to the object.",
        "segment_53": "Returns:",
        "segment_55": "self - The instance itself.",
        "segment_57": "__exit__\u200b",
        "segment_58": "def __exit__(exc_type, exc_value, traceback)",
        "segment_59": "Exit the runtime context related to the object.",
        "segment_60": "Perform cleanup actions such as closing the cache.",
        "segment_61": "Arguments:",
        "segment_63": "exc_type - The exception type if an exception was raised in the context.",
        "segment_64": "exc_value - The exception value if an exception was raised in the context.",
        "segment_65": "traceback - The traceback if an exception was raised in the context.",
        "segment_66": "Edit this pagePreviouscache_factoryNextredis_cacheDiskCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Migrating to 0.2\u200b",
        "segment_2": "openai v1 is a total rewrite of the library with many breaking changes. For example, the inference requires instantiating a client, instead of using a global class method.",
        "segment_3": "Therefore, some changes are required for users of pyautogen<0.2.",
        "segment_5": "api_base -> base_url, request_timeout -> timeout in llm_config and config_list. max_retry_period and retry_wait_time are deprecated. max_retries can be set for each client.",
        "segment_6": "MathChat is unsupported until it is tested in future release.",
        "segment_7": "autogen.Completion and autogen.ChatCompletion are deprecated. The essential functionalities are moved to autogen.OpenAIWrapper:",
        "segment_9": "from autogen import OpenAIWrapperclient = OpenAIWrapper(config_list=config_list)response = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}])print(client.extract_text_or_completion_object(response))",
        "segment_11": "Inference parameter tuning and inference logging features are currently unavailable in OpenAIWrapper. Logging will be added in a future release.",
        "segment_12": "Inference parameter tuning can be done via flaml.tune.",
        "segment_13": "seed in autogen is renamed into cache_seed to accommodate the newly added seed param in openai chat completion api. use_cache is removed as a kwarg in OpenAIWrapper.create() for being automatically decided by cache_seed: int | None. The difference between autogen's cache_seed and openai's seed is that:",
        "segment_15": "autogen uses local disk cache to guarantee the exactly same output is produced for the same input and when cache is hit, no openai api call will be made.",
        "segment_16": "openai's seed is a best-effort deterministic sampling with no guarantee of determinism. When using openai's seed with cache_seed set to None, even for the same input, an openai api call will be made and there is no guarantee for getting exactly the same output.",
        "segment_19": "Edit this pagePreviousResearchMigrating to 0.2CommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "Fig.1 illustrates the general flow of AgentEval",
        "segment_2": "TL;DR:",
        "segment_4": "As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?",
        "segment_5": "To shed light on the question above, we introduce AgentEval \u2014 the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.",
        "segment_6": "We demonstrate how AgentEval work using math problems dataset as an example in the following notebook. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_8": "Introduction\u200b",
        "segment_9": "AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics \u2013 essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.",
        "segment_10": "Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of AgentEval framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.",
        "segment_12": "Fig. 2 provides an overview of the tasks taxonomy",
        "segment_13": "Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:",
        "segment_15": "Success is not clearly defined - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.",
        "segment_16": "Success is clearly defined - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:",
        "segment_18": "The optimal solution exits - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.",
        "segment_19": "Multiple solutions exist - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.",
        "segment_23": "In our AgentEval framework, we are currently focusing on tasks where Success is clearly defined. Next, we will introduce the suggested framework.",
        "segment_24": "AgentEval Framework\u200b",
        "segment_25": "Our previous research on assistive agents in Minecraft suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, 'the first agent was faster in execution,' or 'the second agent moves more naturally.' So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed AgentEval (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task utility for the multi-agent system. Namely:",
        "segment_27": "The goal of CriticAgent is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how CriticAgent is defined using Autogen:",
        "segment_29": "critic = autogen.AssistantAgent( name=\"critic\", llm_config={\"config_list\": config_list}, system_message=\"\"\"You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant. Convert the evaluation criteria into a dictionary where the keys are the criteria. The value of each key is a dictionary as follows {\"description\": criteria description, \"accepted_values\": possible accepted inputs for this key} Make sure the keys are criteria for assessing the given task. \"accepted_values\" include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. \"description\" includes the criterion description. Return only the dictionary.\"\"\")",
        "segment_30": "Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the following notebook.",
        "segment_32": "The goal of QuantifierAgent is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:",
        "segment_34": "quantifier = autogen.AssistantAgent( name=\"quantifier\", llm_config={\"config_list\": config_list}, system_message = \"\"\"You are a helpful assistant. You quantify the output of different tasks based on the given criteria. The criterion is given in a dictionary format where each key is a distinct criteria. The value of each key is a dictionary as follows {\"description\": criteria description , \"accepted_values\": possible accepted inputs for this key} You are going to quantify each of the criteria for a given task based on the task description. Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria. Return only the dictionary.\"\"\")",
        "segment_35": "AgentEval Results based on Math Problems Dataset\u200b",
        "segment_36": "As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:",
        "segment_37": "CriteriaDescriptionAccepted ValuesProblem InterpretationAbility to correctly interpret the problem[\"completely off\", \"slightly relevant\", \"relevant\", \"mostly accurate\", \"completely accurate\"]Mathematical MethodologyAdequacy of the chosen mathematical or algorithmic methodology for the question[\"inappropriate\", \"barely adequate\", \"adequate\", \"mostly effective\", \"completely effective\"]Calculation CorrectnessAccuracy of calculations made and solutions given[\"completely incorrect\", \"mostly incorrect\", \"neither\", \"mostly correct\", \"completely correct\"]Explanation ClarityClarity and comprehensibility of explanations, including language use and structure[\"not at all clear\", \"slightly clear\", \"moderately clear\", \"very clear\", \"completely clear\"]Code EfficiencyQuality of code in terms of efficiency and elegance[\"not at all efficient\", \"slightly efficient\", \"moderately efficient\", \"very efficient\", \"extremely efficient\"]Code CorrectnessCorrectness of the provided code[\"completely incorrect\", \"mostly incorrect\", \"partly correct\", \"mostly correct\", \"completely correct\"]",
        "segment_38": "Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:",
        "segment_40": "AgentChat",
        "segment_41": "ReAct",
        "segment_42": "GPT-4 Vanilla Solver",
        "segment_44": "Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.",
        "segment_46": "Fig.3 presents results based on overall math problems dataset _s stands for successful cases, _f - stands for failed cases",
        "segment_47": "We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval's ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.",
        "segment_48": "It's important not only to identify what is not working but also to recognize what and why actually went well.",
        "segment_49": "Limitations and Future Work\u200b",
        "segment_50": "The current implementation of AgentEval has a number of limitations which are planning to overcome in the future:",
        "segment_52": "The list of criteria varies per run (unless you store a seed). We would recommend to run CriticAgent at least two times, and pick criteria you think is important for your domain.",
        "segment_53": "The results of the QuantifierAgent can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.",
        "segment_55": "To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.",
        "segment_56": "Summary\u200b",
        "segment_57": "CriticAgent and QuantifierAgent can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.",
        "segment_58": "We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our Discord.",
        "segment_59": "Previous Research\u200b",
        "segment_60": "@InProceedings{pmlr-v176-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021\", author = \"Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\\^o}t{\\'e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon\", booktitle = \"Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track\", pages = \"146--161\", year = 2022, editor = \"Kiela, Douwe and Ciccone, Marco and Caputo, Barbara\", volume = 176, series = \"Proceedings of Machine Learning Research\", month = \"06--14 Dec\", publisher = \"PMLR\", pdf = {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf}, url = {https://proceedings.mlr.press/v176/kiseleva22a.html}.}",
        "segment_61": "@InProceedings{pmlr-v220-kiseleva22a, title = \"Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition\", author = \"Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\\^{o}t\\'e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin\", booktitle = \"Proceedings of the NeurIPS 2022 Competitions Track\", pages = \"204--216\", year = 2022, editor = \"Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob\", volume = 220, series = \"Proceedings of Machine Learning Research\", month = \"28 Nov--09 Dec\", publisher = \"PMLR\", pdf = \"https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf\", url = \"https://proceedings.mlr.press/v220/kiseleva22a.html\".}Tags:LLMGPTevaluationtask utilityCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "In Brief:",
        "segment_3": "Introducing the Multimodal Conversable Agent and the LLaVA Agent to enhance LMM functionalities.",
        "segment_4": "Users can input text and images simultaneously using the tag to specify image loading.",
        "segment_5": "Demonstrated through the GPT-4V notebook.",
        "segment_6": "Demonstrated through the LLaVA notebook.",
        "segment_8": "Introduction\u200b",
        "segment_9": "Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.",
        "segment_10": "This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.",
        "segment_11": "We support the gpt-4-vision-preview model from OpenAI and LLaVA model from Microsoft now.",
        "segment_12": "Here, we emphasize the Multimodal Conversable Agent and the LLaVA Agent due to their growing popularity.",
        "segment_13": "GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.",
        "segment_14": "Installation\u200b",
        "segment_15": "Incorporate the lmm feature during AutoGen installation:",
        "segment_16": "pip install \"pyautogen[lmm]\"",
        "segment_17": "Subsequently, import the Multimodal Conversable Agent or LLaVA Agent from AutoGen:",
        "segment_18": "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent # for GPT-4Vfrom autogen.agentchat.contrib.llava_agent import LLaVAAgent # for LLaVA",
        "segment_19": "Usage\u200b",
        "segment_20": "A simple syntax has been defined to incorporate both messages and images within a single string.",
        "segment_21": "Example of an in-context learning prompt:",
        "segment_22": "prompt = \"\"\"You are now an image classifier for facial expressions. Here aresome examples. depicts a happy expression. represents a sad expression. portrays a neutral expression.Now, identify the facial expression of this individual: \"\"\"agent = MultimodalConversableAgent()user = UserProxyAgent()user.initiate_chat(agent, message=prompt)",
        "segment_23": "The MultimodalConversableAgent interprets the input prompt, extracting images from local or internet sources.",
        "segment_24": "Advanced Usage\u200b",
        "segment_25": "Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.",
        "segment_26": "For example, the FigureCreator in our GPT-4V notebook and LLaVA notebook integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).",
        "segment_27": "The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.",
        "segment_28": "With human_input_mode=ALWAYS, you can also contribute suggestions for better visualizations.",
        "segment_29": "Reference\u200b",
        "segment_31": "GPT-4V System Card",
        "segment_32": "LLaVA GitHub",
        "segment_34": "Future Enhancements\u200b",
        "segment_35": "For further inquiries or suggestions, please open an issue in the AutoGen repository or contact me directly at beibin.li@microsoft.com.",
        "segment_36": "AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.Tags:LMMmultimodalNewer PostEcoAssistant - Using LLM Assistants More Accurately and AffordablyOlder PostAutoGen's Teachable AgentsIntroductionInstallationUsageAdvanced UsageReferenceFuture EnhancementsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Agent()",
        "segment_2": "(In preview) An abstract class for AI agent.",
        "segment_3": "An agent can communicate with other agents and perform actions.",
        "segment_4": "Different agents can differ in what actions they perform in the receive method.",
        "segment_5": "__init__\u200b",
        "segment_6": "def __init__(name: str)",
        "segment_7": "Arguments:",
        "segment_9": "name str - name of the agent.",
        "segment_11": "name\u200b",
        "segment_12": "@propertydef name()",
        "segment_13": "Get the name of the agent.",
        "segment_14": "send\u200b",
        "segment_15": "def send(message: Union[Dict, str], recipient: \"Agent\", request_reply: Optional[bool] = None)",
        "segment_16": "(Abstract method) Send a message to another agent.",
        "segment_17": "a_send\u200b",
        "segment_18": "async def a_send(message: Union[Dict, str], recipient: \"Agent\", request_reply: Optional[bool] = None)",
        "segment_19": "(Abstract async method) Send a message to another agent.",
        "segment_20": "receive\u200b",
        "segment_21": "def receive(message: Union[Dict, str], sender: \"Agent\", request_reply: Optional[bool] = None)",
        "segment_22": "(Abstract method) Receive a message from another agent.",
        "segment_23": "a_receive\u200b",
        "segment_24": "async def a_receive(message: Union[Dict, str], sender: \"Agent\", request_reply: Optional[bool] = None)",
        "segment_25": "(Abstract async method) Receive a message from another agent.",
        "segment_26": "reset\u200b",
        "segment_27": "def reset()",
        "segment_28": "(Abstract method) Reset the agent.",
        "segment_29": "generate_reply\u200b",
        "segment_30": "def generate_reply(messages: Optional[List[Dict]] = None, sender: Optional[\"Agent\"] = None, **kwargs) -> Union[str, Dict, None]",
        "segment_31": "(Abstract method) Generate a reply based on the received messages.",
        "segment_32": "Arguments:",
        "segment_34": "messages list[dict] - a list of messages received.",
        "segment_35": "sender - sender of an Agent instance.",
        "segment_37": "Returns:",
        "segment_38": "str or dict or None: the generated reply. If None, no reply is generated.",
        "segment_39": "a_generate_reply\u200b",
        "segment_40": "async def a_generate_reply(messages: Optional[List[Dict]] = None, sender: Optional[\"Agent\"] = None, **kwargs) -> Union[str, Dict, None]",
        "segment_41": "(Abstract async method) Generate a reply based on the received messages.",
        "segment_42": "Arguments:",
        "segment_44": "messages list[dict] - a list of messages received.",
        "segment_45": "sender - sender of an Agent instance.",
        "segment_47": "Returns:",
        "segment_48": "str or dict or None: the generated reply. If None, no reply is generated.Edit this pagePreviousweb_surferNextassistant_agentAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_2": "Introducing AgentOptimizer, a new class for training LLM agents in the era of LLMs as a service.",
        "segment_3": "AgentOptimizer is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.",
        "segment_4": "Checkout one implementation for AgentOptimizer on MATH dataset",
        "segment_5": "here.",
        "segment_6": "Paper is coming soon!",
        "segment_7": "Introduction\u200b",
        "segment_8": "In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?",
        "segment_9": "Here, we take an initial step towards the agent training.",
        "segment_10": "Inspired by the function calling capabilities provided by OpenAI,",
        "segment_11": "we draw an analogy between model weights and agent functions/skills, and update an agent\u2019s functions/skills based on its historical performance on a training set.",
        "segment_12": "Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents\u2019 functions as a set of function calls,",
        "segment_13": "to support iteratively adding, revising, and removing existing functions.",
        "segment_14": "As an agentic way of training an agent, our approach helps enhance the agents\u2019 abilities without requiring access to the LLMs weights.",
        "segment_15": "AgentOptimizer\u200b",
        "segment_16": "AgentOptimizer is a class designed to optimize the agents by improving their function calls.",
        "segment_17": "It contains two core methods:",
        "segment_19": "step(): step() takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).",
        "segment_21": "actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)",
        "segment_22": "It has two outputs actions and updated_functions. actions is a series of actions to manipulate the current functions. And updated_functions is the updated functions after the actions are applied (including code implementation).",
        "segment_24": "update_function_call():",
        "segment_25": "This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from step().",
        "segment_26": "For AssistantAgent, it first uses update_function_signature to update the function signatures.",
        "segment_27": "Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from step().",
        "segment_29": "Sometimes, the function signatures (JSON schema) returned by the step() may not be valid, and the generated code may also face syntax errors.",
        "segment_30": "AgentOptimizer includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.",
        "segment_31": "Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.",
        "segment_32": "Pseudocode for the optimization process\u200b",
        "segment_33": "The optimization process is as follows:",
        "segment_34": "for - in range(EPOCH): history, statistic, current_functions = solve_problems(train_problems) actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions) AgentOptimizer.update_function_call(actions)",
        "segment_35": "Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.",
        "segment_36": "The functions are then improved using AgentOptimizer.",
        "segment_37": "Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.",
        "segment_38": "After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks",
        "segment_39": "The implementation technology behind the AgentOptimizer\u200b",
        "segment_40": "To obtain stable and structured function signatures and code implementations from AgentOptimizer,",
        "segment_41": "we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.",
        "segment_42": "Specifically, we introduce three function calls to manipulate the current functions at each step: add_function, remove_function, and revise_function.",
        "segment_43": "These calls add, remove, and revise functions in the existing function list, respectively.",
        "segment_44": "This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.",
        "segment_45": "Below is the JSON schema of these function calls:",
        "segment_47": "add_function: Add one new function that may be used in the future tasks.",
        "segment_49": "ADD_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"add_function\", \"description\": \"Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_51": "revise_function: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.",
        "segment_53": "REVISE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"revise_function\", \"description\": \"Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" }, \"description\": { \"type\": \"string\", \"description\": \"A short description of the function.\" }, \"arguments\": { \"type\": \"string\", \"description\": \"JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \\\"url\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The URL\\\", }}. Please avoid the error 'array schema missing items' when using array type.\" }, \"packages\": { \"type\": \"string\", \"description\": \"A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.\" }, \"code\": { \"type\": \"string\", \"description\": \"The implementation in Python. Do not include the function declaration.\" } }, \"required\": [\"name\", \"description\", \"arguments\", \"packages\", \"code\"] } }}",
        "segment_55": "remove_function: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.",
        "segment_57": "REMOVE_FUNC = { \"type\": \"function\", \"function\": { \"name\": \"remove_function\", \"description\": \"Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"The name of the function in the code implementation.\" } }, \"required\": [\"name\"] } }}",
        "segment_58": "Limitation & Future work\u200b",
        "segment_60": "Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.",
        "segment_61": "When the training epoch is small, the agent\u2019s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.",
        "segment_62": "The Current implementation of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.",
        "segment_63": "It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.",
        "segment_64": "Currently, it only supports optimizing the multi-agent system in solving problems from MATH dataset. We will integrate it into autogen with more general interface in the future.",
        "segment_65": "Tags:LLMresearchNewer PostAll About Agent DescriptionsOlder PostAutoGen Studio: Interactively Explore Multi-Agent WorkflowsIntroductionAgentOptimizerPseudocode for the optimization processThe implementation technology behind the AgentOptimizerLimitation & Future workCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_4": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_5": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_7": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_8": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_9": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_10": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_11": "Experiment Setup\u200b",
        "segment_12": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_14": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_15": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_17": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_19": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_20": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_21": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_22": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_23": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_25": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_26": "Experiment Results\u200b",
        "segment_27": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_28": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_29": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_31": "However, the selected model changes on level 4 Algebra.",
        "segment_33": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_34": "On level 5 the result is similar.",
        "segment_36": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_37": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_38": "Analysis and Discussion\u200b",
        "segment_39": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_40": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_41": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_42": "For Further Reading\u200b",
        "segment_44": "Research paper about the tuning technique",
        "segment_45": "Documentation about inference tuning",
        "segment_47": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostAchieve More, Pay Less - Use GPT-4 SmartlyExperiment SetupExperiment ResultsAnalysis and DiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.",
        "segment_4": "For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.",
        "segment_5": "AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.",
        "segment_7": "Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?",
        "segment_8": "In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for MATH, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.",
        "segment_9": "We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.",
        "segment_10": "We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.",
        "segment_11": "Experiment Setup\u200b",
        "segment_12": "We use AutoGen to select between the following models with a target inference budget $0.02 per instance:",
        "segment_14": "gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app",
        "segment_15": "gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo",
        "segment_17": "We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:",
        "segment_19": "temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].",
        "segment_20": "top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].",
        "segment_21": "max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].",
        "segment_22": "n: The number of responses to generate. We search for the optimal n in the range of [1, 100].",
        "segment_23": "prompt: We use the template: \"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\boxed{{}}.\" where {problem} will be replaced by the math problem instance.",
        "segment_25": "In this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.",
        "segment_26": "Experiment Results\u200b",
        "segment_27": "The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.",
        "segment_28": "Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.",
        "segment_29": "The same observation can be obtained on the level 3 Algebra test set.",
        "segment_31": "However, the selected model changes on level 4 Algebra.",
        "segment_33": "This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.",
        "segment_34": "On level 5 the result is similar.",
        "segment_36": "We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.",
        "segment_37": "An example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.",
        "segment_38": "Analysis and Discussion\u200b",
        "segment_39": "While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.",
        "segment_40": "There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via flaml.tune.",
        "segment_41": "The need for model selection, parameter tuning and cost saving is not specific to the math problems. The Auto-GPT project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.",
        "segment_42": "For Further Reading\u200b",
        "segment_44": "Research paper about the tuning technique",
        "segment_45": "Documentation about inference tuning",
        "segment_47": "Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostAchieve More, Pay Less - Use GPT-4 SmartlyExperiment SetupExperiment ResultsAnalysis and DiscussionFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def token_left(input: Union[str, List, Dict], model=\"gpt-3.5-turbo-0613\") -> int",
        "segment_2": "Count number of tokens left for an OpenAI model.",
        "segment_3": "Arguments:",
        "segment_5": "input - (str, list, dict): Input to the model.",
        "segment_6": "model - (str): Model name.",
        "segment_8": "Returns:",
        "segment_10": "int - Number of tokens left that the model can use for completion.",
        "segment_12": "count_token\u200b",
        "segment_13": "def count_token(input: Union[str, List, Dict], model: str = \"gpt-3.5-turbo-0613\") -> int",
        "segment_14": "Count number of tokens used by an OpenAI model.",
        "segment_15": "Arguments:",
        "segment_17": "input - (str, list, dict): Input to the model.",
        "segment_18": "model - (str): Model name.",
        "segment_20": "Returns:",
        "segment_22": "int - Number of tokens from the input.",
        "segment_24": "num_tokens_from_functions\u200b",
        "segment_25": "def num_tokens_from_functions(functions, model=\"gpt-3.5-turbo-0613\") -> int",
        "segment_26": "Return the number of tokens used by a list of functions.",
        "segment_27": "Arguments:",
        "segment_29": "functions - (list): List of function descriptions that will be passed in model.",
        "segment_30": "model - (str): Model name.",
        "segment_32": "Returns:",
        "segment_34": "int - Number of tokens from the function descriptions.",
        "segment_35": "Edit this pagePreviousretrieve_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class CompressibleAgent(ConversableAgent)",
        "segment_2": "(Experimental) CompressibleAgent agent. While this agent retains all the default functionalities of the AssistantAgent,",
        "segment_3": "it also provides the added feature of compression when activated through the compress_config setting.",
        "segment_4": "compress_config is set to False by default, making this agent equivalent to the AssistantAgent.",
        "segment_5": "This agent does not work well in a GroupChat: The compressed messages will not be sent to all the agents in the group.",
        "segment_6": "The default system message is the same as AssistantAgent.",
        "segment_7": "human_input_mode is default to \"NEVER\"",
        "segment_8": "and code_execution_config is default to False.",
        "segment_9": "This agent doesn't execute code or function call by default.",
        "segment_10": "__init__\u200b",
        "segment_11": "def __init__(name: str, system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE, is_termination_msg: Optional[Callable[[Dict], bool]] = None, max_consecutive_auto_reply: Optional[int] = None, human_input_mode: Optional[str] = \"NEVER\", function_map: Optional[Dict[str, Callable]] = None, code_execution_config: Optional[Union[Dict, bool]] = False, llm_config: Optional[Union[Dict, bool]] = None, default_auto_reply: Optional[Union[str, Dict, None]] = \"\", compress_config: Optional[Dict] = False, description: Optional[str] = None, **kwargs)",
        "segment_12": "Arguments:",
        "segment_14": "name str - agent name.",
        "segment_15": "system_message str - system message for the ChatCompletion inference.",
        "segment_16": "Please override this attribute if you want to reprogram the agent.",
        "segment_17": "llm_config dict - llm inference configuration.",
        "segment_18": "Please refer to OpenAIWrapper.create",
        "segment_19": "for available options.",
        "segment_20": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_21": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_22": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_23": "max_consecutive_auto_reply int - the maximum number of consecutive auto replies.",
        "segment_24": "default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).",
        "segment_25": "The limit only plays a role when human_input_mode is not \"ALWAYS\".",
        "segment_26": "compress_config dict or True/False - config for compression before oai_reply. Default to False.",
        "segment_27": "You should contain the following keys:",
        "segment_29": "\"mode\" (Optional, str, default to \"TERMINATE\"): Choose from [\"COMPRESS\", \"TERMINATE\", \"CUSTOMIZED\"].",
        "segment_32": "\"TERMINATE\" - terminate the conversation ONLY when token count exceeds the max limit of current model.",
        "segment_33": "trigger_count is NOT used in this mode.",
        "segment_34": "\"COMPRESS\" - compress the messages when the token count exceeds the limit.",
        "segment_35": "\"CUSTOMIZED\" - pass in a customized function to compress the messages.",
        "segment_37": "\"compress_function\" (Optional, callable, default to None): Must be provided when mode is \"CUSTOMIZED\".",
        "segment_38": "The function should takes a list of messages and returns a tuple of (is_compress_success: bool, compressed_messages: List[Dict]).",
        "segment_39": "\"trigger_count\" (Optional, float, int, default to 0.7): the threshold to trigger compression.",
        "segment_40": "If a float between (0, 1], it is the percentage of token used. if a int, it is the number of tokens used.",
        "segment_41": "\"async\" (Optional, bool, default to False): whether to compress asynchronously.",
        "segment_42": "\"broadcast\" (Optional, bool, default to True): whether to update the compressed message history to sender.",
        "segment_43": "\"verbose\" (Optional, bool, default to False): Whether to print the content before and after compression. Used when mode=\"COMPRESS\".",
        "segment_44": "\"leave_last_n\" (Optional, int, default to 0): If provided, the last n messages will not be compressed. Used when mode=\"COMPRESS\".",
        "segment_47": "system_message0 str - a short description of the agent. This description is used by other agents",
        "segment_48": "(e.g. the GroupChatManager) to decide when to call upon this agent. (Default: system_message)",
        "segment_49": "system_message1 dict - Please refer to other kwargs in",
        "segment_50": "ConversableAgent.",
        "segment_52": "generate_reply\u200b",
        "segment_53": "def generate_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None]",
        "segment_54": "Adding to line 202:",
        "segment_55": "if messages is not None and messages != self._oai_messages[sender]: messages = self._oai_messages[sender]",
        "segment_56": "on_oai_token_limit\u200b",
        "segment_57": "def on_oai_token_limit( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]",
        "segment_58": "(Experimental) Compress previous messages when a threshold of tokens is reached.",
        "segment_59": "TODO: async compress",
        "segment_60": "TODO: maintain a list for old oai messages (messages before compression)",
        "segment_61": "compress_messages\u200b",
        "segment_62": "def compress_messages( messages: Optional[List[Dict]] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None, List]]",
        "segment_63": "Compress a list of messages into one message.",
        "segment_64": "The first message (the initial prompt) will not be compressed.",
        "segment_65": "The rest of the messages will be compressed into one message, the model is asked to distinguish the role of each message: USER, ASSISTANT, FUNCTION_CALL, FUNCTION_RETURN.",
        "segment_66": "Check out the compress_sys_msg.",
        "segment_67": "TODO: model used in compression agent is different from assistant agent: For example, if original model used by is gpt-4; we start compressing at 70% of usage, 70% of 8092 = 5664; and we use gpt 3.5 here max_toke = 4096, it will raise error."
    },
    {
        "segment_1": "class Teachability(AgentCapability)",
        "segment_2": "Teachability uses a vector database to give an agent the ability to remember user teachings,",
        "segment_3": "where the user is any caller (human or not) sending messages to the teachable agent.",
        "segment_4": "Teachability is designed to be composable with other agent capabilities.",
        "segment_5": "To make any conversable agent teachable, instantiate both the agent and the Teachability class,",
        "segment_6": "then pass the agent to teachability.add_to_agent(agent).",
        "segment_7": "Note that teachable agents in a group chat must be given unique path_to_db_dir values.",
        "segment_8": "__init__\u200b",
        "segment_9": "def __init__(verbosity: Optional[int] = 0, reset_db: Optional[bool] = False, path_to_db_dir: Optional[str] = \"./tmp/teachable_agent_db\", recall_threshold: Optional[float] = 1.5, max_num_retrievals: Optional[int] = 10, llm_config: Optional[Union[Dict, bool]] = None)",
        "segment_10": "Arguments:",
        "segment_12": "verbosity Optional, int - # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.",
        "segment_13": "reset_db Optional, bool - True to clear the DB before starting. Default False.",
        "segment_14": "path_to_db_dir Optional, str - path to the directory where this particular agent's DB is stored. Default \"./tmp/teachable_agent_db\"",
        "segment_15": "recall_threshold Optional, float - The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.",
        "segment_16": "max_num_retrievals Optional, int - The maximum number of memos to retrieve from the DB. Default 10.",
        "segment_17": "llm_config dict or False - llm inference configuration passed to TextAnalyzerAgent.",
        "segment_18": "If None, TextAnalyzerAgent uses llm_config from the teachable agent.",
        "segment_20": "add_to_agent\u200b",
        "segment_21": "def add_to_agent(agent: ConversableAgent)",
        "segment_22": "Adds teachability to the given agent.",
        "segment_23": "prepopulate_db\u200b",
        "segment_24": "def prepopulate_db()",
        "segment_25": "Adds a few arbitrary memos to the DB.",
        "segment_26": "process_last_message\u200b",
        "segment_27": "def process_last_message(text)",
        "segment_28": "Appends any relevant memos to the message text, and stores any apparent teachings in new memos.",
        "segment_29": "Uses TextAnalyzerAgent to make decisions about memo storage and retrieval.",
        "segment_30": "MemoStore Objects\u200b",
        "segment_31": "class MemoStore()",
        "segment_32": "Provides memory storage and retrieval for a teachable agent, using a vector database.",
        "segment_33": "Each DB entry (called a memo) is a pair of strings: an input text and an output text.",
        "segment_34": "The input text might be a question, or a task to perform.",
        "segment_35": "The output text might be an answer to the question, or advice on how to perform the task.",
        "segment_36": "Vector embeddings are currently supplied by Chroma's default Sentence Transformers.",
        "segment_37": "__init__\u200b",
        "segment_38": "def __init__(verbosity, reset, path_to_db_dir)",
        "segment_39": "Arguments:",
        "segment_41": "verbosity (Optional, int): 1 to print memory operations, 0 to omit them. 3+ to print memo lists.",
        "segment_42": "path_to_db_dir (Optional, str): path to the directory where the DB is stored.",
        "segment_44": "list_memos\u200b",
        "segment_45": "def list_memos()",
        "segment_46": "Prints the contents of MemoStore.",
        "segment_47": "reset_db\u200b",
        "segment_48": "def reset_db()",
        "segment_49": "Forces immediate deletion of the DB's contents, in memory and on disk.",
        "segment_50": "add_input_output_pair\u200b",
        "segment_51": "def add_input_output_pair(input_text, output_text)",
        "segment_52": "Adds an input-output pair to the vector DB.",
        "segment_53": "get_nearest_memo\u200b",
        "segment_54": "def get_nearest_memo(query_text)",
        "segment_55": "Retrieves the nearest memo to the given query text.",
        "segment_56": "get_related_memos\u200b",
        "segment_57": "def get_related_memos(query_text, n_results, threshold)",
        "segment_58": "Retrieves memos that are related to the given query text within the specified distance threshold.",
        "segment_59": "prepopulate\u200b",
        "segment_60": "def prepopulate()",
        "segment_61": "Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial.Edit this pagePreviousagent_capabilityNextagent_builderTeachability ObjectsMemoStore ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using FastChat and perform inference on ChatGLMv2-6b.",
        "segment_2": "Preparations\u200b",
        "segment_3": "Clone FastChat\u200b",
        "segment_4": "FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.",
        "segment_5": "git clone https://github.com/lm-sys/FastChat.gitcd FastChat",
        "segment_6": "Download checkpoint\u200b",
        "segment_7": "ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.",
        "segment_8": "Before downloading from HuggingFace Hub, you need to have Git LFS installed.",
        "segment_9": "git clone https://huggingface.co/THUDM/chatglm2-6b",
        "segment_10": "Initiate server\u200b",
        "segment_11": "First, launch the controller",
        "segment_12": "python -m fastchat.serve.controller",
        "segment_13": "Then, launch the model worker(s)",
        "segment_14": "python -m fastchat.serve.model_worker --model-path chatglm2-6b",
        "segment_15": "Finally, launch the RESTful API server",
        "segment_16": "python -m fastchat.serve.openai_api_server --host localhost --port 8000",
        "segment_17": "Normally this will work. However, if you encounter error like this, commenting out all the lines containing finish_reason in fastchat/protocol/api_protocol.py and fastchat/protocol/openai_api_protocol.py will fix the problem. The modified code looks like:",
        "segment_18": "class CompletionResponseChoice(BaseModel): index: int text: str logprobs: Optional[int] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]]class CompletionResponseStreamChoice(BaseModel): index: int text: str logprobs: Optional[float] = None # finish_reason: Optional[Literal[\"stop\", \"length\"]] = None",
        "segment_19": "Interact with model using oai.Completion (requires openai<1)\u200b",
        "segment_20": "Now the models can be directly accessed through openai-python library as well as autogen.oai.Completion and autogen.oai.ChatCompletion.",
        "segment_21": "from autogen import oai# create a text completion requestresponse = oai.Completion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", # just a placeholder } ], prompt=\"Hi\",)print(response)# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_22": "If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).",
        "segment_23": "interacting with multiple local LLMs\u200b",
        "segment_24": "If you would like to interact with multiple LLMs on your local machine, replace the model_worker step above with a multi model variant:",
        "segment_25": "python -m fastchat.serve.multi_model_worker \\ --model-path lmsys/vicuna-7b-v1.3 \\ --model-names vicuna-7b-v1.3 \\ --model-path chatglm2-6b \\ --model-names chatglm2-6b",
        "segment_26": "The inference code would be:",
        "segment_27": "from autogen import oai# create a chat completion requestresponse = oai.ChatCompletion.create( config_list=[ { \"model\": \"chatglm2-6b\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", }, { \"model\": \"vicuna-7b-v1.3\", \"base_url\": \"http://localhost:8000/v1\", \"api_type\": \"openai\", \"api_key\": \"NULL\", } ], messages=[{\"role\": \"user\", \"content\": \"Hi\"}])print(response)",
        "segment_28": "For Further Reading\u200b",
        "segment_30": "Documentation about autogen.",
        "segment_31": "Documentation about FastChat.",
        "segment_32": "Tags:LLMNewer PostRetrieval-Augmented Generation (RAG) Applications with AutoGenOlder PostMathChat - An Conversational Framework to Solve Math ProblemsPreparationsClone FastChatDownload checkpointInitiate serverInteract with model using oai.Completion (requires openai<1)interacting with multiple local LLMsFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "This page lists libraries that have integrations with Autogen for LLM applications using multiple agents in alphabetical order. Including your own integration to this list is highly encouraged. Simply open a pull request with a few lines of text, see the dropdown below for more information.",
        "segment_2": "MemGPT + AutoGen\u200b",
        "segment_4": "MemGPT enables LLMs to manage their own memory and overcome limited context windows. You can use MemGPT to create perpetual chatbots that learn about you and modify their own personalities over time. You can connect MemGPT to your own local filesystems and databases, as well as connect MemGPT to your own tools and APIs. The MemGPT + AutoGen integration allows you to equip any AutoGen agent with MemGPT capabilities.",
        "segment_6": "MemGPT + AutoGen Documentation with Code Examples",
        "segment_8": "Microsoft Fabric + AutoGen\u200b",
        "segment_10": "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. In this notenook, we give a simple example for using AutoGen in Microsoft Fabric.",
        "segment_12": "Microsoft Fabric + AutoGen Code Examples",
        "segment_13": "Edit this pageMemGPT + AutoGenMicrosoft Fabric + AutoGenCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "These formats will be parsed by the 'unstructured' library, if installed.",
        "segment_2": "split_text_to_chunks\u200b",
        "segment_3": "def split_text_to_chunks(text: str, max_tokens: int = 4000, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, overlap: int = 10)",
        "segment_4": "Split a long text into chunks of max_tokens.",
        "segment_5": "extract_text_from_pdf\u200b",
        "segment_6": "def extract_text_from_pdf(file: str) -> str",
        "segment_7": "Extract text from PDF files",
        "segment_8": "split_files_to_chunks\u200b",
        "segment_9": "def split_files_to_chunks(files: list, max_tokens: int = 4000, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, custom_text_split_function: Callable = None)",
        "segment_10": "Split a list of files into chunks of max_tokens.",
        "segment_11": "get_files_from_dir\u200b",
        "segment_12": "def get_files_from_dir(dir_path: Union[str, List[str]], types: list = TEXT_FORMATS, recursive: bool = True)",
        "segment_13": "Return a list of all the files in a given directory, a url, a file path or a list of them.",
        "segment_14": "get_file_from_url\u200b",
        "segment_15": "def get_file_from_url(url: str, save_path: str = None)",
        "segment_16": "Download a file from a URL.",
        "segment_17": "is_url\u200b",
        "segment_18": "def is_url(string: str)",
        "segment_19": "Return True if the string is a valid URL.",
        "segment_20": "create_vector_db_from_dir\u200b",
        "segment_21": "def create_vector_db_from_dir(dir_path: Union[str, List[str]], max_tokens: int = 4000, client: API = None, db_path: str = \"/tmp/chromadb.db\", collection_name: str = \"all-my-documents\", get_or_create: bool = False, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, embedding_model: str = \"all-MiniLM-L6-v2\", embedding_function: Callable = None, custom_text_split_function: Callable = None, custom_text_types: List[str] = TEXT_FORMATS, recursive: bool = True, extra_docs: bool = False) -> API",
        "segment_22": "Create a vector db from all the files in a given directory, the directory can also be a single file or a url to",
        "segment_23": "a single file. We support chromadb compatible APIs to create the vector db, this function is not required if",
        "segment_24": "you prepared your own vector db.",
        "segment_25": "Arguments:",
        "segment_27": "dir_path Union[str, List[str]] - the path to the directory, file, url or a list of them.",
        "segment_28": "max_tokens Optional, int - the maximum number of tokens per chunk. Default is 4000.",
        "segment_29": "client Optional, API - the chromadb client. Default is None.",
        "segment_30": "db_path Optional, str - the path to the chromadb. Default is \"/tmp/chromadb.db\".",
        "segment_31": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_32": "get_or_create Optional, bool - Whether to get or create the collection. Default is False. If True, the collection",
        "segment_33": "will be returned if it already exists. Will raise ValueError if the collection already exists and get_or_create is False.",
        "segment_34": "chunk_mode Optional, str - the chunk mode. Default is \"multi_lines\".",
        "segment_35": "must_break_at_empty_line Optional, bool - Whether to break at empty line. Default is True.",
        "segment_36": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if",
        "segment_37": "embedding_function is not None.",
        "segment_38": "embedding_function Optional, Callable - the embedding function to use. Default is None, SentenceTransformer with",
        "segment_39": "the given embedding_model will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding",
        "segment_40": "functions, you can pass it here, follow the examples in max_tokens1.",
        "segment_41": "max_tokens2 Optional, Callable - a custom function to split a string into a list of strings.",
        "segment_42": "Default is None, will use the default function in max_tokens3.",
        "segment_43": "max_tokens4 Optional, List[str] - a list of file types to be processed. Default is TEXT_FORMATS.",
        "segment_44": "max_tokens5 Optional, bool - whether to search documents recursively in the dir_path. Default is True.",
        "segment_45": "max_tokens6 Optional, bool - whether to add more documents in the collection. Default is False",
        "segment_47": "Returns:",
        "segment_49": "max_tokens7 - the chromadb client.",
        "segment_51": "query_vector_db\u200b",
        "segment_52": "def query_vector_db(query_texts: List[str], n_results: int = 10, client: API = None, db_path: str = \"/tmp/chromadb.db\", collection_name: str = \"all-my-documents\", search_string: str = \"\", embedding_model: str = \"all-MiniLM-L6-v2\", embedding_function: Callable = None) -> QueryResult",
        "segment_53": "Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db",
        "segment_54": "and query function.",
        "segment_55": "Arguments:",
        "segment_57": "query_texts List[str] - the list of strings which will be used to query the vector db.",
        "segment_58": "n_results Optional, int - the number of results to return. Default is 10.",
        "segment_59": "client Optional, API - the chromadb compatible client. Default is None, a chromadb client will be used.",
        "segment_60": "db_path Optional, str - the path to the vector db. Default is \"/tmp/chromadb.db\".",
        "segment_61": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_62": "search_string Optional, str - the search string. Only docs that contain an exact match of this string will be retrieved. Default is \"\".",
        "segment_63": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if",
        "segment_64": "embedding_function is not None.",
        "segment_65": "embedding_function Optional, Callable - the embedding function to use. Default is None, SentenceTransformer with",
        "segment_66": "the given embedding_model will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding",
        "segment_67": "functions, you can pass it here, follow the examples in https://docs.trychroma.com/embeddings.",
        "segment_69": "Returns:",
        "segment_71": "n_results0 - the query result. The format is:",
        "segment_72": "class QueryResult(TypedDict):",
        "segment_73": "n_results1 - List[IDs]",
        "segment_74": "n_results2 - Optional[List[List[Embedding]]]",
        "segment_75": "n_results3 - Optional[List[List[Document]]]",
        "segment_76": "n_results4 - Optional[List[List[Metadata]]]",
        "segment_77": "n_results5 - Optional[List[List[float]]]",
        "segment_78": "Edit this pagePreviousmath_utilsNexttoken_count_utilsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "TL;DR:",
        "segment_3": "We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.",
        "segment_4": "MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.",
        "segment_5": "Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.",
        "segment_7": "Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.",
        "segment_8": "In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.",
        "segment_9": "We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.",
        "segment_10": "The MathChat Framework\u200b",
        "segment_11": "MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.",
        "segment_12": "The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:",
        "segment_15": "Tool-using Prompt: This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.",
        "segment_18": "Problem-Solving Strategy Selection Prompt: The assistant is instructed to choose one of three potential problem-solving strategies, including:",
        "segment_20": "Writing a Python program to solve the problem directly.",
        "segment_21": "Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.",
        "segment_22": "Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.",
        "segment_26": "Final Answer Encapsulation Prompt: This part instructs the assistant to put the final answer in \\boxed.",
        "segment_29": "The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.",
        "segment_30": "Let's take a look at an example between the User Proxy Agent and the LLM Assistant (GPT-4). The conversation focuses on how to solve inequality using Python.",
        "segment_31": "(The conversation is modified for readability.)",
        "segment_32": "ParticipantDialogueUser Proxy Agent(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) -3.LLM assistant (continued)Here's Python code to test the inequality for each interval: from sympy import symbols; x = symbols(\"x\"); inequality = (2*x + 10)*(x + 3) -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e)",
        "segment_33": "Experiment Setup\u200b",
        "segment_34": "We evaluate the improvement brought by MathChat.",
        "segment_35": "For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.",
        "segment_36": "We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer. For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in \\boxed, and we take the return of the function in PoT as the final answer.",
        "segment_37": "We also evaluate the following methods for comparison:",
        "segment_40": "Vanilla prompting: Evaluates GPT-4's direct problem-solving capability. The prompt used is: \" Solve the problem carefully. Put the final answer in \\boxed\".",
        "segment_43": "Program of Thoughts (PoT): Uses a zero-shot PoT prompt that requests the model to create a Solver function to solve the problem and return the final answer.",
        "segment_46": "Program Synthesis (PS) prompting: Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: \"Write a program that answers the following question: {Problem}\".",
        "segment_49": "Experiment Results\u200b",
        "segment_50": "The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:",
        "segment_52": "We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.",
        "segment_53": "For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.",
        "segment_54": "The code for experiments can be found at this repository.",
        "segment_55": "We now provide an implementation of MathChat using the interactive agents in AutoGen. See this notebook for example usage.",
        "segment_56": "Future Directions\u200b",
        "segment_57": "Despite MathChat's improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.",
        "segment_58": "Further work can be done to enhance this framework or math problem-solving in general:",
        "segment_60": "Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.",
        "segment_61": "Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.",
        "segment_62": "MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.",
        "segment_64": "For Further Reading\u200b",
        "segment_66": "Research paper of MathChat",
        "segment_67": "Documentation about autogen",
        "segment_69": "Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our Discord server for discussion.Tags:LLMGPTresearchNewer PostUse AutoGen for Local LLMsOlder PostAchieve More, Pay Less - Use GPT-4 SmartlyThe MathChat FrameworkExperiment SetupExperiment ResultsFuture DirectionsFor Further ReadingCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "AutoGen enables collaboration among multiple ChatGPTs for complex tasks.",
        "segment_2": "TLDR\u200b",
        "segment_3": "OpenAI assistants are now integrated into AutoGen via GPTAssistantAgent.",
        "segment_4": "This enables multiple OpenAI assistants, which form the backend of the now popular GPTs, to collaborate and tackle complex tasks.",
        "segment_5": "Checkout example notebooks for reference:",
        "segment_7": "Basic example",
        "segment_8": "Code interpreter",
        "segment_9": "Function calls",
        "segment_11": "Introduction\u200b",
        "segment_12": "Earlier last week, OpenAI introduced GPTs, giving users ability to create custom ChatGPTs tailored for them.",
        "segment_13": "But what if these individual GPTs could collaborate to do even more?",
        "segment_14": "Fortunately, because of AutoGen, this is now a reality!",
        "segment_15": "AutoGen has been pioneering agents and supporting multi-agent workflows since earlier this year, and now (starting with version 0.2.0b5) we are introducing compatibility with the Assistant API, which is currently in beta preview.",
        "segment_16": "To accomplish this, we've added a new (experimental) agent called the GPTAssistantAgent that",
        "segment_17": "lets you seamlessly add these new OpenAI assistants into AutoGen-based multi-agent workflows.",
        "segment_18": "This integration shows great potential and synergy, and we plan to continue enhancing it.",
        "segment_19": "Installation\u200b",
        "segment_20": "pip install pyautogen==0.2.0b5",
        "segment_21": "Basic Example\u200b",
        "segment_22": "Here's a basic example that uses a UserProxyAgent to allow an interface",
        "segment_23": "with the GPTAssistantAgent.",
        "segment_24": "First, import the new agent and setup config_list:",
        "segment_25": "from autogen import config_list_from_jsonfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgentfrom autogen import UserProxyAgentconfig_list = config_list_from_json(\"OAI_CONFIG_LIST\")",
        "segment_26": "Then simply define the OpenAI assistant agent and give it the task!",
        "segment_27": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_28": "GPTAssistantAgent supports both creating new OpenAI assistants or reusing existing assistants",
        "segment_29": "(e.g, by providing an assistant_id).",
        "segment_30": "Code Interpreter Example\u200b",
        "segment_31": "GPTAssistantAgent allows you to specify an OpenAI tools",
        "segment_32": "(e.g., function calls, code interpreter, etc). The example below enables an assistant",
        "segment_33": "that can use OpenAI code interpreter to solve tasks.",
        "segment_34": "# creates new assistant using Assistant APIgpt_assistant = GPTAssistantAgent( name=\"assistant\", llm_config={ \"config_list\": config_list, \"assistant_id\": None, \"tools\": [ { \"type\": \"code_interpreter\" } ], })user_proxy = UserProxyAgent(name=\"user_proxy\", code_execution_config={ \"work_dir\": \"coding\" }, human_input_mode=\"NEVER\")user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")",
        "segment_35": "Checkout more examples here.",
        "segment_36": "Limitations and Future Work\u200b",
        "segment_38": "Group chat managers using GPT assistant are pending.",
        "segment_39": "GPT assistants with multimodal capabilities haven't been released yet but we are committed to support them.",
        "segment_41": "Acknowledgements\u200b",
        "segment_42": "GPTAssistantAgent was made possible through collaboration with",
        "segment_43": "@IANTHEREAL,",
        "segment_44": "Jiale Liu,",
        "segment_45": "Yiran Wu,",
        "segment_46": "Qingyun Wu,",
        "segment_47": "Chi Wang, and many other AutoGen maintainers.Tags:openai-assistantNewer PostHow to Assess Utility of LLM-powered Applications?Older PostEcoAssistant - Using LLM Assistants More Accurately and AffordablyTLDRIntroductionInstallationBasic ExampleCode Interpreter ExampleLimitations and Future WorkAcknowledgementsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Cache()",
        "segment_2": "A wrapper class for managing cache configuration and instances.",
        "segment_3": "This class provides a unified interface for creating and interacting with",
        "segment_4": "different types of cache (e.g., Redis, Disk). It abstracts the underlying",
        "segment_5": "cache implementation details, providing methods for cache operations.",
        "segment_6": "Attributes:",
        "segment_8": "config Dict[str, Any] - A dictionary containing cache configuration.",
        "segment_9": "cache - The cache instance created based on the provided configuration.",
        "segment_11": "Methods:",
        "segment_12": "redis(cache_seed=42, redis_url=\"redis://localhost:6379/0\"): Static method to create a Redis cache instance.",
        "segment_13": "disk(cache_seed=42, cache_path_root=\".cache\"): Static method to create a Disk cache instance.",
        "segment_14": "init(self, config): Initializes the Cache with the given configuration.",
        "segment_16": "__enter__(self) - Context management entry, returning the cache instance.",
        "segment_17": "exit(self, exc_type, exc_value, traceback): Context management exit.",
        "segment_18": "get(self, key, default=None): Retrieves an item from the cache.",
        "segment_19": "set(self, key, value): Sets an item in the cache.",
        "segment_20": "close(self) - Closes the cache.",
        "segment_22": "redis\u200b",
        "segment_23": "@staticmethoddef redis(cache_seed=42, redis_url=\"redis://localhost:6379/0\")",
        "segment_24": "Create a Redis cache instance.",
        "segment_25": "Arguments:",
        "segment_27": "cache_seed int, optional - A seed for the cache. Defaults to 42.",
        "segment_28": "redis_url str, optional - The URL for the Redis server. Defaults to \"redis://localhost:6379/0\".",
        "segment_30": "Returns:",
        "segment_32": "Cache - A Cache instance configured for Redis.",
        "segment_34": "disk\u200b",
        "segment_35": "@staticmethoddef disk(cache_seed=42, cache_path_root=\".cache\")",
        "segment_36": "Create a Disk cache instance.",
        "segment_37": "Arguments:",
        "segment_39": "cache_seed int, optional - A seed for the cache. Defaults to 42.",
        "segment_40": "cache_path_root str, optional - The root path for the disk cache. Defaults to \".cache\".",
        "segment_42": "Returns:",
        "segment_44": "Cache - A Cache instance configured for Disk caching.",
        "segment_46": "__init__\u200b",
        "segment_47": "def __init__(config: Dict[str, Any])",
        "segment_48": "Initialize the Cache with the given configuration.",
        "segment_49": "Validates the configuration keys and creates the cache instance.",
        "segment_50": "Arguments:",
        "segment_52": "config Dict[str, Any] - A dictionary containing the cache configuration.",
        "segment_54": "Raises:",
        "segment_56": "ValueError - If an invalid configuration key is provided.",
        "segment_58": "__enter__\u200b",
        "segment_59": "def __enter__()",
        "segment_60": "Enter the runtime context related to the cache object.",
        "segment_61": "Returns:",
        "segment_62": "The cache instance for use within a context block.",
        "segment_63": "__exit__\u200b",
        "segment_64": "def __exit__(exc_type, exc_value, traceback)",
        "segment_65": "Exit the runtime context related to the cache object.",
        "segment_66": "Cleans up the cache instance and handles any exceptions that occurred",
        "segment_67": "within the context.",
        "segment_68": "Arguments:",
        "segment_70": "exc_type - The exception type if an exception was raised in the context.",
        "segment_71": "exc_value - The exception value if an exception was raised in the context.",
        "segment_72": "traceback - The traceback if an exception was raised in the context.",
        "segment_74": "get\u200b",
        "segment_75": "def get(key, default=None)",
        "segment_76": "Retrieve an item from the cache.",
        "segment_77": "Arguments:",
        "segment_79": "key str - The key identifying the item in the cache.",
        "segment_80": "default optional - The default value to return if the key is not found.",
        "segment_81": "Defaults to None.",
        "segment_83": "Returns:",
        "segment_84": "The value associated with the key if found, else the default value.",
        "segment_85": "set\u200b",
        "segment_86": "def set(key, value)",
        "segment_87": "Set an item in the cache.",
        "segment_88": "Arguments:",
        "segment_90": "key str - The key under which the item is to be stored.",
        "segment_91": "value - The value to be stored in the cache.",
        "segment_93": "close\u200b",
        "segment_94": "def close()",
        "segment_95": "Close the cache.",
        "segment_96": "Perform any necessary cleanup, such as closing connections or releasing resources.Edit this pagePreviousabstract_cache_baseNextcache_factoryCache ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {},
    {
        "segment_1": "class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)",
        "segment_2": "__init__\u200b",
        "segment_3": "def __init__(name=\"RetrieveChatAgent\", human_input_mode: Optional[str] = \"ALWAYS\", is_termination_msg: Optional[Callable[[Dict], bool]] = None, retrieve_config: Optional[Dict] = None, **kwargs)",
        "segment_4": "Arguments:",
        "segment_6": "name str - name of the agent.",
        "segment_7": "human_input_mode str - whether to ask for human inputs every time a message is received.",
        "segment_8": "Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".",
        "segment_9": "(1) When \"ALWAYS\", the agent prompts for human input every time a message is received.",
        "segment_10": "Under this mode, the conversation stops when the human input is \"exit\",",
        "segment_11": "or when is_termination_msg is True and there is no human input.",
        "segment_12": "(2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or",
        "segment_13": "the number of auto reply reaches the max_consecutive_auto_reply.",
        "segment_14": "(3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops",
        "segment_15": "when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.",
        "segment_16": "is_termination_msg function - a function that takes a message in the form of a dictionary",
        "segment_17": "and returns a boolean value indicating if this received message is a termination message.",
        "segment_18": "The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".",
        "segment_19": "retrieve_config dict or None - config for the retrieve agent.",
        "segment_20": "To use default config, set to None. Otherwise, set to a dictionary with the following keys:",
        "segment_22": "task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System",
        "segment_23": "prompt will be different for different tasks. The default value is default, which supports both code and qa.",
        "segment_24": "client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.",
        "segment_25": "will be used. If you want to use other vector db, extend this class and override the retrieve_docs function.",
        "segment_26": "docs_path (Optional, Union[str, List[str]]): the path to the docs directory. It can also be the path to a single file,",
        "segment_27": "the url to a single file or a list of directories, files and urls. Default is None, which works only if the collection is already created.",
        "segment_28": "extra_docs (Optional, bool): when true, allows adding documents with unique IDs without overwriting existing ones; when false, it replaces existing documents using default IDs, risking collection overwrite.,",
        "segment_29": "when set to true it enables the system to assign unique IDs starting from \"length+i\" for new document chunks, preventing the replacement of existing documents and facilitating the addition of more content to the collection..",
        "segment_30": "By default, \"extra_docs\" is set to false, starting document IDs from zero. This poses a risk as new documents might overwrite existing ones, potentially causing unintended loss or alteration of data in the collection.",
        "segment_31": "collection_name (Optional, str): the name of the collection.",
        "segment_32": "If key not provided, a default name autogen-docs will be used.",
        "segment_33": "model (Optional, str): the model to use for the retrieve chat.",
        "segment_34": "If key not provided, a default model gpt-4 will be used.",
        "segment_35": "chunk_token_size (Optional, int): the chunk token size for the retrieve chat.",
        "segment_36": "If key not provided, a default size max_tokens * 0.4 will be used.",
        "segment_37": "context_max_tokens (Optional, int): the context max token size for the retrieve chat.",
        "segment_38": "If key not provided, a default size max_tokens * 0.8 will be used.",
        "segment_39": "chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are",
        "segment_40": "\"multi_lines\" and \"one_line\". If key not provided, a default mode human_input_mode0 will be used.",
        "segment_41": "must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.",
        "segment_42": "If chunk_mode is \"one_line\", this parameter will be ignored.",
        "segment_43": "embedding_model (Optional, str): the embedding model to use for the retrieve chat.",
        "segment_44": "If key not provided, a default model human_input_mode1 will be used. All available models",
        "segment_45": "can be found at human_input_mode2.",
        "segment_46": "customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.",
        "segment_47": "customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".",
        "segment_48": "If not \"\" and the customized_answer_prefix is not in the answer, human_input_mode3 will be triggered.",
        "segment_49": "update_context (Optional, bool): if False, will not apply human_input_mode3 for interactive retrieval. Default is True.",
        "segment_50": "custom_token_count_function (Optional, Callable): a custom function to count the number of tokens in a string.",
        "segment_51": "The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).",
        "segment_52": "Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.",
        "segment_53": "custom_text_split_function (Optional, Callable): a custom function to split a string into a list of strings.",
        "segment_54": "Default is None, will use the default function in human_input_mode5.",
        "segment_55": "custom_text_types (Optional, List[str]): a list of file types to be processed. Default is human_input_mode6.",
        "segment_56": "This only applies to files under the directories in human_input_mode7. Explicitly included files and urls will be chunked regardless of their types.",
        "segment_57": "recursive (Optional, bool): whether to search documents recursively in the docs_path. Default is True.",
        "segment_58": "parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.",
        "segment_59": "on_disk (Optional, bool): Whether to store the collection on disk. Default is False.",
        "segment_60": "quantization_config: Quantization configuration. If None, quantization will be disabled.",
        "segment_61": "hnsw_config: HNSW configuration. If None, default configuration will be used.",
        "segment_62": "You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/`human_input_mode`8-index.",
        "segment_63": "API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_64": "payload_indexing: Whether to create a payload index for the document field. Default is False.",
        "segment_65": "You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/`human_input_mode`9-index",
        "segment_66": "API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index",
        "segment_69": "is_termination_msg0 dict - other kwargs in UserProxyAgent.",
        "segment_71": "retrieve_docs\u200b",
        "segment_72": "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")",
        "segment_73": "Arguments:",
        "segment_75": "problem str - the problem to be solved.",
        "segment_76": "n_results int - the number of results to be retrieved. Default is 20.",
        "segment_77": "search_string str - only docs that contain an exact match of this string will be retrieved. Default is \"\".",
        "segment_79": "create_qdrant_from_dir\u200b",
        "segment_80": "def create_qdrant_from_dir( dir_path: str, max_tokens: int = 4000, client: QdrantClient = None, collection_name: str = \"all-my-documents\", chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True, embedding_model: str = \"BAAI/bge-small-en-v1.5\", custom_text_split_function: Callable = None, custom_text_types: List[str] = TEXT_FORMATS, recursive: bool = True, extra_docs: bool = False, parallel: int = 0, on_disk: bool = False, quantization_config: Optional[models.QuantizationConfig] = None, hnsw_config: Optional[models.HnswConfigDiff] = None, payload_indexing: bool = False, qdrant_client_options: Optional[Dict] = {})",
        "segment_81": "Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a",
        "segment_82": "url to a single file.",
        "segment_83": "Arguments:",
        "segment_85": "dir_path str - the path to the directory, file or url.",
        "segment_86": "max_tokens Optional, int - the maximum number of tokens per chunk. Default is 4000.",
        "segment_87": "client Optional, QdrantClient - the QdrantClient instance. Default is None.",
        "segment_88": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_89": "chunk_mode Optional, str - the chunk mode. Default is \"multi_lines\".",
        "segment_90": "must_break_at_empty_line Optional, bool - Whether to break at empty line. Default is True.",
        "segment_91": "embedding_model Optional, str - the embedding model to use. Default is \"BAAI/bge-small-en-v1.5\".",
        "segment_92": "The list of all the available models can be at https://qdrant.github.io/fastembed/examples/Supported_Models/.",
        "segment_93": "custom_text_split_function Optional, Callable - a custom function to split a string into a list of strings.",
        "segment_94": "Default is None, will use the default function in autogen.retrieve_utils.split_text_to_chunks.",
        "segment_95": "custom_text_types Optional, List[str] - a list of file types to be processed. Default is TEXT_FORMATS.",
        "segment_96": "max_tokens0 Optional, bool - whether to search documents recursively in the dir_path. Default is True.",
        "segment_97": "max_tokens1 Optional, bool - whether to add more documents in the collection. Default is False",
        "segment_98": "max_tokens2 Optional, int - How many parallel workers to use for embedding. Defaults to the number of CPU cores",
        "segment_99": "max_tokens3 Optional, bool - Whether to store the collection on disk. Default is False.",
        "segment_100": "max_tokens4 - Quantization configuration. If None, quantization will be disabled.",
        "segment_101": "max_tokens5 - https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_102": "max_tokens6 - HNSW configuration. If None, default configuration will be used.",
        "segment_103": "max_tokens5 - https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection",
        "segment_104": "max_tokens8 - Whether to create a payload index for the document field. Default is False.",
        "segment_105": "max_tokens9 - (Optional, dict): the options for instantiating the qdrant client.",
        "segment_106": "max_tokens5 - https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.",
        "segment_108": "query_qdrant\u200b",
        "segment_109": "def query_qdrant( query_texts: List[str], n_results: int = 10, client: QdrantClient = None, collection_name: str = \"all-my-documents\", search_string: str = \"\", embedding_model: str = \"BAAI/bge-small-en-v1.5\", qdrant_client_options: Optional[Dict] = {}) -> List[List[QueryResponse]]",
        "segment_110": "Perform a similarity search with filters on a Qdrant collection",
        "segment_111": "Arguments:",
        "segment_113": "query_texts List[str] - the query texts.",
        "segment_114": "n_results Optional, int - the number of results to return. Default is 10.",
        "segment_115": "client Optional, API - the QdrantClient instance. A default in-memory client will be instantiated if None.",
        "segment_116": "collection_name Optional, str - the name of the collection. Default is \"all-my-documents\".",
        "segment_117": "search_string Optional, str - the search string. Default is \"\".",
        "segment_118": "embedding_model Optional, str - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if embedding_function is not None.",
        "segment_119": "qdrant_client_options - (Optional, dict): the options for instantiating the qdrant client. Reference: https://github.com/qdrant/qdrant-client/blob/master/qdrant_client/qdrant_client.py#L36-L58.",
        "segment_121": "Returns:",
        "segment_123": "List[List[QueryResponse]] - the query result. The format is:",
        "segment_124": "class QueryResponse(BaseModel, extra=\"forbid\"): # type: ignore",
        "segment_125": "id - Union[str, int]",
        "segment_126": "embedding - Optional[List[float]]",
        "segment_127": "n_results0 - Dict[str, Any]",
        "segment_128": "n_results1 - str",
        "segment_129": "n_results2 - float",
        "segment_130": "Edit this pagePreviousmultimodal_conversable_agentNextretrieve_assistant_agentQdrantRetrieveUserProxyAgent ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class Teachability(AgentCapability)",
        "segment_2": "Teachability uses a vector database to give an agent the ability to remember user teachings,",
        "segment_3": "where the user is any caller (human or not) sending messages to the teachable agent.",
        "segment_4": "Teachability is designed to be composable with other agent capabilities.",
        "segment_5": "To make any conversable agent teachable, instantiate both the agent and the Teachability class,",
        "segment_6": "then pass the agent to teachability.add_to_agent(agent).",
        "segment_7": "Note that teachable agents in a group chat must be given unique path_to_db_dir values.",
        "segment_8": "__init__\u200b",
        "segment_9": "def __init__(verbosity: Optional[int] = 0, reset_db: Optional[bool] = False, path_to_db_dir: Optional[str] = \"./tmp/teachable_agent_db\", recall_threshold: Optional[float] = 1.5, max_num_retrievals: Optional[int] = 10, llm_config: Optional[Union[Dict, bool]] = None)",
        "segment_10": "Arguments:",
        "segment_12": "verbosity Optional, int - # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.",
        "segment_13": "reset_db Optional, bool - True to clear the DB before starting. Default False.",
        "segment_14": "path_to_db_dir Optional, str - path to the directory where this particular agent's DB is stored. Default \"./tmp/teachable_agent_db\"",
        "segment_15": "recall_threshold Optional, float - The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.",
        "segment_16": "max_num_retrievals Optional, int - The maximum number of memos to retrieve from the DB. Default 10.",
        "segment_17": "llm_config dict or False - llm inference configuration passed to TextAnalyzerAgent.",
        "segment_18": "If None, TextAnalyzerAgent uses llm_config from the teachable agent.",
        "segment_20": "add_to_agent\u200b",
        "segment_21": "def add_to_agent(agent: ConversableAgent)",
        "segment_22": "Adds teachability to the given agent.",
        "segment_23": "prepopulate_db\u200b",
        "segment_24": "def prepopulate_db()",
        "segment_25": "Adds a few arbitrary memos to the DB.",
        "segment_26": "process_last_message\u200b",
        "segment_27": "def process_last_message(text)",
        "segment_28": "Appends any relevant memos to the message text, and stores any apparent teachings in new memos.",
        "segment_29": "Uses TextAnalyzerAgent to make decisions about memo storage and retrieval.",
        "segment_30": "MemoStore Objects\u200b",
        "segment_31": "class MemoStore()",
        "segment_32": "Provides memory storage and retrieval for a teachable agent, using a vector database.",
        "segment_33": "Each DB entry (called a memo) is a pair of strings: an input text and an output text.",
        "segment_34": "The input text might be a question, or a task to perform.",
        "segment_35": "The output text might be an answer to the question, or advice on how to perform the task.",
        "segment_36": "Vector embeddings are currently supplied by Chroma's default Sentence Transformers.",
        "segment_37": "__init__\u200b",
        "segment_38": "def __init__(verbosity, reset, path_to_db_dir)",
        "segment_39": "Arguments:",
        "segment_41": "verbosity (Optional, int): 1 to print memory operations, 0 to omit them. 3+ to print memo lists.",
        "segment_42": "path_to_db_dir (Optional, str): path to the directory where the DB is stored.",
        "segment_44": "list_memos\u200b",
        "segment_45": "def list_memos()",
        "segment_46": "Prints the contents of MemoStore.",
        "segment_47": "reset_db\u200b",
        "segment_48": "def reset_db()",
        "segment_49": "Forces immediate deletion of the DB's contents, in memory and on disk.",
        "segment_50": "add_input_output_pair\u200b",
        "segment_51": "def add_input_output_pair(input_text, output_text)",
        "segment_52": "Adds an input-output pair to the vector DB.",
        "segment_53": "get_nearest_memo\u200b",
        "segment_54": "def get_nearest_memo(query_text)",
        "segment_55": "Retrieves the nearest memo to the given query text.",
        "segment_56": "get_related_memos\u200b",
        "segment_57": "def get_related_memos(query_text, n_results, threshold)",
        "segment_58": "Retrieves memos that are related to the given query text within the specified distance threshold.",
        "segment_59": "prepopulate\u200b",
        "segment_60": "def prepopulate()",
        "segment_61": "Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial.Edit this pagePreviousagent_capabilityNextagent_builderTeachability ObjectsMemoStore ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "def get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any",
        "segment_2": "Get the type annotation of a parameter.",
        "segment_3": "Arguments:",
        "segment_5": "annotation - The annotation of the parameter",
        "segment_6": "globalns - The global namespace of the function",
        "segment_8": "Returns:",
        "segment_9": "The type annotation of the parameter",
        "segment_10": "get_typed_signature\u200b",
        "segment_11": "def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature",
        "segment_12": "Get the signature of a function with type annotations.",
        "segment_13": "Arguments:",
        "segment_15": "call - The function to get the signature for",
        "segment_17": "Returns:",
        "segment_18": "The signature of the function with type annotations",
        "segment_19": "get_typed_return_annotation\u200b",
        "segment_20": "def get_typed_return_annotation(call: Callable[..., Any]) -> Any",
        "segment_21": "Get the return annotation of a function.",
        "segment_22": "Arguments:",
        "segment_24": "call - The function to get the return annotation for",
        "segment_26": "Returns:",
        "segment_27": "The return annotation of the function",
        "segment_28": "get_param_annotations\u200b",
        "segment_29": "def get_param_annotations( typed_signature: inspect.Signature) -> Dict[int, Union[Annotated[Type[Any], str], Type[Any]]]",
        "segment_30": "Get the type annotations of the parameters of a function",
        "segment_31": "Arguments:",
        "segment_33": "typed_signature - The signature of the function with type annotations",
        "segment_35": "Returns:",
        "segment_36": "A dictionary of the type annotations of the parameters of the function",
        "segment_37": "Parameters Objects\u200b",
        "segment_38": "class Parameters(BaseModel)",
        "segment_39": "Parameters of a function as defined by the OpenAI API",
        "segment_40": "Function Objects\u200b",
        "segment_41": "class Function(BaseModel)",
        "segment_42": "A function as defined by the OpenAI API",
        "segment_43": "ToolFunction Objects\u200b",
        "segment_44": "class ToolFunction(BaseModel)",
        "segment_45": "A function under tool as defined by the OpenAI API.",
        "segment_46": "get_parameter_json_schema\u200b",
        "segment_47": "def get_parameter_json_schema( k: str, v: Union[Annotated[Type[Any], str], Type[Any]], default_values: Dict[str, Any]) -> JsonSchemaValue",
        "segment_48": "Get a JSON schema for a parameter as defined by the OpenAI API",
        "segment_49": "Arguments:",
        "segment_51": "k - The name of the parameter",
        "segment_52": "v - The type of the parameter",
        "segment_53": "default_values - The default values of the parameters of the function",
        "segment_55": "Returns:",
        "segment_56": "A Pydanitc model for the parameter",
        "segment_57": "get_required_params\u200b",
        "segment_58": "def get_required_params(typed_signature: inspect.Signature) -> List[str]",
        "segment_59": "Get the required parameters of a function",
        "segment_60": "Arguments:",
        "segment_62": "signature - The signature of the function as returned by inspect.signature",
        "segment_64": "Returns:",
        "segment_65": "A list of the required parameters of the function",
        "segment_66": "get_default_values\u200b",
        "segment_67": "def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]",
        "segment_68": "Get default values of parameters of a function",
        "segment_69": "Arguments:",
        "segment_71": "signature - The signature of the function as returned by inspect.signature",
        "segment_73": "Returns:",
        "segment_74": "A dictionary of the default values of the parameters of the function",
        "segment_75": "get_parameters\u200b",
        "segment_76": "def get_parameters(required: List[str], param_annotations: Dict[str, Union[Annotated[Type[Any], str], Type[Any]]], default_values: Dict[str, Any]) -> Parameters",
        "segment_77": "Get the parameters of a function as defined by the OpenAI API",
        "segment_78": "Arguments:",
        "segment_80": "required - The required parameters of the function",
        "segment_81": "hints - The type hints of the function as returned by typing.get_type_hints",
        "segment_83": "Returns:",
        "segment_84": "A Pydantic model for the parameters of the function",
        "segment_85": "get_missing_annotations\u200b",
        "segment_86": "def get_missing_annotations(typed_signature: inspect.Signature, required: List[str]) -> Tuple[Set[str], Set[str]]",
        "segment_87": "Get the missing annotations of a function",
        "segment_88": "Ignores the parameters with default values as they are not required to be annotated, but logs a warning.",
        "segment_89": "Arguments:",
        "segment_91": "typed_signature - The signature of the function with type annotations",
        "segment_92": "required - The required parameters of the function",
        "segment_94": "Returns:",
        "segment_95": "A set of the missing annotations of the function",
        "segment_96": "get_function_schema\u200b",
        "segment_97": "def get_function_schema(f: Callable[..., Any], *, name: Optional[str] = None, description: str) -> Dict[str, Any]",
        "segment_98": "Get a JSON schema for a function as defined by the OpenAI API",
        "segment_99": "Arguments:",
        "segment_101": "f - The function to get the JSON schema for",
        "segment_102": "name - The name of the function",
        "segment_103": "description - The description of the function",
        "segment_105": "Returns:",
        "segment_106": "A JSON schema for the function",
        "segment_107": "Raises:",
        "segment_109": "TypeError - If the function is not annotated",
        "segment_111": "Examples:",
        "segment_112": "```def f(a: Annotated[str, \"Parameter a\"], b: int = 2, c: Annotated[float, \"Parameter c\"] = 0.1) -> None: passget_function_schema(f, description=\"function f\")# {'type': 'function',# 'function': {'description': 'function f',# 'name': 'f',# 'parameters': {'type': 'object',# 'properties': {'a': {'type': 'str', 'description': 'Parameter a'},# 'b': {'type': 'int', 'description': 'b'},# 'c': {'type': 'float', 'description': 'Parameter c'}},# 'required': ['a']}}} ```",
        "segment_113": "get_load_param_if_needed_function\u200b",
        "segment_114": "def get_load_param_if_needed_function( t: Any) -> Optional[Callable[[T, Type[Any]], BaseModel]]",
        "segment_115": "Get a function to load a parameter if it is a Pydantic model",
        "segment_116": "Arguments:",
        "segment_118": "t - The type annotation of the parameter",
        "segment_120": "Returns:",
        "segment_121": "A function to load the parameter if it is a Pydantic model, otherwise None",
        "segment_122": "load_basemodels_if_needed\u200b",
        "segment_123": "def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any]",
        "segment_124": "A decorator to load the parameters of a function if they are Pydantic models",
        "segment_125": "Arguments:",
        "segment_127": "func - The function with annotated parameters",
        "segment_129": "Returns:",
        "segment_130": "A function that loads the parameters before calling the original functionEdit this pagePreviouscode_utilsNextgraph_utilsParameters ObjectsFunction ObjectsToolFunction ObjectsCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    },
    {
        "segment_1": "class WebSurferAgent(ConversableAgent)",
        "segment_2": "(In preview) An agent that acts as a basic web surfer that can search the web and visit web pages.",
        "segment_3": "generate_surfer_reply\u200b",
        "segment_4": "def generate_surfer_reply( messages: Optional[List[Dict]] = None, sender: Optional[Agent] = None, config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]]"
    },
    {
        "segment_1": "AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.",
        "segment_2": "TLDR\u200b",
        "segment_3": "To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by AutoGen. It allows you to:",
        "segment_5": "Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).",
        "segment_6": "Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).",
        "segment_7": "Explicitly add skills to your agents and accomplish more tasks.",
        "segment_8": "Publish your sessions to a local gallery.",
        "segment_10": "AutoGen Studio is open source code here, and can be installed via pip. Give it a try!",
        "segment_11": "pip install autogenstudio",
        "segment_12": "Introduction\u200b",
        "segment_13": "The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. AutoGen has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: AutoGen Studio.",
        "segment_14": "With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.",
        "segment_16": "Note: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.",
        "segment_18": "Getting Started with AutoGen Studio\u200b",
        "segment_19": "The following guide will help you get AutoGen Studio up and running on your system.",
        "segment_20": "Configuring an LLM Provider\u200b",
        "segment_21": "To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation here. Configure your environment with either OPENAI_API_KEY or AZURE_OPENAI_API_KEY.",
        "segment_22": "For example, in your terminal, you would set the API key like this:",
        "segment_23": "export OPENAI_API_KEY=",
        "segment_24": "You can also specify the model directly in the agent's configuration as shown below.",
        "segment_25": "llm_config = LLMConfig( config_list=[{ \"model\": \"gpt-4\", \"api_key\": \"\", \"base_url\": \"\", \"api_type\": \"azure\", \"api_version\": \"2023-06-01-preview\" }], temperature=0,)",
        "segment_26": "Installation\u200b",
        "segment_27": "There are two ways to install AutoGen Studio - from PyPi or from source. We recommend installing from PyPi unless you plan to modify the source code.",
        "segment_30": "Install from PyPi",
        "segment_31": "We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:",
        "segment_32": "pip install autogenstudio",
        "segment_35": "Install from Source",
        "segment_37": "Note: This approach requires some familiarity with building interfaces in React.",
        "segment_39": "If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here's how you get started:",
        "segment_42": "Clone the AutoGen Studio repository and install its Python dependencies:",
        "segment_43": "pip install -e .",
        "segment_46": "Navigate to the samples/apps/autogen-studio/frontend directory, install dependencies, and build the UI:",
        "segment_47": "npm install -g gatsby-clinpm install --global yarnyarn installyarn build",
        "segment_50": "For Windows users, to build the frontend, you may need alternative commands provided in the autogen studio readme.",
        "segment_53": "Running the Application\u200b",
        "segment_54": "Once installed, run the web UI by entering the following in your terminal:",
        "segment_55": "autogenstudio ui --port 8081",
        "segment_56": "This will start the application on the specified port. Open your web browser and go to http://localhost:8081/ to begin using AutoGen Studio.",
        "segment_57": "Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.",
        "segment_58": "What Can You Do with AutoGen Studio?\u200b",
        "segment_59": "The AutoGen Studio UI is organized into 3 high level sections - Build, Playground, and Gallery.",
        "segment_60": "Build\u200b",
        "segment_62": "This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:",
        "segment_63": "Skills: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. generate_images), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.",
        "segment_65": "AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.",
        "segment_66": "Agents: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base AutoGen conversable agent class).",
        "segment_67": "Agent Workflows: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents \u2013 a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.",
        "segment_68": "Playground\u200b",
        "segment_70": "AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf's).",
        "segment_71": "The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:",
        "segment_72": "Session: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be \u201cpublished\u201d to a \u201cgallery\u201d.",
        "segment_73": "Chat View: A chat is a sequence of interactions between a user and an agent. It is a part of a session.",
        "segment_74": "Gallery\u200b",
        "segment_75": "This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).",
        "segment_76": "AutoGen Studio comes with 3 example skills: fetch_profile, find_papers, generate_images. Please feel free to review the repo to learn more about how they work.",
        "segment_77": "The AutoGen Studio API\u200b",
        "segment_78": "While AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the AutoGen Studio repo for more details.",
        "segment_79": "import jsonfrom autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig# load an agent specification in JSONagent_spec = json.load(open('agent_spec.json'))# Create an AutoGen Workflow Configuration from the agent specificationagent_work_flow_config = FlowConfig(**agent_spec)# Create a Workflow from the configurationagent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)# Run the workflow on a tasktask_query = \"What is the height of the Eiffel Tower?\"agent_work_flow.run(message=task_query)",
        "segment_80": "Road Map and Next Steps\u200b",
        "segment_81": "As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here's what users can look forward to:",
        "segment_83": "Complex Agent Workflows: We're working on integrating support for more sophisticated agent workflows, such as GroupChat, allowing for richer interaction between multiple agents or dynamic topologies.",
        "segment_84": "Improved User Experience: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.",
        "segment_85": "Expansion of Agent Skills: We will work towards improving the workflow for authoring, composing and reusing agent skills.",
        "segment_86": "Community Features: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We're exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.",
        "segment_88": "Contribution Guide\u200b",
        "segment_89": "We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:",
        "segment_91": "Review the overall AutoGen project contribution guide.",
        "segment_92": "Please review the AutoGen Studio roadmap to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with help-wanted.",
        "segment_93": "Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.",
        "segment_94": "Please review the autogenstudio dev branch here [dev branch].(https://github.com/microsoft/autogen/tree/autogenstudio) and use as a base for your contribution. This way, your contribution will be aligned with the latest changes in the AutoGen Studio project.",
        "segment_95": "Submit a pull request with your contribution!",
        "segment_96": "If you are modifying AutoGen Studio in vscode, it has its own devcontainer to simplify dev work. See instructions in .devcontainer/README.md on how to use it.",
        "segment_97": "Please use the tag studio for any issues, questions, and PRs related to Studio.",
        "segment_99": "FAQ\u200b",
        "segment_100": "Q: Where can I adjust the default skills, agent and workflow configurations?",
        "segment_101": "A: You can modify agent configurations directly from the UI or by editing the autogentstudio/utils/dbdefaults.json file which is used to initialize the database.",
        "segment_102": "Q: If I want to reset the entire conversation with an agent, how do I go about it?",
        "segment_103": "A: To reset your conversation history, you can delete the database.sqlite file. If you need to clear user-specific data, remove the relevant autogenstudio/web/files/user/ folder.",
        "segment_104": "Q: Is it possible to view the output and messages generated by the agents during interactions?",
        "segment_105": "A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the database.sqlite file for a comprehensive record of messages.",
        "segment_106": "Q: Where can I find documentation and support for AutoGen Studio?",
        "segment_107": "A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the AutoGen Studio Readme. For additional support, please open an issue on GitHub or ask questions on Discord.",
        "segment_108": "Q: Can I use Other Models with AutoGen Studio?",
        "segment_109": "Yes. AutoGen standardizes on the openai model api format, and you can use any api server that offers an openai compliant endpoint. In the AutoGen Studio UI, each agent has an llm_config field where you can input your model endpoint details including model name, api key, base url, model type and api version. For Azure OpenAI models, you can find these details in the Azure portal. Note that for Azure OpenAI, the model name is the deployment id or engine, and the model type is \"azure\".",
        "segment_110": "For other OSS models, we recommend using a server such as vllm to instantiate an openai compliant endpoint.",
        "segment_111": "Q: The Server Starts But I Can't Access the UI",
        "segment_112": "A: If you are running the server on a remote machine (or a local machine that fails to resolve localhost correstly), you may need to specify the host address. By default, the host address is set to localhost. You can specify the host address using the --host argument. For example, to start the server on port 8081 and local address such that it is accessible from other machines on the network, you can run the following command:",
        "segment_113": "autogenstudio ui --port 8081 --host 0.0.0.0",
        "segment_114": "Tags:AutoGenUIwebUXCommunityDiscordTwitterCopyright \u00a9 2024 AutoGen Authors | Privacy and Cookies"
    }
]