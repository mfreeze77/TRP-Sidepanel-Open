(embeddings)=

Embeddings
Embedding models allow you to take a piece of text - a word, sentence, paragraph or even a whole article, and convert that into an array of floating point numbers.

This floating point array is called an "embedding vector", and works as a numerical representation of the semantic meaning of the content in a many-multi-dimensional space.

By calculating the distance between embedding vectors, we can identify which content is semantically "nearest" to other content.

This can be used to build features like related article lookups. It can also be used to build semantic search, where a user can search for a phrase and get back results that are semantically similar to that phrase even if they do not share any exact keywords.

Some embedding models like CLIP can even work against binary files such as images. These can be used to search for images that are similar to other images, or to search for images that are semantically similar to a piece of text.

LLM supports multiple embedding models through {ref}plugins <plugins>. Once installed, an embedding model can be used on the command-line or via the Python API to calculate and store embeddings for content, and then to perform similarity searches against those embeddings.

See LLM now provides tools for working with embeddings for an extended explanation of embeddings, why they are useful and what you can do with them.

---
maxdepth: 3
---
cli
python-api
writing-plugins
storage

Embedding storage format
The default output format of the llm embed command is a JSON array of floating point numbers.

LLM stores embeddings in space-efficient format: a little-endian binary sequences of 32-bit floating point numbers, each represented using 4 bytes.

These are stored in a BLOB column in a SQLite database.

The following Python functions can be used to convert between this format and an array of floating point numbers:

import struct

def encode(values):
    return struct.pack("<" + "f" * len(values), *values)

def decode(binary):
    return struct.unpack("<" + "f" * (len(binary) // 4), binary)
These functions are available as llm.encode() and llm.decode().

If you are using NumPy you can decode one of these binary values like this:

import numpy as np

numpy_array = np.frombuffer(value, "<f4")
The <f4 format string here ensures NumPy will treat the data as a little-endian sequence of 32-bit floats.
Using embeddings from Python
You can load an embedding model using its model ID or alias like this:

import llm

embedding_model = llm.get_embedding_model("ada-002")
To embed a string, returning a Python list of floating point numbers, use the .embed() method:

vector = embedding_model.embed("my happy hound")
If the embedding model can handle binary input, you can call .embed() with a byte string instead. You can check the supports_binary property to see if this is supported:

if embedding_model.supports_binary:
    vector = embedding_model.embed(open("my-image.jpg", "rb").read())
The embedding_model.supports_text property indicates if the model supports text input.

Many embeddings models are more efficient when you embed multiple strings or binary strings at once. To embed multiple strings at once, use the .embed_multi() method:

vectors = list(embedding_model.embed_multi(["my happy hound", "my dissatisfied cat"]))
This returns a generator that yields one embedding vector per string.

Embeddings are calculated in batches. By default all items will be processed in a single batch, unless the underlying embedding model has defined its own preferred batch size. You can pass a custom batch size using batch_size=N, for example:

vectors = list(embedding_model.embed_multi(lines_from_file, batch_size=20))
(embeddings-python-collections)=

Working with collections
The llm.Collection class can be used to work with collections of embeddings from Python code.

A collection is a named group of embedding vectors, each stored along with their IDs in a SQLite database table.

To work with embeddings in this way you will need an instance of a sqlite-utils Database object. You can then pass that to the llm.Collection constructor along with the unique string name of the collection and the ID of the embedding model you will be using with that collection:

import sqlite_utils
import llm

# This collection will use an in-memory database that will be
# discarded when the Python process exits
collection = llm.Collection("entries", model_id="ada-002")

# Or you can persist the database to disk like this:
db = sqlite_utils.Database("my-embeddings.db")
collection = llm.Collection("entries", db, model_id="ada-002")

# You can pass a model directly using model= instead of model_id=
embedding_model = llm.get_embedding_model("ada-002")
collection = llm.Collection("entries", db, model=embedding_model)
If the collection already exists in the database you can omit the model or model_id argument - the model ID will be read from the collections table.

To embed a single string and store it in the collection, use the embed() method:

collection.embed("hound", "my happy hound")
This stores the embedding for the string "my happy hound" in the entries collection under the key hound.

Add store=True to store the text content itself in the database table along with the embedding vector.

To attach additional metadata to an item, pass a JSON-compatible dictionary as the metadata= argument:

collection.embed("hound", "my happy hound", metadata={"name": "Hound"}, store=True)
This additional metadata will be stored as JSON in the metadata column of the embeddings database table.

(embeddings-python-bulk)=

Storing embeddings in bulk
The collection.embed_multi() method can be used to store embeddings for multiple items at once. This can be more efficient for some embedding models.

collection.embed_multi(
    [
        ("hound", "my happy hound"),
        ("cat", "my dissatisfied cat"),
    ],
    # Add this to store the strings in the content column:
    store=True,
)
To include metadata to be stored with each item, call embed_multi_with_metadata():

collection.embed_multi_with_metadata(
    [
        ("hound", "my happy hound", {"name": "Hound"}),
        ("cat", "my dissatisfied cat", {"name": "Cat"}),
    ],
    # This can also take the store=True argument:
    store=True,
)
The batch_size= argument defaults to 100, and will be used unless the embedding model itself defines a lower batch size. You can adjust this if you are having trouble with memory while embedding large collections:

collection.embed_multi(
    (
        (i, line)
        for i, line in enumerate(lines_in_file)
    ),
    batch_size=10
)
(embeddings-python-collection-class)=

Collection class reference
A collection instance has the following properties and methods:

id - the integer ID of the collection in the database
name - the string name of the collection (unique in the database)
model_id - the string ID of the embedding model used for this collection
model() - returns the EmbeddingModel instance, based on that model_id
count() - returns the integer number of items in the collection
embed(id: str, text: str, metadata: dict=None, store: bool=False) - embeds the given string and stores it in the collection under the given ID. Can optionally include metadata (stored as JSON) and store the text content itself in the database table.
embed_multi(entries: Iterable, store: bool=False, batch_size: int=100) - see above
embed_multi_with_metadata(entries: Iterable, store: bool=False, batch_size: int=100) - see above
similar(query: str, number: int=10) - returns a list of entries that are most similar to the embedding of the given query string
similar_by_id(id: str, number: int=10) - returns a list of entries that are most similar to the embedding of the item with the given ID
similar_by_vector(vector: List[float], number: int=10, skip_id: str=None) - returns a list of entries that are most similar to the given embedding vector, optionally skipping the entry with the given ID
delete() - deletes the collection and its embeddings from the database
There is also a Collection.exists(db, name) class method which returns a boolean value and can be used to determine if a collection exists or not in a database:

if Collection.exists(db, "entries"):
    print("The entries collection exists")
(embeddings-python-similar)=

Retrieving similar items
Once you have populated a collection of embeddings you can retrieve the entries that are most similar to a given string using the similar() method.

This method uses a brute force approach, calculating distance scores against every document. This is fine for small collections, but will not scale to large collections. See issue 216 for plans to add a more scalable approach via vector indexes provided by plugins.

for entry in collection.similar("hound"):
    print(entry.id, entry.score)
The string will first by embedded using the model for the collection.

The entry object returned is an object with the following properties:

id - the string ID of the item
score - the floating point similarity score between the item and the query string
content - the string text content of the item, if it was stored - or None
metadata - the dictionary (from JSON) metadata for the item, if it was stored - or None
This defaults to returning the 10 most similar items. You can change this by passing a different number= argument:

for entry in collection.similar("hound", number=5):
    print(entry.id, entry.score)
The similar_by_id() method takes the ID of another item in the collection and returns the most similar items to that one, based on the embedding that has already been stored for it:

for entry in collection.similar_by_id("cat"):
    print(entry.id, entry.score)
The item itself is excluded from the results.

(embeddings-sql-schema)=

SQL schema
Here's the SQL schema used by the embeddings database:

CREATE TABLE [collections] (
   [id] INTEGER PRIMARY KEY,
   [name] TEXT,
   [model] TEXT
)
CREATE TABLE "embeddings" (
   [collection_id] INTEGER REFERENCES [collections]([id]),
   [id] TEXT,
   [embedding] BLOB,
   [content] TEXT,
   [content_blob] BLOB,
   [content_hash] BLOB,
   [metadata] TEXT,
   [updated] INTEGER,
   PRIMARY KEY ([collection_id], [id])
)

Embedding with the CLI
LLM provides command-line utilities for calculating and storing embeddings for pieces of content.

(embeddings-cli-embed)=

llm embed
The llm embed command can be used to calculate embedding vectors for a string of content. These can be returned directly to the terminal, stored in a SQLite database, or both.

Returning embeddings to the terminal
The simplest way to use this command is to pass content to it using the -c/--content option, like this:

llm embed -c 'This is some content' -m 3-small
-m 3-small specifies the OpenAI text-embedding-3-small model. You will need to have set an OpenAI API key using llm keys set openai for this to work.

You can install plugins to access other models. The llm-sentence-transformers plugin can be used to run models on your own laptop, such as the MiniLM-L6 model:

llm install llm-sentence-transformers
llm embed -c 'This is some content' -m sentence-transformers/all-MiniLM-L6-v2
The llm embed command returns a JSON array of floating point numbers directly to the terminal:

[0.123, 0.456, 0.789...]
You can omit the -m/--model option if you set a {ref}default embedding model <embeddings-cli-embed-models-default>.

LLM also offers a binary storage format for embeddings, described in {ref}embeddings storage format <embeddings-storage>.

You can output embeddings using that format as raw bytes using --format blob, or in hexadecimal using --format hex, or in Base64 using --format base64:

llm embed -c 'This is some content' -m 3-small --format base64
This outputs:

8NGzPFtdgTqHcZw7aUT6u+++WrwwpZo8XbSxv...
Some models such as llm-clip can run against binary data. You can pass in binary data using the -i and --binary options:

llm embed --binary -m clip -i image.jpg
Or from standard input like this:

cat image.jpg | llm embed --binary -m clip -i -
(embeddings-collections)=

Storing embeddings in SQLite
Embeddings are much more useful if you store them somewhere, so you can calculate similarity scores between different embeddings later on.

LLM includes the concept of a collection of embeddings. A collection groups together a set of stored embeddings created using the same model, each with a unique ID within that collection.

Embeddings also store a hash of the content that was embedded. This hash is later used to avoid calculating duplicate embeddings for the same content.

First, we'll set a default model so we don't have to keep repeating it:

llm embed-models default 3-small
The llm embed command can store results directly in a named collection like this:

llm embed quotations philkarlton-1 -c \
  'There are only two hard things in Computer Science: cache invalidation and naming things'
This stores the given text in the quotations collection under the key philkarlton-1.

You can also pipe content to standard input, like this:

cat one.txt | llm embed files one
This will store the embedding for the contents of one.txt in the files collection under the key one.

A collection will be created the first time you mention it.

Collections have a fixed embedding model, which is the model that was used for the first embedding stored in that collection.

In the above example this would have been the default embedding model at the time that the command was run.

The following example stores the embedding for the string "my happy hound" in a collection called phrases under the key hound and using the model 3-small:

llm embed phrases hound -m 3-small -c 'my happy hound'
By default, the SQLite database used to store embeddings is the embeddings.db in the user content directory managed by LLM.

You can see the path to this directory by running llm collections path.

You can store embeddings in a different SQLite database by passing a path to it using the -d/--database option to llm embed. If this file does not exist yet the command will create it:

llm embed phrases hound -d my-embeddings.db -c 'my happy hound'
This creates a database file called my-embeddings.db in the current directory.

(embeddings-collections-content-metadata)=

Storing content and metadata
By default, only the entry ID and the embedding vector are stored in the database table.

You can store a copy of the original text in the content column by passing the --store option:

llm embed phrases hound -c 'my happy hound' --store
You can also store a JSON object containing arbitrary metadata in the metadata column by passing the --metadata option. This example uses both --store and --metadata options:

llm embed phrases hound \
  -m 3-small \
  -c 'my happy hound' \
  --metadata '{"name": "Hound"}' \
  --store
Data stored in this way will be returned by calls to llm similar, for example:

llm similar phrases -c 'hound'
{"id": "hound", "score": 0.8484683588631485, "content": "my happy hound", "metadata": {"name": "Hound"}}
(embeddings-cli-embed-multi)=

llm embed-multi
The llm embed command embeds a single string at a time.

llm embed-multi can be used to embed multiple strings at once, taking advantage of any efficiencies that the embedding model may provide when processing multiple strings.

This command can be called in one of three ways:

With a CSV, TSV, JSON or newline-delimited JSON file
With a SQLite database and a SQL query
With one or more paths to directories, each accompanied by a glob pattern
All three mechanisms support these options:

-m model_id to specify the embedding model to use
-d database.db to specify a different database file to store the embeddings in
--store to store the original content in the embeddings table in addition to the embedding vector
--prefix to prepend a prefix to the stored ID of each item
--batch-size SIZE to process embeddings in batches of the specified size
(embeddings-cli-embed-multi-csv-etc)=

Embedding data from a CSV, TSV or JSON file
You can embed data from a CSV, TSV or JSON file by passing that file to the command as the second option, after the collection name.

Your file must contain at least two columns. The first one is expected to contain the ID of the item, and any subsequent columns will be treated as containing content to be embedded.

An example CSV file might look like this:

id,content
one,This is the first item
two,This is the second item
TSV would use tabs instead of commas.

JSON files can be structured like this:

[
  {"id": "one", "content": "This is the first item"},
  {"id": "two", "content": "This is the second item"}
]
Or as newline-delimited JSON like this:

{"id": "one", "content": "This is the first item"}
{"id": "two", "content": "This is the second item"}
In each of these cases the file can be passed to llm embed-multi like this:

llm embed-multi items mydata.csv
The first argument is the name of the collection, the second is the filename.

You can also pipe content to standard input of the tool using -:

cat mydata.json | llm embed-multi items -
LLM will attempt to detect the format of your data automatically. If this doesn't work you can specify the format using the --format option. This is required if you are piping newline-delimited JSON to standard input.

cat mydata.json | llm embed-multi items - --format nl
Other supported --format options are csv, tsv and json.

This example embeds the data from a JSON file in a collection called items in database called docs.db using the 3-small model and stores the original content in the embeddings table as well, adding a prefix of my-items/ to each ID:

llm embed-multi items mydata.json \
  -d docs.db \
  -m 3-small \
  --prefix my-items/ \
  --store
(embeddings-cli-embed-multi-sqlite)=

Embedding data from a SQLite database
You can embed data from a SQLite database using --sql, optionally combined with --attach to attach an additional database.

If you are storing embeddings in the same database as the source data, you can do this:

llm embed-multi docs \
  -d docs.db \
  --sql 'select id, title, content from documents' \
  -m 3-small
The docs.db database here contains a documents table, and we want to embed the title and content columns from that table and store the results back in the same database.

To load content from a database other than the one you are using to store embeddings, attach it with the --attach option and use alias.table in your SQLite query:

llm embed-multi docs \
  -d embeddings.db \
  --attach other other.db \
  --sql 'select id, title, content from other.documents' \
  -m 3-small
(embeddings-cli-embed-multi-directories)=

Embedding data from files in directories
LLM can embed the content of every text file in a specified directory, using the file's path and name as the ID.

Consider a directory structure like this:

docs/aliases.md
docs/contributing.md
docs/embeddings/binary.md
docs/embeddings/cli.md
docs/embeddings/index.md
docs/index.md
docs/logging.md
docs/plugins/directory.md
docs/plugins/index.md
To embed all of those documents, you can run the following:

llm embed-multi documentation \
  -m 3-small \
  --files docs '**/*.md' \
  -d documentation.db \
  --store
Here --files docs '**/*.md' specifies that the docs directory should be scanned for files matching the **/*.md glob pattern - which will match Markdown files in any nested directory.

The result of the above command is a embeddings table with the following IDs:

aliases.md
contributing.md
embeddings/binary.md
embeddings/cli.md
embeddings/index.md
index.md
logging.md
plugins/directory.md
plugins/index.md
Each corresponding to embedded content for the file in question.

The --prefix option can be used to add a prefix to each ID:

llm embed-multi documentation \
  -m 3-small \
  --files docs '**/*.md' \
  -d documentation.db \
  --store \
  --prefix llm-docs/
This will result in the following IDs instead:

llm-docs/aliases.md
llm-docs/contributing.md
llm-docs/embeddings/binary.md
llm-docs/embeddings/cli.md
llm-docs/embeddings/index.md
llm-docs/index.md
llm-docs/logging.md
llm-docs/plugins/directory.md
llm-docs/plugins/index.md
Files are assumed to be utf-8, but LLM will fall back to latin-1 if it encounters an encoding error. You can specify a different set of encodings using the --encoding option.

This example will try utf-16 first and then mac_roman before falling back to latin-1:

llm embed-multi documentation \
  -m 3-small \
  --files docs '**/*.md' \
  -d documentation.db \
  --encoding utf-16 \
  --encoding mac_roman \
  --encoding latin-1
If a file cannot be read it will be logged to standard error but the script will keep on running.

If you are embedding binary content such as images for use with CLIP, add the --binary option:

llm embed-multi photos \
  -m clip \
  --files photos/ '*.jpeg' --binary
(embeddings-cli-similar)=

llm similar
The llm similar command searches a collection of embeddings for the items that are most similar to a given or item ID.

This currently uses a slow brute-force approach which does not scale well to large collections. See issue 216 for plans to add a more scalable approach via vector indexes provided by plugins.

To search the quotations collection for items that are semantically similar to 'computer science':

llm similar quotations -c 'computer science'
This embeds the provided string and returns a newline-delimited list of JSON objects like this:

{"id": "philkarlton-1", "score": 0.8323904531677017, "content": null, "metadata": null}
You can compare against text stored in a file using -i filename:

llm similar quotations -i one.txt
Or feed text to standard input using -i -:

echo 'computer science' | llm similar quotations -i -
When using a model like CLIP, you can find images similar to an input image using -i filename with --binary:

llm similar photos -i image.jpg --binary
(embeddings-cli-embed-models)=

llm embed-models
To list all available embedding models, including those provided by plugins, run this command:

llm embed-models
The output should look something like this:

3-small (aliases: ada)
sentence-transformers/all-MiniLM-L6-v2 (aliases: all-MiniLM-L6-v2)
(embeddings-cli-embed-models-default)=

llm embed-models default
This command can be used to get and set the default embedding model.

This will return the name of the current default model:

llm embed-models default
You can set a different default like this:

llm embed-models default 3-small
This will set the default model to OpenAI's 3-small model.

Any of the supported aliases for a model can be passed to this command.

You can unset the default model using --remove-default:

llm embed-models default --remove-default
When no default model is set, the llm embed and llm embed-multi commands will require that a model is specified using -m/--model.

llm collections list
To list all of the collections in the embeddings database, run this command:

llm collections list
Add --json for JSON output:

llm collections list --json
Add -d/--database to specify a different database file:

llm collections list -d my-embeddings.db
llm collections delete
To delete a collection from the database, run this:

llm collections delete collection-name
Pass -d to specify a different database file:

llm collections delete collection-name -d my-embeddings.db
